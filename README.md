DevOps Engineer (3 - 7 yrs)

Course Type: role-specific
Role: DevOps Engineer
Course Title: DevOps Engineer Interview
Pre-read: DevOps Engineer (0 - 3 yrs)

Course Description: Achieve comprehensive readiness for DevOps Engineer roles at top-tier technology companies and high-growth startups. Master cloud infrastructure management, CI/CD pipeline implementation, container orchestration, and security-first architecture. Learn through industry-grade challenges, real-world case studies, and insider strategies derived from hiring standards at FAANG and unicorn startups. If This seems too advanced don’t worry we have a 0-3yrs experience one too.

Module 1:
Title: DevOps Interviews Introduction
Description: Gain a complete roadmap for DevOps Engineer interviews. Understand the role evolution, master high-impact skills, and know exactly what to expect at every stage of the hiring process.
Order: 1
Learning Outcomes:
Understand DevOps role expectations across different company sizes
Master the DevOps skill matrix and toolchain
Navigate the complete interview lifecycle

Topic 1.1:
Title: DevOps Landscape & Role Overview
Order: 1

Class 1.1.1:
	Title: DevOps Interviews Introduction
	Description: Introduction to the DevOps interview landscape and course philosophy.
Content Type: text
Duration: 300 
Order: 1
		Text Content :

## 1. Course Overview
DevOps at top-tier tech companies is not just about knowing Jenkins or Docker—it is about **culture, automation, and reliability**.

From startups to FAANG, the industry has shifted from manual System Administration to Site Reliability Engineering (SRE). This module is designed to bridge the gap between "knowing the tools" and "passing the interview," providing a complete roadmap to crack roles at high-impact organizations.

---

## 2. Curriculum Design Philosophy
We did not build this curriculum in isolation. It is **reverse-engineered** from the actual hiring rubrics of companies like **Google (SRE), AWS, Netflix, and Uber**.

By analyzing hundreds of real-world interview loops, we have isolated the specific signals—both technical and architectural—that hiring committees look for when making an offer.

---

## 3. Key Learning Outcomes
* **Role clarity:** Understand exactly what is expected of a DevOps Engineer vs. an SRE vs. a Platform Engineer.
* **Toolchain Mastery:** Move beyond "Hello World" tutorials to understanding production-grade toolchains.
* **Lifecycle Navigation:** Master the end-to-end interview process, from the initial recruiter screen to the final system design round.

---

## 4. Target Audience & Prerequisites

### Who Should Join
* **Experienced Infrastructure Engineers:** Professionals looking to modernize their stack and move into cloud-native roles.
* **Software Engineers:** Developers transitioning to "Shift-Left" DevOps or Platform Engineering roles.
* **System Administrators:** Sysadmins aiming to pivot into automation and high-scale cloud operations.
* **High-Potential Graduates:** Candidates with a strong Linux/Scripting foundation seeking a specialized career path.

### Who This Is Not For
> **Note:** This is an interview accelerator, **not a Linux boot camp**. We assume you possess a foundational comfort with the command line and basic scripting concepts. For People with 0 -3 yrs of experience we do have a another devops Engineer program

---

## 5. Recommended Learning Path
* **Identify Your Gap:** If you are strong in Cloud but weak in Coding, focus heavily on the scripting modules.
* **Practice Design:** DevOps interviews are heavy on whiteboarding. Do not just watch the videos—draw the architectures yourself.
* **Consistency:** Commit to solving **one troubleshooting scenario or scripting challenge daily**.

Class 1.1.2:
	Title: DevOps Job Titles Decoded
	Description: Understanding the differences between DevOps roles across companies.
Content Type: text
Duration: 400 
Order: 2
		Text Content :
 # Decoding the DevOps Title Chaos

"SRE, DevOps Engineer, Platform Engineer, CloudOps..." The infrastructure landscape is flooded with varied titles. You have likely asked yourself:

* **Is an SRE just a DevOps engineer who writes more code?**
* **Does a Platform Engineer handle on-call rotations?**
* **How do I know which role fits my skills?**

In this module, we will deconstruct the ecosystem. We will look beyond the labels to understand the core archetypes, ensuring you align your preparation with the right role.

---

## Why Do Job Titles Vary So Much?
Job titles are often a reflection of a company's scale and maturity rather than a universal standard.

### 1. The "Big Tech" Specialist Model
Tech giants (like Google, AWS, or Meta) usually have rigid, highly specialized tracks.
* **Example:** Google invented **Site Reliability Engineering (SRE)** to apply software engineering principles to operations. Here, you are a specialist focused on reliability, SLOs, and error budgets.

### 2. The "Startup" Generalist Model
In fast-paced startups, boundaries blur.
* **Example:** A **DevOps Engineer** at a Series B startup is often a "Jack of all trades"—managing AWS accounts, writing Terraform, fixing Jenkins pipelines, and debugging production DB issues simultaneously.

| Large Tech Ecosystems | Startups / Agile Teams |
| :--- | :--- |
| **Highly Specialized:** Distinct teams for SRE, Core Infra, and Release Engineering. | **Broad Scope:** One "DevOps" team handles everything from CI/CD to Cloud Cost. |
| **Focus:** Depth (e.g., deep kernel tuning or database internals). | **Focus:** Breadth (e.g., getting the product shipped fast). |

---

## Role Specializations
Modern infrastructure roles are increasingly domain-specific:

### Site Reliability Engineer (SRE)
* **Focus:** Reliability, Scalability, Incident Management.
* **Key Metrics:** SLOs, SLIs, MTTR (Mean Time to Recovery).

### Platform Engineer (Internal Developer Platform)
* **Focus:** Building tooling for internal developers (e.g., "Heroku" for the company).
* **Goal:** Reduce friction for devs to deploy code.

### Cloud Engineer
* **Focus:** Architecture, Migration, and Governance.
* **Goal:** Designing secure and cost-effective cloud environments.

### DevSecOps Engineer
* **Focus:** Security Automation.
* **Goal:** Integrating security scanners (SAST/DAST) into the pipeline.

> **Pro Tip:** Ignore the title; decode the Job Description (JD). If the JD mentions "SLOs and Error Budgets," it is an SRE role. If it mentions "Developer Experience," it is a Platform role.

Class 1.1.3:
	Title: The DevOps Skill Matrix
	Description: Core competencies and technical skills required.
Content Type: text
Duration: 450 
Order: 3
		Text Content :
 # The DevOps Skill Matrix: The 6 Pillars

We didn't just guess what skills matter. We analyzed the hiring requirements of top-tier infrastructure teams to identify the **6 Pillars of DevOps Excellence**.

To get hired, you need a T-shaped skill set: broad knowledge across all pillars, and deep expertise in at least two.

---

## The 6 Technical Pillars

### 1. Linux Mastery (The Foundation)
You cannot automate what you do not understand.
* **System Administration:** Boot process, file systems, permissions.
* **Network Troubleshooting:** `curl`, `netstat`, `tcpdump`, DNS debugging.

### 2. Cloud Infrastructure (AWS / GCP / Azure)
* **Core Services:** Compute (EC2), Storage (S3), Networking (VPC).
* **Managed Services:** RDS, ElastiCache, Lambda.
* **FinOps:** Understanding cost optimization.

### 3. Container Orchestration (Kubernetes & Docker)
* **Containerization:** Dockerfiles, multi-stage builds.
* **K8s Architecture:** Pods, Deployments, Services, Ingress.
* **Production:** Helm charts, cluster upgrades, troubleshooting CrashLoopBackOff.

### 4. CI/CD & Automation
* **Tools:** Jenkins, GitHub Actions, GitLab CI.
* **Concepts:** Pipeline design, Blue/Green deployments, Canary releases.

### 5. Infrastructure as Code (IaC)
* **Tools:** Terraform (Industry Standard), CloudFormation.
* **Concepts:** State management, Modules, Immutable infrastructure.

### 6. Observability & Monitoring
* **The "Three Pillars":** Metrics (Prometheus), Logs (ELK/Loki), Traces (Jaeger).
* **Alerting:** Designing meaningful alerts that don't cause fatigue.

---

## The "Power Skills" Beyond Technical
While technical skills get your foot in the door, these soft skills get you the offer letter.

* **Incident Management:** Can you keep calm when production is down?
* **Communication:** Can you explain a complex outage to a non-technical Product Manager?
* **Security-First Mindset:** Do you think about permissions and secrets management by default?

> **The Universal Baseline:** Whether you apply for SRE or Cloud Engineer, **Linux and Scripting (Python/Bash)** are non-negotiable. You must be able to manipulate text and manage processes via the terminal.

Class 1.1.4:
	Title: The DevOps Interview Roadmap
	Description: What to expect at every stage of the hiring process.
Content Type: text
Duration: 500 
Order: 4
		Text Content :
 # The Roadmap to the Offer Letter

While every company has its nuances, the anatomy of a DevOps/SRE interview at top-tier firms follows a rigorous structure. Understanding this flow is the first step to mastering it.

---

## Round 1: The Recruiter Screen (The Gatekeeper)
* **Duration:** 15–30 Minutes
* **Goal:** A high-level check to ensure your experience matches the tech stack.

### The Conversation
Standard questions about your background and tool proficiency.
* *Example:* "Have you used Terraform in production?"
* *The Trap:* Recruiters use keyword matching. Ensure you clearly articulate the tools you have used without lying.

---

## Round 2: The Hiring Manager Technical Screen
* **Duration:** 45–60 Minutes
* **Goal:** To assess team fit and verify depth.

This round often involves:
1.  **System Design Lite:** "Describe the architecture of the last platform you built."
2.  **Past Project Deep Dive:** "Tell me about the hardest outage you debugged."
3.  **The "Vibe Check":** Assessing if you are a culture fit for the on-call rotation.

---

## Round 3: The Technical Deep Dive (The Gauntlet)
* **Duration:** 60–90 Minutes
* **Goal:** To stress-test your hands-on skills.

Companies diverge in their format here:
* **Live Coding:** Solving algorithmic problems (usually Easy/Medium) or practical scripting (e.g., "Write a Python script to parse these logs").
* **Infrastructure Design:** Drawing a high-availability architecture on a whiteboard.
* **Troubleshooting Scenario:** "The server is unresponsive. How do you debug it?" (The interviewer acts as the terminal).

---

## Round 4: The System Design / Architecture Round
* **Duration:** 60 Minutes
* **Goal:** To test Scalability and Trade-offs.

You will be asked to design a complex system from scratch.
* **The Prompt:** *"Design a global log aggregation system for microservices."*
* **The Expectation:** Focus on High Availability, Disaster Recovery, Latency, and Cost.

---

## Round 5: Operational Excellence / SRE Round
* **Duration:** 45–60 Minutes
* **Goal:** To test your production readiness.

This is unique to DevOps roles.
* **Incident Simulation:** You are placed in a mock outage scenario.
* **Post-Mortem Analysis:** How do you learn from failure?
* **On-Call Scenarios:** How do you prioritize alerts at 3 AM?

---

## Round 6: Behavioral & Cultural Fit
* **Duration:** 30–45 Minutes
* **Goal:** To assess Conflict Resolution and Ownership.

* **Example:** "Tell me about a time you disagreed with a developer about a deployment strategy."
* **Strategy:** Use the **STAR Framework** (Situation, Task, Action, Result) to show how you balance speed with stability.



Module 2:
Title: Linux & System Administration
Description: Gain practical fluency in Linux fundamentals, process management, and system troubleshooting. Master the command line, understand kernel internals, and develop the diagnostic skills required to debug production systems at scale.
Order: 2
Learning Outcomes:
Master Linux architecture and command line operations
Understand process lifecycle and resource management
Develop advanced troubleshooting and debugging skills
Navigate networking fundamentals and security principles

Topic 2.1:
Title: Linux Fundamentals
Order: 1

Class 2.1.1:
	Title: Linux Architecture & Philosophy
	Description: Understanding the kernel, shell, and file system hierarchy.
Content Type: text
Duration: 300 
Order: 1
		Text Content :
 # Linux Architecture: The Engine of DevOps

## 1. The Core Philosophy
Linux is not just an operating system; it is the foundation of the modern internet. In a DevOps interview, you won't be asked how to install Ubuntu. You will be asked how the **Kernel** interacts with hardware and how processes communicate.

### Kernel Space vs. User Space
* **The Kernel:** The "Boss." It manages CPU, memory, and devices. It operates in "Kernel Mode" (Ring 0) with unrestricted access to hardware.
* **User Space:** Where your applications (Nginx, Python, Bash) run. They operate in "User Mode" (Ring 3) and cannot touch hardware directly.
* **System Calls:** The bridge between the two. When your Python script reads a file, it makes a `syscall` (like `open()` or `read()`) to ask the Kernel to fetch the data.

---

## 2. The File System Hierarchy (FHS)
Unlike Windows (C:\, D:\), Linux uses a single tree structure starting at Root (`/`).

* `/bin` & `/usr/bin`: User binaries (commands like `ls`, `grep`).
* `/etc`: Configuration files (start here for troubleshooting).
* `/var`: Variable data (logs, databases, mail queues).
* `/proc`: A virtual filesystem. It doesn't exist on the disk; it exists in RAM and contains runtime system information (e.g., `/proc/cpuinfo`).
* `/dev`: Device files. In Linux, **"Everything is a file,"** even your hard drive (`/dev/sda`).

---

## 3. Distribution Differences
* **Debian/Ubuntu:** Uses `apt` (Advanced Package Tool). Common in startups and developer environments.
* **RHEL/CentOS/Fedora:** Uses `yum` or `dnf`. The standard for enterprise/banking environments due to long-term support.
* **Alpine:** Extremely lightweight (5MB). The standard for **Docker containers**.

---

## 4. The Boot Process Deep Dive
Understanding boot is critical when systems fail to start.

### The Sequence
1. **BIOS/UEFI POST:** Hardware initialization
2. **Bootloader (GRUB):** Kernel selection and loading
3. **Kernel Init:** Memory management, device drivers
4. **Init System (systemd):** Service orchestration
5. **Login Prompt:** System ready

### Common Boot Failures
* **Kernel Panic:** Usually hardware or filesystem corruption
* **Failed to Mount Root:** Check `/etc/fstab` for UUID mismatches
* **Service Dependencies:** systemd targets not met

Class 2.1.2:
	Title: Command Line Mastery
	Description: Essential commands for file manipulation and text processing.
Content Type: text
Duration: 400 
Order: 2
		Text Content :
 # Command Line Mastery

A DevOps engineer without CLI skills is like a surgeon who cannot use a scalpel. You must be able to manipulate data streams without opening a text editor.

---

## 1. The "Swiss Army Knife" Commands
* **`find`:** Searching for files based on metadata.
    * *Interview Q:* "Find all log files larger than 100MB modified in the last 24 hours."
    * `find /var/log -name "*.log" -size +100M -mtime -1`
* **`grep`:** Searching for text within files.
    * `grep -r "Error" /var/log/nginx/`
* **`awk`:** Column-based text processing. Used to extract specific fields (like IP addresses) from logs.
* **`sed`:** Stream editor. Used for find-and-replace in pipelines.

---

## 2. Pipes and Redirection
The power of Linux lies in chaining small tools together using the pipe (`|`).

* **Stdin (0), Stdout (1), Stderr (2):** Every process has these three streams.
* **The Pipeline:**
    `cat access.log | grep "404" | awk '{print $1}' | sort | uniq -c | sort -nr`
    *(This single line finds the top IP addresses causing 404 errors).*

### Advanced Redirection
* `command > file` - Redirect stdout, overwrite
* `command >> file` - Redirect stdout, append
* `command 2> file` - Redirect stderr
* `command &> file` - Redirect both stdout and stderr
* `command 2>&1` - Redirect stderr to stdout

---

## 3. Permissions & Ownership
* **`chmod`:** Change mode.
    * **755 (rwxr-xr-x):** Owner can do everything; everyone else can read/execute.
    * **400 (r-------):** Read-only for owner (standard for **SSH private keys**).
* **`chown`:** Change owner (`chown user:group file`).
* **SUID/SGID:** Special permissions that allow a user to run a file with the permissions of the file owner (e.g., `passwd` needs root access to write to `/etc/shadow`).

### The Octal Permission System
Understanding the numbers:
* **Read (r) = 4**
* **Write (w) = 2**
* **Execute (x) = 1**

Example: `chmod 644 file.txt` = rw-r--r-- (Owner: 6=4+2, Group: 4, Others: 4)

---

## 4. Text Processing Power Tools

### AWK for Data Extraction
```bash
# Extract the 5th column from a CSV
awk -F',' '{print $5}' data.csv

# Sum values in the 3rd column
awk '{sum += $3} END {print sum}' numbers.txt
```

### SED for Stream Editing
```bash
# Replace all occurrences
sed 's/old/new/g' file.txt

# Delete lines matching a pattern
sed '/pattern/d' file.txt

# In-place editing
sed -i 's/foo/bar/g' config.conf
```

Class 2.1.3:
	Title: User & Group Management
	Description: Managing access and security.
Content Type: text
Duration: 250 
Order: 3
		Text Content :
 # User & Group Management

Security starts with the Principle of Least Privilege.

---

## 1. Managing Users
* **`useradd` vs `adduser`:** `useradd` is the low-level binary; `adduser` is the friendly interactive script.
* **The Shadow File:** Passwords are NOT stored in `/etc/passwd`. They are hashed and stored in `/etc/shadow`, which only root can read.

### Key Files
* `/etc/passwd` - User account information
* `/etc/shadow` - Encrypted passwords and aging info
* `/etc/group` - Group definitions
* `/etc/sudoers` - Sudo privileges configuration

---

## 2. Sudo Privileges
Never run as root. Use `sudo`.
* **`/etc/sudoers`:** The configuration file defining who can run what.
* **NOPASSWD:** Common in automation (Jenkins/Ansible users), but dangerous if not scoped correctly.

### Best Practices
```bash
# Edit sudoers safely
visudo

# Grant specific command access
jenkins ALL=(ALL) NOPASSWD: /usr/bin/systemctl restart nginx

# Group-based permissions
%developers ALL=(ALL) ALL
```

---

## 3. SSH Key-Based Authentication
In production, password authentication is usually disabled.
1.  **Generate:** `ssh-keygen -t rsa -b 4096`
2.  **Copy:** `ssh-copy-id user@server`
3.  **Permissions:** The `.ssh` directory must be `700`, and `authorized_keys` must be `600`. If these are wrong, SSH will fail silently.

### SSH Configuration
```bash
# Client config: ~/.ssh/config
Host prod-server
    HostName 10.0.1.50
    User deploy
    IdentityFile ~/.ssh/prod_key
    Port 22
```

Topic 2.2:
Title: Process & Resource Management
Order: 2

Class 2.2.1:
	Title: Process Management
	Description: Lifecycle, signals, and background jobs.
Content Type: text
Duration: 350 
Order: 1
		Text Content :
 # Process Management

---

## 1. The Process Lifecycle
Every process is created by a parent process (except `init`/`systemd`).
* **Fork():** Creates a copy of the process.
* **Exec():** Replaces the copy with a new program.
* **Zombie Process:** A process that has finished execution but the parent hasn't read its exit code yet. You cannot kill a zombie; you must kill its parent.

### Process States
* **Running (R):** Currently executing
* **Sleeping (S):** Waiting for an event
* **Stopped (T):** Paused by signal
* **Zombie (Z):** Terminated but not cleaned up
* **Uninterruptible Sleep (D):** Usually waiting for I/O

---

## 2. Signals (Communicating with Processes)
When you type `Ctrl+C`, you are sending a signal.
* **SIGTERM (15):** "Please stop." The process can catch this, save data, and exit gracefully.
* **SIGKILL (9):** "Die immediately." The kernel rips the process out of memory. Data corruption can occur.
* **SIGHUP (1):** "Reload config." Used to restart services without downtime (e.g., Nginx).

### Signal Usage
```bash
# Graceful termination
kill -15 PID

# Force kill
kill -9 PID

# Reload configuration
kill -HUP PID

# List all signals
kill -l
```

---

## 3. Systemd (The Init System)
`systemd` is the first process (PID 1) on modern Linux.
* `systemctl start/stop/restart service_name`
* `systemctl enable service_name` (Start on boot)
* **Unit Files:** Defined in `/etc/systemd/system/`. This is where you define how your custom app starts automatically.

### Creating Custom Services
```ini
[Unit]
Description=My Application
After=network.target

[Service]
Type=simple
User=appuser
WorkingDirectory=/opt/myapp
ExecStart=/opt/myapp/start.sh
Restart=always

[Install]
WantedBy=multi-user.target
```

Class 2.2.2:
	Title: System Resource Monitoring
	Description: CPU, Memory, and Disk analysis.
Content Type: text
Duration: 400 
Order: 2
		Text Content :
 # System Resource Monitoring

"The server is slow." Your job is to prove *why*.

---

## 1. CPU: Load Average vs. Usage
* **Usage:** Percentage of time the CPU is busy.
* **Load Average (uptime/top):** The number of processes *waiting* for CPU time or Disk I/O.
    * *Rule of Thumb:* If Load Average > Number of Cores, you have a bottleneck.

### Key Monitoring Commands
* **`top`:** Real-time process viewer
* **`htop`:** Enhanced, interactive version of top
* **`vmstat`:** Virtual memory statistics
* **`mpstat`:** Multi-processor statistics

---

## 2. Memory: The "Free" RAM Myth
New users panic when they see "Free Memory: 200MB" on a 16GB server.
* **Cached/Buffer:** Linux borrows unused RAM to cache files for speed. This is good. If applications need RAM, the kernel instantly reclaims it.
* **Swap:** When RAM is full, Linux writes to the hard drive. This kills performance.
* **OOM Killer:** If Swap fills up, the kernel's "Out of Memory Killer" will sacrifice a process (often your database) to save the system.

### Memory Analysis
```bash
# Detailed memory breakdown
free -h

# Per-process memory usage
ps aux --sort=-%mem | head

# Memory pressure check
cat /proc/pressure/memory
```

---

## 3. Disk I/O
High CPU wait time often means the CPU is bored waiting for the Disk.
* **`iostat` / `iotop`:** Identifies which process is hammering the disk.

### Disk Space Management
```bash
# Disk usage by filesystem
df -h

# Directory size analysis
du -sh /var/* | sort -h

# Find large files
find / -type f -size +1G -exec ls -lh {} \;
```

---

## 4. Network Monitoring
* **`netstat` / `ss`:** Socket statistics and connections
* **`iftop`:** Real-time bandwidth usage by connection
* **`nethogs`:** Network usage per process

Class 2.2.3:
	Title: Performance Tuning
	Description: Kernel parameters and limits.
Content Type: text
Duration: 300 
Order: 3
		Text Content :
 # Performance Tuning

---

## 1. Ulimit (File Descriptors)
"Too many open files." This is the #1 error in high-scale Nginx/Database setups.
* Linux limits how many files a user can open (default 1024).
* **Fix:** Edit `/etc/security/limits.conf` to increase the soft/hard limits.

### Setting Limits
```bash
# View current limits
ulimit -a

# Temporary increase
ulimit -n 65536

# Permanent: /etc/security/limits.conf
* soft nofile 65536
* hard nofile 65536
```

---

## 2. Kernel Parameters (sysctl)
You can tune the kernel at runtime using `/etc/sysctl.conf`.
* **Swappiness:** Controls how aggressively Linux swaps to disk. For databases, we often lower this to `1` or `10` (default 60).
* **Network Tuning:** Increasing TCP buffer sizes for high-throughput connections.

### Common Tuning Parameters
```bash
# Reduce swap usage
vm.swappiness=10

# Increase network buffer
net.core.rmem_max=16777216
net.core.wmem_max=16777216

# Handle more connections
net.ipv4.tcp_max_syn_backlog=8192

# Apply changes
sysctl -p
```

---

## 3. Cgroups (Control Groups)
Limit resource usage for containers and processes.
* **CPU:** Restrict CPU shares
* **Memory:** Set hard memory limits
* **I/O:** Throttle disk bandwidth

Topic 2.3:
Title: System Troubleshooting
Order: 3

Class 2.3.1:
	Title: Log Management & Analysis
	Description: Finding the root cause in the logs.
Content Type: text
Duration: 350 
Order: 1
		Text Content :
 # Log Management: The Black Box

---

## 1. Key Log Locations
* `/var/log/syslog` (or `messages`): General system activity.
* `/var/log/auth.log` (or `secure`): SSH logins and sudo usage.
* `/var/log/kern.log`: Kernel crashes and hardware errors.
* `/var/log/dmesg`: Boot-time hardware detection.

### Application-Specific Logs
* **Apache/Nginx:** `/var/log/nginx/` or `/var/log/apache2/`
* **Database:** `/var/log/mysql/` or `/var/log/postgresql/`
* **System Services:** `journalctl -u service_name`

---

## 2. Journalctl
On systemd systems, logs are binary. Use `journalctl` to read them.
* `journalctl -u nginx -f`: Follow logs for a specific service.
* `journalctl --since "1 hour ago"`: Time-based filtering.

### Advanced Journalctl Usage
```bash
# Boot-specific logs
journalctl -b

# Priority filtering (error and above)
journalctl -p err

# Kernel messages only
journalctl -k

# Export to file
journalctl --since yesterday > logs.txt
```

---

## 3. Log Rotation
Logs grow forever until they fill the disk.
* **`logrotate`:** A utility that compresses old logs and deletes them after X days. Always check log rotation policies for high-volume apps.

### Logrotate Configuration
```bash
# /etc/logrotate.d/myapp
/var/log/myapp/*.log {
    daily
    rotate 7
    compress
    delaycompress
    missingok
    notifempty
    create 0640 appuser appgroup
}
```

Class 2.3.2:
	Title: Performance Debugging
	Description: Advanced troubleshooting techniques.
Content Type: text
Duration: 400 
Order: 2
		Text Content :
 # Performance Debugging: The Detective Work

---

## 1. The USE Method
Created by Brendan Gregg. For every resource (CPU, Disk, Net), check:
1.  **Utilization:** How busy is it? (0-100%)
2.  **Saturation:** Is there a queue/backlog?
3.  **Errors:** Are there hardware errors?

### Applying USE to Resources
**CPU:**
* Utilization: `mpstat 1`
* Saturation: Load average, `vmstat` run queue
* Errors: `dmesg | grep -i error`

**Memory:**
* Utilization: `free`, `vmstat`
* Saturation: Swap activity, page faults
* Errors: OOM events in logs

**Disk:**
* Utilization: `iostat -x 1`
* Saturation: `iostat` await time
* Errors: `smartctl -a /dev/sda`

---

## 2. Strace (System Call Trace)
When logs are silent, `strace` reveals the truth. It attaches to a process and shows every system call it makes.
* *Scenario:* A script hangs.
* *Action:* `strace -p PID`. You see it hanging on `connect()`, meaning it's a network firewall issue, not a code issue.

### Strace Usage Patterns
```bash
# Attach to running process
strace -p 1234

# Trace file operations
strace -e trace=file program

# Count system calls
strace -c program

# Follow forks
strace -f program
```

---

## 3. Lsof (List Open Files)
* Find who is using port 80: `lsof -i :80`
* Find who deleted a file but is still holding the space: `lsof +L1`

### Common Lsof Scenarios
```bash
# All network connections
lsof -i

# Files opened by user
lsof -u username

# Files opened by process
lsof -p PID

# Recover deleted files
lsof | grep deleted
```

---

## 4. Perf and Flamegraphs
For deep performance analysis:
* **`perf`:** CPU profiler showing where time is spent
* **Flamegraphs:** Visual representation of call stacks
* **`bpftrace`:** Dynamic tracing for production systems

Class 2.3.3:
	Title: System Recovery & Boot Process
	Description: Boot loaders and rescue modes.
Content Type: text
Duration: 300 
Order: 3
		Text Content :
 # System Recovery

---

## 1. The Linux Boot Process
1.  **BIOS/UEFI:** Hardware check.
2.  **MBR/GPT:** Finds the bootloader.
3.  **GRUB:** The Grand Unified Bootloader (allows selecting Kernels).
4.  **Kernel:** Mounts the root filesystem.
5.  **Init (Systemd):** Starts services.

### Boot Parameters
Modify boot behavior via GRUB:
* `single` or `1`: Single user mode
* `init=/bin/bash`: Emergency shell
* `systemd.unit=rescue.target`: Rescue mode
* `ro`: Read-only root filesystem

---

## 2. Recovery Modes
* **Single User Mode:** Boots into a root shell with no networking. Used to reset the root password.
* **Emergency Mode:** Used when the filesystem is corrupted and needs `fsck`.

### Common Recovery Scenarios
**Forgot Root Password:**
1. Boot to GRUB menu
2. Press 'e' to edit
3. Add `init=/bin/bash` to kernel line
4. Boot and run `passwd root`
5. Remount: `mount -o remount,rw /`

**Filesystem Corruption:**
1. Boot to rescue mode
2. Run `fsck /dev/sdaX`
3. Fix errors automatically with `-y` flag

---

## 3. Backup and Disaster Recovery
* **System Backups:** `rsync`, `tar`, `dd`
* **Configuration Management:** Track `/etc` with Git
* **Disaster Recovery Plan:** Document recovery procedures

Topic 2.4:
Title: Networking Fundamentals
Order: 4

Class 2.4.1:
	Title: Linux Networking Essentials
	Description: TCP/IP, DNS, and Firewalls.
Content Type: text
Duration: 400 
Order: 1
		Text Content :
 # Linux Networking Essentials

---

## 1. The TCP/IP Stack
Understanding how packets move through the system is foundational for debugging latency, packet loss, and connectivity issues in production environments.

### TCP vs UDP (Deep Dive)

**TCP (Transmission Control Protocol)**
- Connection-oriented (3-way handshake: SYN → SYN-ACK → ACK)
- Guarantees:
  - Ordered delivery
  - Retransmission on loss
  - Congestion control (CUBIC, Reno, BBR)
  - Flow control (sliding window)
- Trade-offs:
  - Higher latency
  - Head-of-line blocking
- Common Use Cases:
  - HTTP/HTTPS
  - SSH
  - Databases (Postgres, MySQL)
  - gRPC (over HTTP/2)

**UDP (User Datagram Protocol)**
- Connectionless
- No guarantees:
  - No retransmission
  - No ordering
  - No congestion control (handled at app layer if needed)
- Advantages:
  - Low latency
  - Minimal overhead
- Common Use Cases:
  - DNS
  - NTP
  - VoIP / WebRTC
  - Streaming (QUIC builds reliability on top of UDP)

---

## TCP/IP vs OSI Model Mapping

| OSI Layer | OSI Name        | TCP/IP Layer | Examples |
|----------|-----------------|--------------|----------|
| 7        | Application     | Application  | HTTP, HTTPS, DNS, SSH |
| 6        | Presentation    | Application  | TLS/SSL, Encoding |
| 5        | Session         | Application  | Sessions, Auth |
| 4        | Transport       | Transport    | TCP, UDP |
| 3        | Network         | Internet     | IP, ICMP |
| 2        | Data Link       | Network      | Ethernet, ARP |
| 1        | Physical        | Network      | Cables, NICs |

---

## The OSI Layers in Practice (DevOps View)

### 1. Physical Layer
- Responsible for bit transmission
- Components:
  - Ethernet cables (Cat5/6)
  - Fiber optics
  - NICs (eth0, ens5, wlan0)
- Common Issues:
  - Cable unplugged
  - Interface down
- Debug Commands:
  - `ip link`
  - `ethtool eth0`

---

### 2. Data Link Layer
- Handles frame delivery within the same network
- Uses MAC addresses
- Key Protocols:
  - Ethernet
  - ARP (IP → MAC resolution)
- Devices:
  - Switches
  - Bridges
- Debug Commands:
  - `arp -a`
  - `ip neigh`
  - `tcpdump -e`

---

### 3. Network Layer
- Responsible for routing packets across networks
- Uses IP addresses (IPv4 / IPv6)
- Key Protocols:
  - IP
  - ICMP (ping, traceroute)
  - Routing protocols (BGP, OSPF)
- Common Issues:
  - No route to host
  - Incorrect CIDR
- Debug Commands:
  - `ip route`
  - `traceroute`
  - `ping`

---

### 4. Transport Layer
- End-to-end communication between processes
- Uses ports (0–65535)
- TCP Concepts:
  - SYN backlog
  - TIME_WAIT
  - Retransmissions
- UDP Concepts:
  - Stateless delivery
- Debug Commands:
  - `ss -lntup`
  - `netstat -an`
  - `tcpdump port 443`

---

### 5. Application Layer
- User-facing protocols
- Handles request/response semantics
- Common Protocols:
  - HTTP/HTTPS
  - DNS
  - SSH
  - SMTP
- Common Issues:
  - 5xx errors
  - Timeouts
  - Misconfigured TLS
- Debug Commands:
  - `curl -v`
  - `dig`
  - `openssl s_client`

---

## Packet Journey (High-Level Flow)

1. Application generates data (HTTP request)
2. Transport layer assigns port + protocol (TCP 443)
3. Network layer assigns source/destination IP
4. Data link layer resolves MAC via ARP
5. Physical layer transmits bits
6. Reverse process on receiver side

---

## Why This Matters in Production

- Load balancer issues often occur at L4 vs L7
- Kubernetes networking spans:
  - L3 (Pod IP routing)
  - L4 (Services, kube-proxy)
  - L7 (Ingress, Service Mesh)
- Effective incident response requires knowing *which layer is failing*

---

## Quick Mental Model for Debugging

- **Can't connect at all?** → L1/L2/L3
- **Connection established but slow?** → L4
- **Connected but getting errors?** → L7


---

## 2. DNS (Domain Name System)
"It's always DNS."  
DNS is a distributed, hierarchical system that translates human-readable names into IP addresses. Most production outages attributed to “network issues” ultimately fail at DNS resolution or caching.

---

### DNS Resolution Order (Linux)

1. `/etc/hosts`
2. Local DNS cache (systemd-resolved / nscd)
3. Configured nameservers (`/etc/resolv.conf`)
4. Recursive resolver
5. Authoritative nameserver

This order is controlled by:
```bash
/etc/nsswitch.conf
hosts: files dns
````

---

### Key Configuration Files

**`/etc/hosts`**

* Static hostname → IP mappings
* Highest priority
* Commonly used for:

  * Local testing
  * Temporary overrides
* Risk:

  * Drift across nodes in a cluster

**`/etc/resolv.conf`**

* Defines DNS behavior for the system
* Typical fields:

  ```text
  nameserver 10.0.0.2
  search svc.cluster.local cluster.local
  options ndots:5 timeout:2 attempts:3
  ```
* In Kubernetes, this file is auto-managed per Pod

---

### Core DNS Record Types

| Record | Purpose              | Example                     |
| ------ | -------------------- | --------------------------- |
| A      | Hostname → IPv4      | example.com → 93.184.216.34 |
| AAAA   | Hostname → IPv6      | example.com → 2606:2800::   |
| CNAME  | Alias                | www → example.com           |
| MX     | Mail routing         | mail.example.com            |
| TXT    | Metadata             | SPF, DKIM                   |
| NS     | Authoritative server | ns1.example.com             |
| PTR    | Reverse lookup       | IP → name                   |

---

### DNS Query Types

* **Recursive Query**: Client expects full resolution
* **Iterative Query**: Resolver queries each level
* **Non-recursive Query**: Cached answer only

---

### DNS Caching and TTL

* TTL controls how long records are cached
* High TTL:

  * Faster lookups
  * Slower propagation
* Low TTL:

  * Faster failover
  * Higher DNS load
* Common outage pattern:

  * DNS change made
  * Old TTL still cached in clients or resolvers

---

## DNS Resolution Flow (example.com)

1. Client checks `/etc/hosts`
2. Queries recursive resolver
3. Root server (`.`)
4. TLD server (`.com`)
5. Authoritative server (`example.com`)
6. Response cached based on TTL

---

## DNS Troubleshooting (Extended)

```bash
# Query all records
dig example.com ANY

# Short answer (script-friendly)
dig +short example.com

# Trace DNS resolution path
dig +trace example.com

# Reverse lookup
dig -x 8.8.8.8

# Use specific nameserver
dig @1.1.1.1 example.com

# Check authoritative answer
dig example.com +norecurse

# Test TCP-based DNS (large responses)
dig +tcp example.com
```
---

## Common DNS Failure Scenarios

### 1. NXDOMAIN

* Record does not exist
* Causes:

  * Typo
  * Wrong environment (prod vs stage)
  * Deleted record

### 2. SERVFAIL

* Resolver failed
* Causes:

  * DNSSEC misconfiguration
  * Upstream resolver issue

### 3. Timeout

* No response
* Causes:

  * Firewall blocking UDP/53 or TCP/53
  * Broken routing

---

## Kubernetes DNS (CoreDNS)

* Runs as a Deployment
* Resolves:

  * Services: `my-svc.my-ns.svc.cluster.local`
  * Pods (optional)
* Common Issues:

  * CoreDNS pod crashloop
  * `ndots` misconfiguration
  * Excessive search domains
* Debug:

  ```bash
  kubectl exec -it pod -- cat /etc/resolv.conf
  kubectl logs -n kube-system deploy/coredns
  ```

---

## Load Balancers and DNS

* L7 Load balancers often rely on DNS
* Health-check-based failover depends on TTL
* Geo-DNS and weighted routing rely on resolvers honoring TTL

---

## Production Debug Checklist

* Does `/etc/hosts` override the record?
* Are correct nameservers configured?
* Is the record present and correct?
* Is TTL still cached?
* Is DNS reachable on UDP/TCP 53?
* Is Kubernetes CoreDNS healthy?

---

## Key Takeaway

If an application "can’t connect", verify DNS *before* debugging networking, TLS, or application logic.


---

## 3. Firewalls
Firewalls enforce network security by controlling traffic based on predefined rules. In production systems, they operate at multiple layers (L3/L4 primarily) and are often the silent cause of “connection refused” or “timeout” issues. Understanding host-level firewalls is critical before debugging cloud security groups, NACLs, or Kubernetes NetworkPolicies.

* **iptables:** The legacy, rule-based packet filtering framework built into the Linux kernel.
* **UFW (Uncomplicated Firewall):** A simplified interface for iptables, commonly used on Ubuntu.
* **firewalld:** A dynamic firewall manager using zones, standard on RHEL/CentOS/Rocky.

---

### Firewall Layers in Practice

| Layer | Example | Scope |
|-----|--------|------|
| L3 | IP allow/deny | Source/Destination IP |
| L4 | Port filtering | TCP/UDP ports |
| L7 | Proxy firewalls | HTTP methods, paths |

Host firewalls usually operate at **L3/L4**.

---

## iptables Architecture

iptables works using **tables**, **chains**, and **rules**.

### Tables
- **filter** (default): Allow/Deny traffic
- **nat**: Address translation (SNAT, DNAT)
- **mangle**: Packet modification
- **raw**: Connection tracking bypass

### Chains
- **INPUT**: Traffic destined for the host
- **OUTPUT**: Traffic originating from the host
- **FORWARD**: Traffic routed through the host

---

### Rule Evaluation Order

1. Rules are evaluated **top to bottom**
2. First match wins
3. Default policy applies if no rule matches

This makes rule ordering critical.

---

### Iptables Basics

```bash
# List rules (numeric, verbose)
iptables -L -n -v

# Show rules with line numbers
iptables -L --line-numbers

# Allow incoming HTTP traffic
iptables -A INPUT -p tcp --dport 80 -j ACCEPT

# Allow established connections
iptables -A INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT

# Block traffic from a specific IP
iptables -A INPUT -s 192.168.1.100 -j DROP

# Set default policy (dangerous over SSH)
iptables -P INPUT DROP
````

---

### Connection Tracking (Very Important)

iptables is **stateful** via `conntrack`.

Common states:

* NEW
* ESTABLISHED
* RELATED
* INVALID

Most production firewalls rely on:

```bash
-m conntrack --ctstate ESTABLISHED,RELATED
```


Without this, replies to outbound traffic may be dropped.

---

### Persisting Rules

iptables rules are **not persistent by default**.

```bash
iptables-save > /etc/iptables/rules.v4
iptables-restore < /etc/iptables/rules.v4
```

On Ubuntu:

```bash
apt install iptables-persistent
```

---

## UFW (Ubuntu)

* Opinionated defaults
* Automatically manages stateful rules
* Easier for single-node systems

```bash
ufw status verbose
ufw allow 22/tcp
ufw allow from 10.0.0.0/24 to any port 5432
ufw deny 8080
ufw enable
```

Limitations:

* Less granular than raw iptables
* Not ideal for complex routing or NAT

---

## firewalld (RHEL/CentOS)

* Zone-based model
* Dynamic rule updates (no connection drops)
* Backend: nftables or iptables

```bash
firewall-cmd --get-active-zones
firewall-cmd --list-all
firewall-cmd --add-port=443/tcp --permanent
firewall-cmd --reload
```

Zones example:

* public
* internal
* trusted

---

## Common Firewall Failure Patterns

### 1. Service Running but Not Reachable

* Port blocked in INPUT chain
* Missing ESTABLISHED rule

### 2. Works Locally, Fails Remotely

* OUTPUT allowed, INPUT blocked
* NAT misconfiguration

### 3. Kubernetes NodePort Issues

* Host firewall blocking NodePort range (30000–32767)

---

## Debugging Checklist

```bash
# Check listening ports
ss -lntup

# Verify firewall counters
iptables -L -n -v

# Capture dropped packets
tcpdump -i eth0 port 443

# Temporarily flush rules (dangerous)
iptables -F
```

---

## Production Takeaway

Before blaming cloud firewalls or Kubernetes networking:

1. Verify host firewall rules
2. Check rule order and default policy
3. Confirm stateful connection tracking

Most “network outages” are firewall misconfigurations hiding in plain sight.

---

## 4. Network Interfaces
* **`ip addr`:** Show IP addresses
* **`ip route`:** Display routing table
* **`ip link`:** Manage network interfaces

### Static IP Configuration
```bash
# /etc/netplan/01-netcfg.yaml (Ubuntu)
network:
  version: 2
  ethernets:
    eth0:
      addresses: [10.0.1.100/24]
      gateway4: 10.0.1.1
      nameservers:
        addresses: [8.8.8.8, 1.1.1.1]
```

Class 2.4.2:
	Title: Network Troubleshooting
	Description: Diagnosing connectivity issues.
Content Type: text
Duration: 350 
Order: 2
		Text Content :
 # Network Troubleshooting Tools

---

## 1. Connectivity Checks
* **`ping`:** Checks reachability (ICMP). *Note: AWS Security Groups often block ICMP, so a failed ping doesn't always mean the server is down.*
* **`telnet` / `nc` (Netcat):** Checks if a specific **port** is open.
    * `nc -zv google.com 443` -> "Connection to google.com 443 port [tcp/https] succeeded!"

### Port Scanning
```bash
# Single port
nc -zv host 22

# Port range
nc -zv host 20-25

# UDP port
nc -zvu host 53
```

---

## 2. Path Analysis
* **`traceroute` / `mtr`:** Shows every "hop" (router) between you and the server. Used to identify where latency is introduced or where packets are being dropped.

### MTR Advanced Usage
```bash
# Continuous monitoring
mtr --report --report-cycles 100 google.com

# Show both hostnames and IPs
mtr -b google.com

# UDP mode
mtr -u google.com
```

---

## 3. Packet Capture
* **`tcpdump`:** Captures raw traffic.
    * `tcpdump -i eth0 port 80`
    * Essential for debugging HTTP headers, SSL handshakes, and database connection strings when everything else fails.

### Tcpdump Filters
```bash
# Capture HTTP traffic
tcpdump -i eth0 -A 'tcp port 80'

# Capture to file
tcpdump -i eth0 -w capture.pcap

# Read from file
tcpdump -r capture.pcap

# Filter by host
tcpdump host 192.168.1.100

# Complex filters
tcpdump 'tcp[tcpflags] & (tcp-syn) != 0'
```

---

## 4. SSL/TLS Debugging
```bash
# Test SSL connection
openssl s_client -connect example.com:443

# Check certificate
openssl x509 -in cert.pem -text -noout

# Test specific SSL version
openssl s_client -connect host:443 -tls1_2
```

Class 2.4.3:
	Title: Network Security Best Practices
	Description: Securing Linux systems at the network level.
Content Type: text
Duration: 300 
Order: 3
		Text Content :
 # Network Security Best Practices

---

## 1. SSH Hardening
* **Disable root login:** `PermitRootLogin no`
* **Key-only authentication:** `PasswordAuthentication no`
* **Change default port:** `Port 2222` (security through obscurity)
* **Limit users:** `AllowUsers deploy admin`

### SSH Configuration
```bash
# /etc/ssh/sshd_config
Port 22
PermitRootLogin no
PasswordAuthentication no
PubkeyAuthentication yes
AllowUsers deploy
MaxAuthTries 3
ClientAliveInterval 300
ClientAliveCountMax 2
```

---

## 2. Port Management
* **Principle:** Only open ports that are absolutely necessary
* **Regular Audits:** `netstat -tulpn` or `ss -tulpn`
* **Disable Unused Services:** `systemctl disable service_name`

---

## 3. Network Segmentation
* **DMZ:** Public-facing servers isolated from internal network
* **VLANs:** Logical network separation
* **Security Groups:** Cloud-native firewall rules

---

## 4. Intrusion Detection
* **fail2ban:** Automatic IP blocking after failed login attempts
* **AIDE:** File integrity monitoring
* **Tripwire:** System integrity checker
---
Topic 2.5:
Title: Linux Challenge
Order: 5

Class 2.5.1:
	Title: Linux Challenge
	Description: A contest to test your command line and troubleshooting
Content Type: text
Duration: 300 
Order: 1
		Text Content :
# Linux Fundamentals – Challenge
Below are **5 challenge-style questions** aligned exactly with the syllabus, each followed by a **model answer** that reflects real-world Linux troubleshooting and interview expectations.

---

## Question 1: Process Management & Signal Handling

### Problem  
A production application (`app.sh`) is consuming high CPU and has become unresponsive. A normal `kill` command does not stop it.

**Tasks:**
1. Identify the process.
2. Attempt a graceful shutdown.
3. Forcefully terminate it if required.
4. Explain the difference between the signals used.

---

### Answer

**Step 1: Identify the process**
`ps aux | grep app.sh`

or (better):

```bash
pgrep -fl app.sh
```

**Step 2: Graceful termination (SIGTERM)**

```bash
kill <PID>
```

or explicitly:

```bash
kill -15 <PID>
```

**Step 3: Force kill if unresponsive (SIGKILL)**

```bash
kill -9 <PID>
```

**Explanation**

* `SIGTERM (15)`: Politely asks the process to exit and clean up resources.
* `SIGKILL (9)`: Immediately kills the process at kernel level; no cleanup.
* Best practice is always **TERM first, KILL last**.

---

## Question 2: File Permissions & Ownership

### Problem

A deployment script fails with `Permission denied` when trying to write to `/var/log/myapp.log`.

**Tasks:**

1. Identify the permission issue.
2. Fix ownership so user `deploy` can write.
3. Set permissions to allow owner read/write, group read-only.

---

### Answer

**Step 1: Check permissions**

```bash
ls -l /var/log/myapp.log
```

**Step 2: Change ownership**

```bash
chown deploy:deploy /var/log/myapp.log
```

**Step 3: Set correct permissions**

```bash
chmod 640 /var/log/myapp.log
```

**Explanation**

* `6` (owner): read + write
* `4` (group): read
* `0` (others): no access
  Principle of **least privilege** is maintained.

---

## Question 3: Log Analysis & Grep Patterns

### Problem

You are investigating a production outage. Logs are stored in `/var/log/app.log`.

**Tasks:**

1. Find all ERROR lines.
2. Extract ERROR lines from the last 10 minutes.
3. Count unique error messages.

---

### Answer

**Step 1: Find ERROR lines**

```bash
grep "ERROR" /var/log/app.log
```

**Step 2: Filter last 10 minutes (assuming timestamped logs)**

```bash
grep "ERROR" /var/log/app.log | grep "$(date --date='10 minutes ago' '+%Y-%m-%d %H:%M')"
```

**Step 3: Count unique error messages**

```bash
grep "ERROR" /var/log/app.log | awk '{$1=$2=""; print}' | sort | uniq -c
```

**Explanation**

* `grep` filters
* `awk` removes timestamps
* `sort | uniq -c` aggregates error frequency

---

## Question 4: Network Troubleshooting with netcat & tcpdump

### Problem

Your application cannot connect to a backend service on port `5432`.

**Tasks:**

1. Verify if the port is reachable.
2. Listen on the port to confirm incoming traffic.
3. Capture packets for debugging.

---

### Answer

**Step 1: Test connectivity using netcat**

```bash
nc -zv backend-server 5432
```

**Step 2: Listen on the port**

```bash
nc -l 5432
```

**Step 3: Capture traffic**

```bash
tcpdump -i eth0 port 5432
```

**Explanation**

* `nc -zv`: Checks port reachability
* `nc -l`: Confirms traffic arrival
* `tcpdump`: Validates packet flow and helps identify drops/firewalls

---

## Question 5: System Resource Monitoring & Performance Tuning

### Problem

A Linux server is slow during peak hours.

**Tasks:**

1. Identify CPU and memory usage.
2. Detect disk I/O bottlenecks.
3. Identify the top resource-hungry process.

---

### Answer

**Step 1: CPU & Memory**

```bash
top
```

or:

```bash
htop
```

**Step 2: Disk I/O**

```bash
iostat -xz 1
```

**Step 3: Identify heavy process**

```bash
ps aux --sort=-%mem | head
```

or:

```bash
ps aux --sort=-%cpu | head
```

**Explanation**

* High `%wa` in `iostat` indicates disk wait
* High load average with low CPU usage implies I/O bottleneck
* Sorting processes helps pinpoint the root cause quickly

---

## Evaluation Criteria (Contest Style)

* Correct command usage
* Logical troubleshooting order
* Understanding of *why*, not just *how*
* Production-safe practices

This challenge reflects **real on-call Linux scenarios**, not textbook questions.

---

Module 3:
Title: Cloud Infrastructure & Services
Description: Master cloud platforms (AWS, GCP, Azure) with focus on compute, networking, storage, and managed services. Learn to design scalable, resilient, and cost-effective cloud architectures.
Order: 3
Learning Outcomes:
Master AWS core services (EC2, VPC, RDS, ECS, EKS, Route53)
Understand multi-cloud concepts (GCP, Azure)
Design high-availability architectures
Optimize cloud costs

Topic 3.1:
Title: AWS Core Services
Order: 1

Class 3.1.1:
	Title: AWS Foundation & Global Infrastructure
	Description: Regions, AZs, IAM, and the CLI.
Content Type: text
Duration: 300 
Order: 1
		Text Content :
 # AWS Foundation: The Bedrock

## 1. Global Infrastructure
When you deploy an application, you must physically place it somewhere. AWS organizes the world into **Regions** and **Availability Zones (AZs)**.

* **Region (e.g., us-east-1):** A separate geographic area. Each region is completely independent (separate power, separate water, separate network).
* **Availability Zone (e.g., us-east-1a):** A discrete data center *within* a region. They are connected by low-latency fiber.
    * **The Golden Rule:** Always deploy across at least **2 AZs** for High Availability. If one data center burns down, your app survives in the other.
* **Edge Locations:** Small data centers used for CloudFront (CDN) to cache content closer to users (e.g., a Netflix movie cached in Mumbai for Indian users).

---

## 2. IAM (Identity and Access Management)
IAM is the security bouncer of AWS.
* **Users:** Real people (DevOps Engineer) or Services.
* **Groups:** Collections of users (e.g., "Developers").
* **Roles:** Temporary hats that services wear.
    * *Scenario:* An EC2 instance needs to upload a file to S3. You do NOT save credentials on the server. You assign an **IAM Role** to the EC2 instance.
* **Policies:** JSON documents defining permissions.
    * **Least Privilege:** Only give the exact permissions needed. Never use `AdministratorAccess` for an application role.

---

## 3. AWS CLI
You won't always have a GUI.
* `aws configure`: Sets up your credentials (`ACCESS_KEY`, `SECRET_KEY`).
* `aws s3 ls`: Lists storage buckets.
* **Profile Management:** Use `--profile` to switch between Dev and Prod accounts safely.

Class 3.1.2:
	Title: EC2 - Elastic Compute Cloud
	Description: Instance types, security groups, and AMIs.
Content Type: text
Duration: 450 
Order: 2
		Text Content :
 # EC2: The Virtual Servers

## 1. Instance Types & Families
Not all servers are created equal.
* **General Purpose (T3, M5):** Balanced CPU/RAM. Good for web servers.
* **Compute Optimized (C5):** High CPU ratio. Good for batch processing/video encoding.
* **Memory Optimized (R5):** High RAM. Good for Databases (Redis, Postgres).
* **Burstable (T-series):** Cheap, but performance is throttled if you use too much CPU. Great for dev environments, dangerous for production.

---

## 2. AMIs (Amazon Machine Images)
An AMI is a blueprint of your server (OS + Pre-installed Software).
* **Golden AMI Strategy:** Instead of installing Nginx every time a server starts (slow), you bake Nginx into a custom AMI. This reduces boot time from 10 minutes to 30 seconds.

---

## 3. Security Groups (The Firewall)
A Security Group acts as a virtual firewall at the **instance level**.
* **Stateful:** If you allow traffic OUT, the response is automatically allowed IN.
* **Common Config:**
    * Allow port 80/443 from `0.0.0.0/0` (The World).
    * Allow port 22 (SSH) ONLY from your Office VPN IP.

Class 3.1.3:
	Title: VPC - Virtual Private Cloud
	Description: Networking, Subnets, and Gateways.
Content Type: text
Duration: 500 
Order: 3
		Text Content :
# VPC: The Network Backbone

A Virtual Private Cloud (VPC) is a logically isolated network within a cloud provider that gives you full control over IP addressing, routing, and network security. In production environments, the VPC design directly impacts scalability, blast radius, latency, and cost.

---

## 1. VPC Architecture
Think of a VPC as your own private slice of the cloud, backed by provider-managed networking fabric.

* **CIDR Block:** Defines the IP address range for the VPC (e.g., `10.0.0.0/16`)
  - Cannot be changed after creation
  - Must not overlap with:
    - On-prem networks
    - Peered VPCs
    - Transit networks
* **Primary Design Goal:** Provide network isolation while enabling controlled connectivity.

**Best Practices**
- Use RFC1918 ranges (`10.0.0.0/8`, `172.16.0.0/12`, `192.168.0.0/16`)
- Allocate large CIDRs early to avoid IP exhaustion
- Plan for multi-AZ expansion

---

## 2. Subnets: Public vs Private
Subnets partition a VPC CIDR into smaller ranges and are typically mapped to a single Availability Zone.

### Public Subnet
A subnet is considered *public* if:
- Its route table contains a route to an **Internet Gateway (IGW)**

**Typical Use Cases**
- Application Load Balancers
- NAT Gateways
- Bastion hosts

**Security Notes**
- Instances still require:
  - Public IP or Elastic IP
  - Security Group rules allowing ingress

---

### Private Subnet
A subnet is considered *private* if:
- It has **no route** to an Internet Gateway

**Typical Use Cases**
- Application servers
- Databases
- Internal services

**Security Advantages**
- No direct inbound internet access
- Reduced attack surface
- Mandatory for compliance-heavy workloads

---

### Public vs Private Subnet Comparison

| Aspect | Public Subnet | Private Subnet |
|-----|--------------|----------------|
| Internet Gateway Route | Yes (`0.0.0.0/0 → IGW`) | No |
| Direct Internet Access | Allowed (with public/elastic IP) | Not allowed |
| Inbound Internet Traffic | Possible (controlled by SG/NACL) | Not possible |
| Outbound Internet Traffic | Direct via IGW | Via NAT Gateway / NAT Instance |
| Typical Workloads | Load Balancers, Bastion Hosts, NAT Gateway | App Servers, Databases, Internal Services |
| Public IP Assignment | Required for internet access | Not used |
| Security Exposure | Higher | Lower |
| Compliance Suitability | Limited | Preferred |
| Route Table Role | Enables public reachability | Enforces isolation |
| Common Misuse | Hosting app servers directly | Forgetting NAT for updates |

---

## 3. Gateways

### Internet Gateway (IGW)
- Horizontally scalable, managed service
- Enables:
  - Inbound internet traffic to public subnets
  - Outbound traffic from public instances
- One IGW per VPC

**Common Misconception**
- Attaching an IGW does *not* make instances public by default  
  Routing + Public IP + Security Group are all required.

---

### NAT Gateway
Allows **private subnet** instances to access the internet **outbound only**.

**Key Characteristics**
- Managed, highly available within an AZ
- Requires:
  - Deployment in a public subnet
  - Route in private subnet pointing to it

**Typical Use Cases**
- OS package updates
- Pulling container images
- External API calls

**Cost Considerations**
- Charged per hour + per GB processed
- Can become expensive at scale
- Alternative:
  - NAT instance (more control, more ops overhead)

---

### Internet Gateway vs NAT Gateway

| Aspect | Internet Gateway (IGW) | NAT Gateway |
|-----|------------------------|-------------|
| Primary Purpose | Enable direct internet access | Enable outbound-only internet access |
| Traffic Direction | Inbound and Outbound | Outbound only |
| Used By | Public Subnets | Private Subnets |
| Requires Public IP | Yes | No (instances remain private) |
| Placement | Attached to VPC | Deployed in a public subnet |
| Route Target | `0.0.0.0/0 → IGW` | `0.0.0.0/0 → NAT Gateway` |
| Internet-Initiated Access | Allowed (controlled by SG/NACL) | Not allowed |
| Common Use Cases | ALB, Bastion, Public EC2 | OS updates, image pulls, API calls |
| Availability | Highly available (managed) | AZ-scoped (one per AZ recommended) |
| Cost Model | No hourly charge | Hourly + per-GB processing |
| Security Impact | Increases exposure | Preserves isolation |
| Failure Blast Radius | VPC-wide | AZ-level |
| Typical Misconfiguration | Assuming attachment makes subnet public | Single NAT for multi-AZ workloads |
| Operational Overhead | Minimal | Moderate (cost + scaling considerations) |
 ---


## 4. Routing (The Hidden Core)

Each subnet is associated with a **route table**.

Example:
```text
0.0.0.0/0 -> igw-xxxx        (public subnet)
0.0.0.0/0 -> nat-xxxx        (private subnet)
10.0.0.0/16 -> local
````

Routing defines *reachability*, not security.

---

## 5. VPC Peering

VPC Peering creates a private, point-to-point connection between two VPCs.

**Key Properties**

* Traffic stays on the provider’s private network
* No bandwidth bottleneck (uses underlying fabric)
* CIDR blocks must not overlap

**Limitations**

* Non-transitive:

  * A ↔ B and B ↔ C does **not** imply A ↔ C
* No centralized routing
* Can become complex at scale

**Common Use Cases**

* Dev ↔ Prod connectivity
* Shared services VPC
* Cross-account communication

---

## 6. Common Design Pitfalls

* Small CIDR leading to IP exhaustion
* Public subnets used for application servers
* Single NAT Gateway for multi-AZ workloads
* Overusing VPC peering instead of Transit Gateway

---

## 7. Production Takeaway

* VPC design is foundational and hard to change later
* Public vs Private is a **routing decision**, not a security one
* Gateways control traffic direction, not permissions
* Poor VPC architecture leads to fragile, expensive systems

Design the VPC first—everything else depends on it.


---

Class 3.1.4:
	Title: RDS - Relational Database Service
	Description: Managed databases and replication strategies.
Content Type: text
Duration: 400 
Order: 4
		Text Content :
# RDS: Managed Databases

Amazon RDS (Relational Database Service) provides managed relational databases where AWS takes responsibility for infrastructure-level concerns, allowing teams to focus on data modeling, queries, and application logic. Understanding *what AWS manages vs what you still own* is critical for designing reliable and cost-effective systems.

---

## 1. Why Managed?

Managed databases abstract away the undifferentiated heavy lifting of running databases in production.

### What AWS Manages
- **Provisioning & Hardware**
  - Instance lifecycle
  - Disk failures
  - Underlying host replacement
- **Automated Backups**
  - Point-in-time recovery (PITR)
  - Transaction log backups
  - Configurable retention window
- **Patching**
  - OS patching
  - Database engine minor version updates (optional automation)
- **High Availability Primitives**
  - Multi-AZ orchestration
  - Failure detection and recovery

### What You Still Manage
- Schema design and migrations
- Indexing and query performance
- Connection pooling
- Capacity planning (instance class, IOPS)
- Major version upgrades

**Key Insight**  
RDS is *not* “serverless databases”. Poor schema design or inefficient queries will still bring down your application.

---

## 2. Multi-AZ Deployments (High Availability & DR)

Multi-AZ is designed for **availability**, not scaling.

### Architecture
- One **Primary** instance
- One **Standby** instance in a different AZ
- Storage is synchronously replicated

### Synchronous Replication
- Writes are acknowledged only after:
  - Primary writes to storage
  - Standby confirms the write
- Guarantees:
  - Zero data loss during AZ failure
  - Strong consistency

### Automatic Failover
- AWS continuously monitors the Primary
- On failure:
  1. Standby is promoted to Primary
  2. RDS updates the **DB endpoint DNS**
  3. Applications reconnect automatically

**Failover Time**
- Typically 30–120 seconds
- Application must handle reconnects

### Performance Consideration
- Slight write latency increase due to sync replication
- Reads still go only to the Primary

**Important Limitation**
- Standby is **not readable**
- Cannot offload traffic to Standby

**Use When**
- Production databases
- Low RPO/RTO requirements
- Mission-critical workloads

---

## 3. Read Replicas (Horizontal Scaling)

Read Replicas are designed for **scaling reads**, not failover.

### Asynchronous Replication
- Primary commits first
- Replica catches up later
- Replica lag can range from milliseconds to seconds

### Typical Use Cases
- Reporting and analytics
- BI dashboards
- Read-heavy APIs
- Offloading long-running queries

### Key Characteristics
- Replicas are **read-only**
- Replication lag is workload-dependent
- Replicas can be:
  - In the same AZ
  - Cross-AZ
  - Cross-Region

### Failure Behavior
- Read Replicas do **not** automatically promote
- Manual promotion possible during disasters
- Promotion breaks replication

**Consistency Trade-off**
- Applications must tolerate stale reads
- Not suitable for strong consistency requirements

---

## 4. Multi-AZ vs Read Replicas (Mental Model)

| Feature | Multi-AZ | Read Replica |
|------|--------|--------------|
| Purpose | High Availability | Read Scaling |
| Replication | Synchronous | Asynchronous |
| Read Traffic | No | Yes |
| Failover | Automatic | Manual |
| Data Loss Risk | None | Possible |
| Standby Readable | No | Yes |

---

## 5. Amazon Aurora (Cloud-Native RDS)

Aurora is not just “RDS but faster”; it is architecturally different.

### Decoupled Architecture
- **Compute**:
  - DB instances (writers/readers)
- **Storage**:
  - Distributed, replicated across **3 AZs**
  - 6 copies of data (2 per AZ)

### Storage Auto-Scaling
- Grows automatically in 10 GB increments
- Up to 128 TB
- No capacity planning for disk

### High Performance Replication
- Writes go to a shared storage layer
- Read replicas read from the same storage
- Replica lag typically <10ms

### Read Scalability
- Up to **15 Read Replicas**
- Aurora Replica Auto Scaling supported
- Reader endpoint for load balancing

### Availability & Recovery
- No traditional standby
- Any replica can be promoted in seconds
- Faster failover than standard RDS

---

## 6. Aurora vs Standard RDS

| Aspect | RDS (MySQL/Postgres) | Aurora |
|-----|----------------------|--------|
| Storage | Instance-attached | Distributed |
| Replication | DB-level | Storage-level |
| Read Replicas | Limited, slower | Up to 15, fast |
| Failover | Slower | Faster |
| Cost | Lower | Higher |
| Operational Simplicity | Moderate | High |

---

## 7. Production Design Guidance

- Enable **Multi-AZ** for all production databases
- Use **Read Replicas** for:
  - Analytics
  - Dashboards
  - Heavy read traffic
- Choose **Aurora** when:
  - You need high read scale
  - Fast failover is critical
  - Storage auto-scaling matters

---

## Key Takeaway

- Multi-AZ protects availability
- Read Replicas protect performance
- Aurora optimizes both—but at higher cost

Design databases around **failure first**, not just throughput.


---

Class 3.1.5:
	Title: Route53 - DNS Management
	Description: Routing policies and health checks.
Content Type: text
Duration: 350 
Order: 5
		Text Content :
# Route 53: The Phonebook of the Cloud

Amazon Route 53 is AWS’s highly available, globally distributed DNS service. Beyond basic name resolution, it provides intelligent traffic routing, health checking, and tight integration with AWS resources. In production architectures, Route 53 often becomes a critical control plane for availability and disaster recovery.

---

## 1. Hosted Zones

A **Hosted Zone** is a container for DNS records for a domain.

### Public Hosted Zone
A Public Hosted Zone is used when the domain must be resolvable from the public internet.

- Used for internet-facing applications (`example.com`)
- Records are accessible by any DNS resolver globally
- Common record targets:
  - Application Load Balancers
  - CloudFront distributions
  - Public IPs

### Private Hosted Zone
A Private Hosted Zone is only resolvable from within one or more VPCs.

- Used for internal services (`db.internal`, `api.service.local`)
- Not reachable from the public internet
- Often paired with:
  - RDS endpoints
  - Internal load balancers
  - Service-to-service communication

**Key Design Insight**  
Private Hosted Zones allow internal DNS without exposing infrastructure publicly, reducing both attack surface and operational risk.

---

## 2. Routing Policies

Route 53 supports multiple routing policies that control how DNS responses are returned. These are evaluated at query time by AWS’s global DNS infrastructure.

### Simple Routing
Returns a single record.

- No health checks
- Used for:
  - Static sites
  - Single-endpoint services

---

### Weighted Routing
Distributes traffic based on assigned percentages.

- Commonly used for:
  - Canary deployments
  - Gradual rollouts
  - A/B testing
- Enables controlled exposure of new versions without changing application logic

---

### Latency-Based Routing
Routes users to the AWS region with the lowest latency.

- Uses AWS latency measurements
- Ideal for:
  - Global applications
  - Multi-region architectures
- Improves user experience without application-level geo logic

---

### Failover Routing
Automatically redirects traffic when health checks fail.

- Requires Route 53 health checks
- Typical pattern:
  - Primary (active)
  - Secondary (standby / DR)
- Often used in:
  - Active–Passive DR setups
  - Regional failover strategies

**Operational Note**  
Failover depends on DNS TTL and client caching behavior.

---

## 3. Alias Records

Alias Records are an AWS-specific DNS extension.

- Used instead of CNAMEs for AWS resources
- Supported targets:
  - ELB / ALB / NLB
  - CloudFront
  - S3 static websites
- Key advantages:
  - No extra DNS lookup
  - No DNS query charges
  - Supports zone apex (`example.com`)

**Best Practice**  
Always use Alias Records when pointing to AWS-managed services. They are more efficient and integrate directly with AWS health and scaling behavior.

---

## Production Takeaway

- Use Public Hosted Zones for internet-facing services
- Use Private Hosted Zones for internal-only communication
- Choose routing policies based on deployment and availability strategy
- Prefer Alias Records for AWS resources to reduce latency and cost

**Route 53** is not just DNS—it is a traffic control system for cloud-native architectures.


---

Topic 3.2:
Title: AWS Container & Serverless Services
Order: 2

Class 3.2.1:
	Title: ECS - Elastic Container Service
	Description: Running Docker containers on AWS.
Content Type: text
Duration: 450 
Order: 1
		Text Content :
# ECS: Docker at Scale

Amazon Elastic Container Service (ECS) is AWS’s native container orchestration platform. It focuses on simplicity and deep AWS integration rather than portability. ECS abstracts scheduling, placement, and lifecycle management of containers while letting you choose how much infrastructure control you want.

---

## 1. Architecture

ECS is built around a small number of core primitives that clearly separate *what to run* from *how it runs*.

### Task Definition
A **Task Definition** is the immutable blueprint for running containers.

It defines:
- Docker image and tag
- CPU and memory requirements
- Environment variables and secrets (SSM / Secrets Manager)
- Port mappings
- Logging configuration (CloudWatch)
- IAM Role (Task Role vs Execution Role)

A task definition revision is versioned. Updating a service always means deploying a **new revision**, never mutating an existing one.

**Key Insight**  
Task Definitions are equivalent to a Kubernetes Pod spec. They define *runtime intent*, not availability.

---

### Service
An **ECS Service** ensures availability.

Responsibilities:
- Maintains a desired number of running tasks
- Replaces failed or unhealthy tasks
- Integrates with:
  - Application Load Balancers
  - Auto Scaling policies
  - Deployment strategies (rolling, blue/green via CodeDeploy)

Services are long-running constructs. If a task exits or a host dies, the service scheduler immediately replaces it.

---

### Cluster
A **Cluster** is a logical boundary for scheduling.

- Groups:
  - EC2 instances (EC2 launch type), or
  - Capacity managed entirely by AWS (Fargate)
- Provides isolation for:
  - Networking
  - Resource placement
  - IAM permissions

A cluster does *not* automatically imply physical separation. It is primarily a scheduling and management boundary.

---

## 2. Launch Types: EC2 vs Fargate

ECS supports two execution models, each with distinct trade-offs.

### EC2 Launch Type
With EC2 launch type, you manage the worker nodes.

Characteristics:
- You provision EC2 instances into the cluster
- ECS schedules tasks onto available instances
- You pay for EC2 instances regardless of task utilization

Advantages:
- Cost-efficient for steady, predictable workloads
- Full control over:
  - Instance types
  - AMIs
  - Local storage
  - GPU workloads

Operational Responsibilities:
- Capacity planning
- OS patching
- Auto Scaling Groups
- Instance draining during deployments

---

### Fargate (Serverless)
With Fargate, AWS manages the compute layer.

Characteristics:
- You specify CPU and memory per task
- No EC2 instances to manage
- Tasks run in isolated AWS-managed infrastructure
- Billed per second based on resource allocation

Advantages:
- Zero host management
- Strong isolation between workloads
- Ideal for:
  - Bursty traffic
  - Event-driven workloads
  - Small teams or fast-moving environments

Trade-offs:
- Higher per-unit cost
- Limited customization
- No access to underlying host

**Decision Rule**
- Predictable, high utilization → EC2
- Variable, spiky, or low-ops → Fargate

---

## 3. Auto Scaling

ECS Auto Scaling adjusts the number of running tasks based on demand. It operates at the **service level**, not the cluster level.

### Scaling Signals

#### CPU Utilization
Scales tasks based on average CPU usage across running tasks.

- Best for compute-bound workloads
- Simple and predictable
- Reactive rather than proactive

Example logic:
`If average CPU > 70% for 5 minutes, add tasks`

---

#### ALB Request Count
Scales based on incoming traffic.

- Uses requests per target from ALB
- Ideal for web services
- Scales *before* CPU saturation

Example logic: `If requests per target > 1000/min, scale out`

---

### Scaling Behavior
- Scaling policies define:
  - Minimum tasks
  - Maximum tasks
  - Cooldown periods
- ECS replaces tasks gracefully during scale-in
- Works seamlessly with both EC2 and Fargate

**Important Note**
Auto Scaling does not replace load testing. Poor scaling thresholds can cause:
- Thrashing
- Slow recovery
- Cost spikes

---

## Production Takeaway

- Task Definitions define *what runs*
- Services define *how many must run*
- Clusters define *where they run*
- EC2 gives control and cost efficiency
- Fargate gives speed and low operational overhead
- Auto Scaling ties application load to infrastructure capacity

ECS excels when tight AWS integration and operational simplicity matter more than orchestration portability.


---

Class 3.2.2:
	Title: EKS - Elastic Kubernetes Service
	Description: Managed Kubernetes control plane.
Content Type: text
Duration: 450 
Order: 2
		Text Content :
# EKS: Kubernetes the AWS Way

Amazon Elastic Kubernetes Service (EKS) is AWS’s managed Kubernetes offering. It preserves upstream Kubernetes behavior while offloading control-plane reliability and tightly integrating IAM, networking, and compute with AWS primitives.

---

## 1. The Control Plane

In self-managed Kubernetes, the control plane (API server, scheduler, controller manager, etcd) is your responsibility. If it becomes unavailable, the cluster cannot accept changes and may lose state.

With EKS, AWS fully manages the control plane.

- The API server and etcd are deployed across **three Availability Zones**
- AWS handles:
  - High availability
  - Patching
  - Version upgrades
  - etcd backups and recovery
- The control plane is isolated from your VPC
- You cannot SSH into it or modify its configuration directly

**Operational Impact**  
Worker nodes and running pods continue operating even if the API server is temporarily unreachable. This removes an entire class of operational failure that commonly affects self-managed clusters.

---

## 2. Node Groups

EKS separates cluster management from compute management. You choose how worker capacity is provided.

### Managed Node Groups
Managed Node Groups are EC2-based worker nodes operated by AWS.

- AWS provisions and manages the EC2 instances
- Automated:
  - OS patching
  - Kubernetes version alignment
  - Rolling upgrades
- Integrated with Auto Scaling Groups
- Full EC2 flexibility remains:
  - Instance types
  - GPUs
  - Local storage

These are the default choice for most production clusters due to balance between control and automation.

---

### Fargate Profiles
Fargate Profiles allow Kubernetes pods to run without managing EC2 nodes.

- Pods are scheduled onto AWS-managed infrastructure
- No node visibility or node management
- Strong workload isolation
- Pod-level billing for CPU and memory

This model works best for:
- Low-volume workloads
- Event-driven jobs
- Teams that want minimal cluster operations

Limitations include reduced control over networking and lack of support for certain daemon-level workloads.

---

## 3. IAM Roles for Service Accounts (IRSA)

By default, Kubernetes pods inherit AWS permissions from the underlying EC2 node. This creates an over-privilege risk.

IRSA solves this by binding AWS IAM directly to Kubernetes identities.

- Each Kubernetes Service Account can be mapped to a unique IAM Role
- AWS uses OIDC federation to issue temporary credentials
- Only pods using that Service Account receive the permissions

**Security Benefit**  
This enforces least privilege at the pod level, eliminating the need to grant broad permissions to entire nodes.

**Production Implication**  
IRSA is the recommended and secure way to allow pods to access AWS services such as S3, DynamoDB, or SQS.

---

## Production Takeaway

- EKS removes control-plane operational risk
- Node Groups define how compute is managed
- Fargate removes node management entirely at the cost of flexibility
- IRSA is essential for secure, least-privilege access to AWS services

EKS works best when you want upstream Kubernetes with AWS-managed reliability and security primitives.

---

Class 3.2.3:
	Title: Load Balancing in AWS
	Description: Distributing traffic with ELB.
Content Type: text
Duration: 350 
Order: 3
		Text Content :
# Elastic Load Balancing (ELB)

Elastic Load Balancing distributes incoming traffic across multiple targets to improve availability, fault tolerance, and scalability. AWS provides different load balancer types optimized for specific layers of the network stack.

---

## 1. Application Load Balancer (ALB) – Layer 7

The Application Load Balancer operates at the **application layer** and understands HTTP and HTTPS semantics.

ALB can inspect:
- HTTP methods
- URLs and paths
- Host headers
- HTTP status codes

This enables advanced routing decisions without application changes.

**Routing Capabilities**
- Path-based routing allows traffic segregation within the same domain.
  - `/api` → backend service
  - `/images` → S3 or a different service
- Host-based routing enables multiple services on the same load balancer.
  - `api.example.com` → Service A
  - `app.example.com` → Service B

**Common Use Cases**
- Microservices architectures
- Kubernetes Ingress
- Blue/Green and Canary deployments
- Web applications requiring TLS termination

**Operational Notes**
- Terminates TLS
- Integrates deeply with:
  - ECS
  - EKS
  - AWS WAF

---

## 2. Network Load Balancer (NLB) – Layer 4

The Network Load Balancer operates at the **transport layer**, forwarding raw TCP or UDP connections.

It does not inspect application payloads and focuses on speed and connection throughput.

**Key Characteristics**
- Extremely low latency
- Capable of handling millions of requests per second
- Preserves client source IP
- Provides static IP addresses per AZ

**Critical Advantage**
Static IPs make NLB suitable for:
- Firewall whitelisting
- Legacy systems requiring fixed endpoints
- Non-HTTP protocols

**Common Use Cases**
- gRPC over TCP
- Databases and stateful services
- Real-time systems
- PrivateLink endpoints

---

## 3. Target Groups

Load balancers route traffic to **Target Groups**, not directly to compute resources.

A Target Group defines:
- Target type:
  - EC2 instances
  - IP addresses
  - Lambda functions (ALB only)
- Health check configuration
- Port and protocol

**Health Checks**
- Periodically probes a defined endpoint
- Removes unhealthy targets automatically
- Prevents traffic from reaching failing instances

**Impact**
Health checks are the first line of defense against partial outages and misbehaving deployments.

---

## ALB vs NLB Comparison

| Aspect | Application Load Balancer (ALB) | Network Load Balancer (NLB) |
|-----|---------------------------------|-----------------------------|
| OSI Layer | Layer 7 | Layer 4 |
| Protocols | HTTP, HTTPS | TCP, UDP |
| Routing Logic | Path-based, Host-based | Port-based only |
| TLS Termination | Yes | Optional (TCP pass-through or TLS) |
| Static IP | No | Yes |
| Source IP Preservation | No (via headers) | Yes |
| Latency | Low | Ultra-low |
| Scale | High | Extremely high |
| Health Checks | HTTP/HTTPS | TCP/HTTP |
| AWS WAF Support | Yes | No |
| Typical Use Case | Web apps, APIs, microservices | High-throughput, low-latency services |

---

## Production Takeaway

- Use **ALB** when routing decisions depend on HTTP semantics
- Use **NLB** when performance, static IPs, or non-HTTP protocols are required
- Target Groups abstract backend changes and enable safe deployments

Choosing the correct load balancer type is foundational to both performance and security.

---

Topic 3.3:
Title: Multi-Cloud Awareness
Order: 3

Class 3.3.1:
	Title: GCP Core Services Overview
	Description: Google Cloud Platform concepts vs AWS.
Content Type: text
Duration: 350 
Order: 1
		Text Content :
# GCP: The Google Way

Google Cloud Platform (GCP) approaches infrastructure with a strong emphasis on global networking, managed services, and opinionated defaults. For engineers familiar with AWS, many concepts map closely, but there are key differences in architecture and operational assumptions.

---

## 1. GCP vs AWS: Conceptual Mapping

Most core primitives exist in both clouds but behave slightly differently.

- **Compute Engine:** Equivalent to EC2, provides VM-based compute.
- **Cloud Storage:** Equivalent to S3, object storage with strong consistency guarantees.
- **VPC:** Virtual Private Cloud, but global by default.
- **Service Accounts:** Equivalent to IAM Roles, used for workload-level permissions.
- **Cloud Load Balancing:** Equivalent to ELB, but fully global by default.
- **GKE:** Equivalent to EKS, managed Kubernetes service.
- **Cloud Monitoring / Logging:** Equivalent to CloudWatch, for metrics and logs.

**Key Insight:** The concepts are familiar, but defaults such as global scope and management philosophy are different.

---

## 2. The Global VPC

GCP’s VPC is **global**, unlike AWS where a VPC is region-bound.

- You can have subnets in multiple regions (e.g., New York and Tokyo) within the same VPC.
- Instances in different regions communicate over **internal IPs**.
- Reduces the need for complex cross-region networking like VPC peering or transit gateways.
- Simplifies multi-region application deployment and reduces operational overhead.

**Considerations**
- IP address planning is crucial because the VPC spans multiple regions.
- Misconfigurations can have wider impact due to the global scope.

---

## 3. GKE (Google Kubernetes Engine)

GKE is Google’s managed Kubernetes service, widely regarded for its maturity and upstream integration.

### Autopilot Mode

- Fully managed Kubernetes execution model.
- Google handles:
  - Node provisioning and lifecycle
  - Scaling and resource allocation
  - OS patching and security
- Users deploy only pods and workloads.
- Billing is per pod resource allocation (CPU & memory).

**Benefits**
- Minimal operational overhead
- Strong workload isolation
- Ideal for small teams or production workloads without a dedicated Kubernetes ops team

**Limitations**
- Less control over node-level configurations
- Some workloads requiring daemon-level access may not be supported
- Requires precise resource specifications for pods

---

## Production Takeaway

- GCP feels familiar to AWS users, but the global scope and managed services are differentiators.
- Global VPCs simplify multi-region networking and internal IP communication.
- GKE Autopilot removes operational burden while maintaining Kubernetes standards.
- GCP is best leveraged by embracing its managed, global-first architecture rather than replicating AWS patterns.


---

Class 3.3.2:
	Title: Azure Core Services Overview
	Description: Microsoft Azure concepts vs AWS.
Content Type: text
Duration: 350 
Order: 2
		Text Content :
# Azure: The Enterprise Cloud

Microsoft Azure is an enterprise-focused cloud platform with deep integration into existing Microsoft services like Active Directory, Windows Server, and Office 365. Understanding Azure’s mapping to AWS/GCP helps engineers quickly translate knowledge across clouds.

---

## 1. Azure vs AWS Mapping

- **Virtual Machine (VM):** Equivalent to EC2, provides compute instances.
- **Blob Storage:** Equivalent to S3, scalable object storage.
- **VNET (Virtual Network):** Equivalent to AWS VPC, network isolation and routing.
- **Azure DNS:** Equivalent to Route 53, managed DNS service.
- **AKS (Azure Kubernetes Service):** Equivalent to EKS/GKE, managed Kubernetes service.

**Key Insight:** Azure preserves enterprise-friendly defaults and integrates tightly with Windows ecosystem, making it popular for corporate workloads.

---

## 2. Azure Active Directory (Entra ID)

Azure AD (recently rebranded as Entra ID) is the identity and access management backbone of Azure.

- Provides **RBAC** for resources at scale.
- Integrates seamlessly with:
  - Microsoft 365
  - On-prem AD via hybrid identity
  - OAuth2 and SAML-based applications
- Enables **conditional access policies**, MFA, and identity governance.

**Operational Implication:** Enterprise customers benefit from centralized identity, consistent authentication, and secure access management across workloads.

---

## 3. AKS (Azure Kubernetes Service)

Azure Kubernetes Service is Microsoft’s managed Kubernetes offering.

- Integrates tightly with Azure AD for authentication.
- Supports:
  - Managed node pools (Azure handles patching/upgrades)
  - Virtual nodes via **Azure Container Instances** (similar to Fargate)
- Billing based on VM/node usage or per-pod resources in virtual nodes.
- Provides simplified networking with **Azure CNI** and built-in monitoring with Azure Monitor.

**Best Practices**
- Use managed node pools for predictable workloads
- Use virtual nodes for bursty or ephemeral workloads
- Enforce RBAC and identity-based security using Azure AD integration

---

## 4. Production Takeaway

- Azure is enterprise-first with strong identity, RBAC, and compliance capabilities.
- AKS simplifies Kubernetes operations while leveraging Azure AD.
- Enterprise teams with existing Microsoft infrastructure benefit from reduced friction and better integration.


# Cloud Comparison Table: AWS vs GCP vs Azure

| Component | AWS | GCP | Azure |
|-----------|-----|-----|-------|
| Compute | EC2 | Compute Engine | Virtual Machine (VM) |
| Object Storage | S3 | Cloud Storage | Blob Storage |
| VPC / Networking | VPC (regional) | VPC (global) | VNET (regional) |
| DNS | Route 53 | Cloud DNS | Azure DNS |
| Managed Kubernetes | EKS | GKE | AKS |
| Serverless Containers | Fargate | GKE Autopilot | AKS Virtual Nodes |
| Identity / IAM | IAM Roles | Service Accounts | Azure AD / Entra ID |
| Load Balancer | ELB (ALB/NLB) | Cloud Load Balancing | Azure Load Balancer / Application Gateway |
| Multi-Region Networking | VPC Peering / Transit Gateway | Global VPC | VNET Peering / Global VNET Gateway |
| Key Differentiator | Operational control vs serverless | Global network-first | Enterprise integration, AD focus |



---

Topic 3.4:
Title: Cloud Architecture Patterns
Order: 4

Class 3.4.1:
	Title: High Availability & Disaster Recovery
	Description: RTO, RPO, and multi-region design.
Content Type: text
Duration: 400 
Order: 1
		Text Content :
# Designing for Failure

In production systems, failures are inevitable. Designing for failure ensures minimal impact on users, predictable recovery, and business continuity. This involves careful planning around availability, disaster recovery, and failover strategies.

---

## 1. High Availability (HA) vs Disaster Recovery (DR)

### High Availability (HA)
HA focuses on minimizing **user-visible downtime** within a region or system.

- Example: Multi-AZ Load Balancer distributing traffic across healthy instances
- Key characteristics:
  - Redundant resources in the same or multiple availability zones
  - Automated failover for component failures
  - Small recovery window (seconds to minutes)

**Operational Impact:** Users rarely notice transient failures.

### Disaster Recovery (DR)
DR addresses **large-scale failures**, such as a whole region outage.

- Example: Replicating database snapshots or backups to a secondary region
- Key characteristics:
  - Full or partial replication of critical systems
  - Secondary infrastructure may remain idle (cost-saving)
  - Recovery may require manual or semi-automated steps
  - Longer recovery windows (minutes to hours)

**Operational Impact:** Focuses on business continuity rather than immediate uptime.

---

## 2. RTO & RPO

Reliable failure planning requires quantifying **tolerances**.

- **Recovery Point Objective (RPO):**
  - Defines acceptable data loss
  - Example: "We can afford to lose 15 minutes of data"
  - Determines replication frequency and backup strategies
- **Recovery Time Objective (RTO):**
  - Defines acceptable downtime
  - Example: "We must be back online within 4 hours"
  - Determines failover automation and infrastructure readiness

**Key Insight:** RPO & RTO drive architecture decisions, backup schedules, and cost trade-offs.

---

## 3. Failover Patterns

### Active-Active
- Both primary and secondary regions serve live traffic simultaneously
- Zero downtime during failure
- More expensive due to duplicate infrastructure
- Requires global load balancing and data replication

### Active-Passive (Pilot Light)
- Secondary region has resources partially running or off
- Data is replicated continuously
- Compute resources scaled up only during disaster
- Lower cost, slightly higher RTO
- Common in disaster recovery plans where cost efficiency matters

**Operational Consideration:** Choosing the right pattern depends on business impact, cost tolerance, and complexity.

---

## Production Takeaway

- HA keeps users happy during small failures
- DR prepares for catastrophic events
- RTO and RPO define tolerances for downtime and data loss
- Failover patterns balance cost, complexity, and availability
- Always test failover regularly to validate assumptions


Class 3.4.2:
	Title: Cost Optimization Strategies
	Description: Saving money on the cloud.
Content Type: text
Duration: 400 
Order: 2
		Text Content :
# Cloud Cost Optimization (FinOps)

Cloud cost optimization, or FinOps, is about **aligning cloud spend with business value**. Without proper controls, cloud costs can spiral because resources are elastic and often forgotten.

---

## 1. The Cost Trap

- "Pay-as-you-go" can lead to **unintentional expenses** if resources are left running.
- Common pitfalls:
  - Idle EC2 instances or VMs
  - Forgotten test environments
  - Orphaned storage volumes and snapshots
- Monitoring and tagging resources is critical to identify unused or underutilized assets.

---

## 2. Savings Plans & Reserved Instances

Commitment-based pricing options can drastically reduce costs.

- **Savings Plan**
  - Commit to a certain spend per hour (e.g., $10/hour)
  - Flexible across instance types, regions, and families
  - Offers discounts up to 72% off On-Demand pricing
- **Reserved Instance**
  - Commit to a specific instance type, region, and term
  - Less flexible but can be cheaper in predictable workloads
  - Best for steady-state production systems

**Operational Advice:** Analyze historical usage and growth trends to choose the right plan.

---

## 3. Spot Instances

- AWS sells **unused capacity at steep discounts** (up to 90% off).
- **Limitations:**
  - AWS can reclaim the instance with **2 minutes notice**
  - Not suitable for stateful workloads like databases
- **Use Cases:**
  - Stateless web or application servers
  - CI/CD pipelines
  - Batch processing jobs
  - Big data processing
- Integrate automated job rescheduling to handle interruptions gracefully.

---

## 4. Storage Lifecycle Management

Long-term storage costs can be reduced by moving infrequently accessed data to cheaper tiers.

- **Example:** Logs older than 90 days
  - Move from S3 Standard to **S3 Glacier** or **Glacier Deep Archive**
- Lifecycle policies automate transitions, deletions, and archival
- Reduces waste and aligns storage cost with actual business need

**Key Insight:** Storage optimization works best when data access patterns are regularly reviewed and automated.

---

## Production Takeaway

- Always monitor and tag resources to prevent cost leaks
- Use **Savings Plans / Reserved Instances** for predictable workloads
- Leverage **Spot Instances** for bursty, stateless workloads
- Implement **storage lifecycle policies** to reduce long-term costs
- Cost optimization is continuous, not a one-time activity

---
Topic 3.5:
Title: Cloud Infrastructure - Challenge
Order: 5


Class 3.5.1:
	Title: Cloud Infrastructure - Challenge
	Description: Scenario-based cloud architecture problems.
Content Type: text
Duration: 300 
Order: 1
		Text Content :
# AWS Fundamentals – Challenge  
**Contest Format | 5 Questions**

These questions are designed to test **real-world AWS troubleshooting and architecture judgment**, not just service definitions.

---

## Question 1: EC2 Instance Troubleshooting & Recovery

### Problem  
An EC2 instance hosting a production application is unreachable via SSH after a reboot.

**Tasks:**
1. Identify possible causes.
2. Recover access without terminating the instance.
3. Prevent this issue in the future.

---

### Answer

**Possible Causes**
- Security Group no longer allows port `22`
- Network ACL blocking inbound SSH
- Instance is in a failed state (disk full, misconfigured startup script)
- CPU or memory exhaustion

**Recovery Steps**
1. Check instance **System Status Checks** and **Instance Status Checks**.
2. Verify Security Group allows:
   `TCP 22 from your IP`

3. If still inaccessible:

   * Stop the instance
   * Detach the root EBS volume
   * Attach it to a healthy helper instance
   * Fix configuration (`sshd_config`, disk cleanup)
   * Reattach and restart

**Prevention**

* Use **SSM Session Manager** instead of SSH
* Enable CloudWatch alarms
* Avoid critical changes via User Data scripts

---

## Question 2: VPC Networking & Security Group Configuration

### Problem

An application in a private subnet cannot reach the internet to download updates.

**Tasks:**

1. Identify why outbound access fails.
2. Fix the networking configuration securely.

---

### Answer

**Root Cause**
Private subnets have no direct route to the Internet Gateway.

**Fix**

1. Create a **NAT Gateway** in a public subnet.
2. Update the private subnet route table:

   `0.0.0.0/0 → NAT Gateway`

**Security Group Check**

* Outbound rule must allow:

  `TCP 443 → 0.0.0.0/0`

**Best Practice**

* Never expose private instances directly to IGW
* Use NAT only for outbound access

---

## Question 3: RDS Multi-AZ & Read Replica Setup

### Problem

Your database must be:

* Highly available
* Scalable for read-heavy workloads

**Tasks:**

1. Choose between Multi-AZ and Read Replicas.
2. Explain when to use each.

---

### Answer

**Multi-AZ**

* Synchronous replication
* Automatic failover
* Used for **High Availability**
* No read scaling

**Read Replicas**

* Asynchronous replication
* Used for **Read Scaling**
* No automatic failover

**Correct Design**

* Enable **Multi-AZ** for availability
* Add **Read Replicas** for scaling
* Route read traffic separately

---

## Question 4: S3 Lifecycle Policies & Cost Optimization

### Problem

An S3 bucket storing application logs is growing rapidly and increasing costs.

**Tasks:**

1. Optimize storage cost.
2. Retain logs for compliance.

---

### Answer

**Solution**
Implement an S3 Lifecycle Policy:

1. Move logs after 30 days:

   `S3 Standard → S3 Glacier`
2. Archive long-term:

   `After 180 days → Glacier Deep Archive`

**Why This Works**

* Logs are rarely accessed after initial analysis
* Glacier storage costs are significantly lower
* Compliance retention is preserved

---

## Question 5: Load Balancer Health Checks & Target Groups

### Problem

Users report intermittent downtime, but EC2 instances appear healthy.

**Tasks:**

1. Identify possible causes.
2. Fix load balancer configuration.

---

### Answer

**Common Causes**

* Incorrect health check path
* Application returns `500` instead of `200`
* Timeout too short for app startup

**Fix**

1. Verify health check configuration:

   * Path: `/health`
   * Expected response: `200`
2. Increase timeout and unhealthy threshold if needed
3. Ensure app dependencies are ready before health endpoint returns success

**Key Insight**
A healthy instance does not mean a healthy **application**.

---

## Contest Evaluation Focus

* Correct use of AWS services
* Clear separation of HA vs scalability
* Security-first networking
* Cost-aware design
* Ability to reason during failures

These scenarios mirror **real production AWS incidents**, not exam-style questions.




Topic 3.6:
Title: Advanced Git Operations
Order: 6

Class 3.6.1:
	Title: Git Hooks and Advanced Features
	Description: Client-side and server-side hooks, detached HEAD, and history management.
Content Type: text
Duration: 500 
Order: 1
		Text Content :
 # Advanced Git: Beyond Branching and Merging

## 1. Git Hooks: Automating Workflows

Git hooks are scripts that run automatically at specific points in the Git lifecycle. They enable:
- **Linting** before commit (catch style violations early)
- **Security checks** (prevent secrets in commits)
- **Tests** (run unit tests before pushing)
- **CI/CD triggers** (notify servers on push)

### Client-Side Hooks (Run Locally)

#### `pre-commit` - Before Commit is Created

```bash
# .git/hooks/pre-commit
#!/bin/bash
set -euo pipefail

# Lint staged files
eslint $(git diff --cached --name-only --diff-filter=ACM | grep '.js$') || exit 1

# Check for large files (>10MB)
git diff --cached --name-only | while read file; do
  size=$(git ls-files -s "$file" | awk '{print $4}' | xargs git cat-file -s)
  if [ $size -gt 10485760 ]; then
    echo "ERROR: File '$file' is too large ($size bytes)"
    exit 1
  fi
done

# Prevent committed secrets
if git diff --cached | grep -qE 'password|secret|api.?key|aws.?key'; then
  echo "ERROR: Secrets detected in staged changes"
  exit 1
fi

exit 0
```

**Usage:**
```bash
# Git automatically runs the hook
git commit -m "Fix bug"
# If hook exits with non-zero, commit is rejected
```

#### `pre-push` - Before Push to Remote

```bash
# .git/hooks/pre-push
#!/bin/bash
# Prevent force-push to main branch

remote=$1
url=$2

while read local_ref local_oid remote_ref remote_oid; do
  if [[ "$remote_ref" == "refs/heads/main" && "$local_oid" != "$remote_oid" ]]; then
    echo "ERROR: Force-push to main is not allowed"
    exit 1
  fi
done

# Run tests before push
npm test || exit 1

exit 0
```

#### `post-merge` - After Merge Completes

```bash
# .git/hooks/post-merge
#!/bin/bash
# Automatically update dependencies if package-lock.json changed

if git diff HEAD@{1} --name-only | grep -q package-lock.json; then
  echo "Dependencies changed. Running npm install..."
  npm install
fi

exit 0
```

### Server-Side Hooks (Run on Server)

#### `pre-receive` - Before Push is Accepted

```bash
# /path/to/repo.git/hooks/pre-receive
#!/bin/bash
# Enforce commit message format

while read oldrev newrev refname; do
  # Get commits in this push
  for commit in $(git rev-list $oldrev..$newrev); do
    message=$(git log -1 --format=%B $commit)
    
    # Enforce conventional commits (feat:, fix:, etc.)
    if ! echo "$message" | grep -qE '^(feat|fix|docs|style|refactor|test|chore):'; then
      echo "ERROR: Commit message must start with type (feat:, fix:, etc.)"
      exit 1
    fi
  done
done

exit 0
```

#### `post-receive` - After Push is Accepted

```bash
# /path/to/repo.git/hooks/post-receive
#!/bin/bash
# Trigger CI/CD pipeline

while read oldrev newrev refname; do
  branch=$(basename $refname)
  
  # Only trigger on main/production
  if [[ "$branch" == "main" || "$branch" == "production" ]]; then
    curl -X POST \
      -H "Authorization: token $GITHUB_TOKEN" \
      https://api.github.com/repos/myorg/myrepo/dispatches \
      -d "{\"event_type\":\"deploy\",\"client_payload\":{\"branch\":\"$branch\"}}"
  fi
done

exit 0
```

### Installing Hooks Safely

```bash
# Store hooks in repo
mkdir -p .githooks
cp pre-commit .githooks/
chmod +x .githooks/*

# Configure Git to use them
git config core.hooksPath .githooks

# Other developers pull and hooks are active
git clone <repo>
# Hooks are automatically available
```

---

## 2. Detached HEAD State

### What is HEAD?

`HEAD` is a pointer to the current commit. Normally it points to a branch:

```
main → commit ABC123
↑
HEAD points to main
```

### Entering Detached HEAD

```bash
# Check out a specific commit
git checkout abc123def

# Now HEAD points directly to the commit
abc123 (HEAD)
↑
HEAD points to commit, not a branch
```

**Symptoms:**
```
HEAD detached at abc123def
```

### Why This Happens (And Why It's Dangerous)

```
Before: main → ABC123 → (you are here)
After:  main → ABC123 (still here)
        ↑
        New commits here are **orphaned**
        They will be garbage collected!
```

```bash
git checkout abc123def
# Make new commits
git commit -m "Fix bug"  # This commit is now orphaned

git checkout main  # Switch away
# Your fix is lost! (Unless you saved the SHA)
```

### Recovering from Detached HEAD

```bash
# You made commits in detached HEAD
git checkout abc123def
git commit -m "My fix"
# Oops, now at defgh789 but HEAD is detached

# Option 1: Create a branch
git branch my-fix  # Saves the current commit to my-fix branch
git checkout my-fix

# Option 2: Use reflog
git reflog  # See all recent commits
# abc123def HEAD@{0}: commit: My fix
git checkout -b recovery-branch abc123def

# Option 3: Cherry-pick if you're on main
git checkout main
git cherry-pick defgh789  # Applies the commit to main
```

### Legitimate Uses of Detached HEAD

```bash
# 1. Inspecting a specific commit
git checkout v1.0.0
# Look at the code, run tests
git checkout main

# 2. Bisecting to find a bug
git bisect start
git bisect bad main
git bisect good v1.0.0
# Git checks out commits for testing
# When done: git bisect reset

# 3. Testing a specific state
git checkout abc123def
npm test
git checkout main
```

---

## 3. Revert vs Reset vs Checkout

These commands all "undo" changes but in different ways.

### Reset - Move the Branch

**Does:** Moves the branch pointer backward

```bash
# File changed and committed
echo "bug" > file.txt
git add file.txt
git commit -m "Introduce bug"
# main → ABC123 (bug)

# Reset to before the bug
git reset --soft HEAD~1
# main → XYZ789 (before bug)
# Changes are in staging area
# Perfect for: "I want to redo this commit"

# Reset hard (DANGEROUS)
git reset --hard HEAD~1
# Commits AND files are gone
# Perfect for: "Delete the last commit completely"
```

### Revert - Create a New Commit

**Does:** Creates a new commit that **undoes** the changes

```bash
# File changed and committed
echo "bug" > file.txt
git add file.txt
git commit -m "Introduce bug"
# main → ABC123 (bug)

# Revert (create new commit that undoes it)
git revert ABC123
# main → ABC123 → XYZ789 (undo of bug)
# History is preserved
# Perfect for: "The commit is already public, I need a clean history"
```

### Checkout - Move HEAD

**Does:** Moves HEAD to a different commit/branch

```bash
# Switch to a branch
git checkout main

# Go back in history (detached HEAD)
git checkout abc123def

# Discard changes to a file
git checkout -- file.txt
```

### Comparison Table

| Command | Use Case | Safety | History |
| :--- | :--- | :--- | :--- |
| **Reset --soft** | Redo last commit | Safe (can recover) | Lost (can recover) |
| **Reset --mixed** | Unstage changes | Safe | Lost (can recover) |
| **Reset --hard** | Delete last commit | Dangerous | Lost completely |
| **Revert** | Undo public commit | Very safe | Preserved (new commit) |
| **Checkout** | Switch branch/discard | Safe (warns on unsaved) | N/A |

**Rule of Thumb:**
- If commit is **public** (pushed): Use `git revert`
- If commit is **local** (not pushed): Use `git reset`
- If you **messed up**: Use `git reflog` to recover

---

## 4. Advanced Rebase

Rebase rewrites history by replaying commits. Powerful but dangerous.

### Interactive Rebase

```bash
# Rewrite the last 3 commits
git rebase -i HEAD~3

# Opens editor with:
# pick abc123 Commit 1
# pick def456 Commit 2
# pick ghi789 Commit 3

# Edit to:
# pick abc123 Commit 1
# squash def456 Commit 2    # Combine with previous
# reword ghi789 Commit 3    # Keep but edit message

# Options:
# pick    - Keep commit as is
# reword  - Change commit message
# edit    - Stop to edit the commit
# squash  - Combine with previous (keep message)
# fixup   - Combine with previous (discard message)
# drop    - Delete commit
```

### Rebase --onto (Advanced)

Replay commits from one branch onto another.

```
Before:
main  → A → B → C
         ↓
        feature → D → E

After (rebase --onto main):
main  → A → B → C
                ↓
           feature → D' → E'
```

```bash
# Move feature branch to base on latest main
git rebase --onto main original-base feature

# Or simpler:
git checkout feature
git rebase main

# Result: feature branch now has all commits from main + its own
```

### Rewriting History Safely

```bash
# BEFORE: Someone pushed a commit with secrets
# main → A → B (has AWS key) → C → D (others made commits)

# Extract the secret-bearing commit
git rebase -i HEAD~3
# edit B

# Remove the secret
rm secrets.txt
git add -A
git commit --amend  # Modify the commit

# Continue rebase
git rebase --continue

# Force-push (ONLY if no one else is working on this branch)
git push -f origin main
```

**Safety Rules:**
1. **Never rewrite public history** unless coordinated
2. **Always rebase locally first** and test
3. **Use `git push --force-with-lease`** instead of `git push -f` (safer)
4. **Communicate** with team before force-pushing

---

## 5. Git Reflog: The Safety Net

Reflog records **all** changes to HEAD, including resets and rebases.

### What Reflog Shows

```bash
git reflog

# Output:
# abc123d HEAD@{0}: rebase: Commit message
# def456e HEAD@{1}: checkout: moving to feature
# ghi789f HEAD@{2}: commit: Fix bug
# jkl012g HEAD@{3}: reset: going back to abc
# ... (goes back weeks)
```

### Recovering Lost Commits

```bash
# You accidentally reset and lost commits
git reset --hard HEAD~5

# Check reflog
git reflog
# abc123 HEAD@{0}: reset: going back
# def456 HEAD@{1}: commit: My fix
# ghi789 HEAD@{2}: commit: Another fix

# Recover the commit
git checkout def456
# or create a new branch from it
git branch recovery def456

# Cherry-pick the commits back
git checkout main
git cherry-pick def456 ghi789
```

### Reflog vs Git Log

| Aspect | git log | git reflog |
| :--- | :--- | :--- |
| **Shows** | Commits in branches | All HEAD movements |
| **Recovers** | Reachable commits | Unreachable commits |
| **Scope** | Per-branch | Local repository only |
| **Lifespan** | Permanent | 30 days (default) |

---

Class 3.6.2:
	Title: Git Internals and Advanced Workflows
	Description: Git objects, references, filter-repo, and dependency management.
Content Type: text
Duration: 450 
Order: 2
		Text Content :
 # Git Internals: How Git Actually Works

## 1. Git Objects: The Foundation

Git stores everything as **immutable objects**. There are 4 types.

### Blob (Binary Large Object)

A blob is a **file's contents**, nothing else.

```bash
# When you stage a file
echo "hello world" > file.txt
git add file.txt

# Git creates a blob (SHA-1 hash of content)
git hash-object file.txt
# e7cf3ef ... (this is the blob SHA)

# The actual storage
.git/objects/e7/cf3ef...
```

**Key Point:** Two files with identical content share the same blob (deduplication).

```bash
echo "hello world" > file1.txt
echo "hello world" > file2.txt
git add file1.txt file2.txt

# Same content = same blob hash
# Storage is deduplicated
```

### Tree

A tree is a **directory listing**. It maps filenames to blobs/subtrees.

```
commit abc123
│
└── tree def456
    ├── src/ → tree ghi789
    │   └── main.py → blob jkl012
    ├── README.md → blob mno345
    └── .gitignore → blob pqr678
```

```bash
# View the tree
git cat-file -p HEAD^{tree}

# Output:
# 100644 blob jkl012abc  README.md
# 100644 blob mno345def  .gitignore
# 040000 tree ghi789jkl  src
```

### Commit

A commit is a **snapshot**: it points to a tree, author, message, and parent commits.

```
commit abc123def
├── tree: def456ghi789  (the state of the repo)
├── parent: xyz789abc123  (previous commit)
├── author: Alice <alice@example.com>
├── date: 2024-01-15 10:00:00
└── message: Fix bug in parser
```

```bash
# View a commit object
git cat-file -p HEAD

# Output:
# tree def456ghi789
# parent xyz789abc123
# author Alice <alice@example.com> 1705325000 +0000
# committer Alice <alice@example.com> 1705325000 +0000
#
# Fix bug in parser
```

### Tag

A tag is a **named reference** to a commit (often for releases).

```bash
# Annotated tag (recommended)
git tag -a v1.0.0 -m "Release 1.0.0" abc123def

# Lightweight tag
git tag v1.0.0 abc123def

# View tag
git cat-file -p v1.0.0
# object abc123def
# type commit
# tagger Alice <alice@example.com> 1705325000
# tag v1.0.0
# Release 1.0.0
```

---

## 2. Git References

References are **pointers** to commits. They enable the human-readable Git.

### Branches as References

```bash
# A branch is just a file pointing to a commit
cat .git/refs/heads/main
# abc123def456...

# When you commit
git commit -m "Fix"
# The file gets updated
cat .git/refs/heads/main
# def456ghi789...
```

### HEAD Reference

```
HEAD → main → abc123def (current commit)
```

```bash
# HEAD normally points to a branch
cat .git/HEAD
# ref: refs/heads/main

# In detached HEAD state
cat .git/HEAD
# abc123def (direct commit reference)
```

### Remote References

```bash
# After git fetch
cat .git/refs/remotes/origin/main
# xyz789abc123...
```

**Remote references are read-only.** They represent the last known state of the remote branch.

---

## 3. The .git Directory Structure

```
.git/
├── HEAD                      # Points to current branch/commit
├── config                    # Local config
├── objects/                  # Blobs, trees, commits, tags
│   ├── ab/
│   │   ├── cd1234...
│   │   └── ef5678...
│   └── ...
├── refs/
│   ├── heads/               # Branches
│   │   ├── main
│   │   ├── feature-1
│   │   └── ...
│   ├── remotes/            # Remote branches
│   │   └── origin/
│   │       ├── main
│   │       └── ...
│   └── tags/               # Tags
│       ├── v1.0.0
│       └── ...
├── hooks/                   # Git hooks
├── logs/                    # Reflog data
├── index                    # Staging area
└── description             # Repository description
```

**Key Files:**
- **HEAD:** Current commit
- **config:** Local settings (remote URLs, user)
- **objects/:** All Git data (immutable)
- **refs/:** Pointers to commits
- **index:** Staging area (what `git add` modifies)

---

## 4. Git Filter-Repo: Rewriting History

Filter-repo rewrites Git history—useful for removing secrets or large files.

### Installing

```bash
pip install git-filter-repo
```

### Removing Sensitive Data

```bash
# Someone accidentally committed AWS keys
git log --all --full-history -- secrets.txt
# Oops, it's there from commit abc123

# Remove it entirely
git filter-repo --invert-paths --path secrets.txt

# All commits are rewritten
# The file is removed from entire history
# Reflog is cleaned up
```

### Removing Large Files

```bash
# Find large files
git filter-repo --analyze

# Review report (shows files by size)
cat .git/filter-repo/analysis/*.txt

# Remove files > 10MB
git filter-repo --strip-blobs-bigger-than 10M
```

### Replacing Values (Find and Replace)

```bash
# Replace old domain with new domain in all commits
git filter-repo --message-callback \
  'return message.replace(b"old-domain.com", b"new-domain.com")'

# Or in file contents
git filter-repo --blob-callback \
  'return blob.replace(b"old-api-key", b"new-api-key")'
```

### Post-Filter Steps

```bash
# After filtering, the repo is "dirty"
# Force-push to remote (DANGEROUS!)
git push --force-with-lease --all origin

# Other developers must:
git clone <repo>  # Fresh clone
# OR (risky)
git reset --hard @{upstream}
```

---

## 5. Git Submodules vs Subtrees

Managing dependencies in Git.

### Submodules (Loose Coupling)

A submodule is a **reference to another Git repository**.

```bash
# Add a dependency
git submodule add https://github.com/lib/json json/

# Creates .gitmodules
cat .gitmodules
# [submodule "json"]
#   path = json
#   url = https://github.com/lib/json

# Cloning a repo with submodules
git clone --recurse-submodules <repo>

# Or fetch separately
git submodule update --init --recursive
```

**Pros:**
- Loose coupling (library has own repo)
- Easy to update to new version
- Library team maintains independently

**Cons:**
- Submodule state must be manually tracked
- Cloning requires extra steps
- Merges can be complex

### Subtrees (Tight Coupling)

A subtree **imports another repo's history** into a subdirectory.

```bash
# Add a dependency
git subtree add --prefix json \
  https://github.com/lib/json main --squash

# Result: json/ directory is now part of this repo
# But history is preserved
```

**Pros:**
- Everything is in one repo (easier for developers)
- No special clone steps
- Full history available

**Cons:**
- Tightly coupled (library is "copied" into repo)
- Updates must be manually pulled

### Comparison

| Aspect | Submodules | Subtrees |
| :--- | :--- | :--- |
| **Coupling** | Loose | Tight |
| **Clone Complexity** | Higher | Same as normal |
| **Update Workflow** | `git submodule update` | `git subtree pull` |
| **History** | Separate | Merged |
| **Use Case** | Shared libraries | Vendored dependencies |

**Best Practice:**
- Use **submodules** for libraries you don't own
- Use **subtrees** for vendored code you might modify
- Use **package managers** (npm, pip) when possible

---

Module 4:
Title: Container Orchestration with Kubernetes
Description: Master containerization and Kubernetes from fundamentals to advanced orchestration. Learn Docker internals, Kubernetes architecture, and production-grade deployment patterns.
Order: 4
Learning Outcomes:
Master Docker containerization
Understand Kubernetes architecture deeply
Deploy and manage production workloads on K8s
Implement security and resource management best practices

Topic 4.1:
Title: Docker Fundamentals
Order: 1

Class 4.1.1:
	Title: Docker Architecture & Internals
	Description: Containers vs VMs, layers, and namespaces.
Content Type: text
Duration: 400 
Order: 1
		Text Content :
 # Docker Architecture: Under the Hood

## 1. Container vs. VM (The Classic Interview Question)
"Explain the difference between a Container and a VM."

* **Virtual Machine (VM):** Virtualizes the **Hardware**. Each VM has its own full OS Kernel. Heavy, slow to boot (minutes).
* **Container:** Virtualizes the **OS**. All containers share the *host's* Linux Kernel but have their own user space (libs, bin). Lightweight, fast to boot (milliseconds).
* **Key Concept:** A container is just a **process** on the host, heavily isolated using Linux primitives (namespaces, cgroups, etc.).

### Security Isolation Benefits of VMs
- **Stronger isolation boundary:** VMs provide hardware-level virtualization via a hypervisor, giving each VM its own dedicated kernel and full guest OS. This means a compromise in one VM (e.g., via a kernel exploit) cannot easily spread to other VMs or the host, as they are completely isolated at the hardware emulation layer.
- **Reduced shared attack surface:** Containers share the host kernel, so a kernel-level vulnerability can potentially allow a container breakout to affect the host or other containers. VMs eliminate this risk by not sharing the kernel—each VM is a standalone system.
- **Preferred for multi-tenant or high-security workloads:** In scenarios like hosting untrusted applications or strict compliance (e.g., finance/healthcare), VMs offer better protection against lateral movement.

### Network Isolation Benefits of VMs
- **Fully virtualized network stack:** Each VM has its own independent virtual network interface, IP stack, routing tables, and firewall rules, emulating a separate physical machine. This provides inherent network isolation without relying on host-level configurations.
- **Stricter security controls:** Easier to enforce isolated networks (e.g., no direct communication between VMs unless explicitly allowed via virtual switches/routers), reducing risks like unauthorized inter-workload traffic.
- **Contrast with containers:** Containers often share the host's network namespace by default (or use bridged/overlays), making isolation more configurable but potentially weaker if misconfigured—e.g., easier lateral movement via shared networking.

In summary, while containers excel in efficiency and density, VMs trade that for superior isolation in both security and networking, making them ideal when maximum protection is needed. Many modern setups run containers *inside* VMs to combine the best of both.

---

## 2. How Isolation Works (The Magic)

Containers leverage Linux kernel features to **isolate processes and resources** while sharing the same host OS kernel. This provides the illusion of dedicated machines without the overhead of full VMs.

### Namespaces: Process & Network Isolation

Namespaces make a container think it has its own system resources.

- **PID Namespace**
  - Each container has its own process ID space.
  - Inside the container, the main process is PID 1.
  - On the host, that same process might be PID 4532.
  - Ensures processes in different containers cannot see or interfere with each other.

- **Network Namespace**
  - Containers have their own virtual network interface (`eth0`) and IP addresses.
  - Network traffic is isolated from the host and other containers.
  - Allows multiple containers to run services on the same port internally without conflict.

- **Other Namespaces**
  - Mount, UTS, IPC, and User namespaces for filesystem, hostname, IPC, and user isolation.

### cgroups (Control Groups): Resource Limiting

- Controls the **CPU, memory, disk I/O, and network bandwidth** a container can consume.
- Prevents one container from monopolizing host resources.
- Example:
  - Limit container to 1 CPU core and 512MB RAM
  - Kernel enforces these limits strictly

**Key Insight:** Namespaces isolate, cgroups constrain. Together, they create lightweight, secure containers.

---

## 3. Images & Layers (Union Filesystem)

Docker images are **immutable templates** built in layers, enabling reuse and efficient storage.

### Read-Only Layers

- Images are composed of multiple **read-only layers**:
  1. Base OS
  2. Installed dependencies (Python, Node, etc.)
  3. Application code
- Each layer is immutable and shared across containers
- Reduces storage usage and speeds up image distribution

### Copy-on-Write (CoW)

- When a container starts, Docker adds a **thin writable layer** on top of the image.
- Any file modification happens in this top layer:
  - Original read-only layers remain unchanged
  - If a file is modified, it is copied from the read-only layer to the writable layer
- This makes container startup **instant** and isolates runtime changes from the image

**Key Insight:** Layers + CoW allow fast, lightweight container creation and efficient image sharing.

---
Class 4.1.2:
	Title: Dockerfile Best Practices
	Description: Multi-stage builds and cache optimization.
Content Type: text
Duration: 450 
Order: 2
		Text Content :
 # Dockerfile Best Practices

## 1. The Build Cache (Speed)
Docker executes instructions from top to bottom. If a line hasn't changed, it uses the cached layer.
* **The Trap:** Putting `COPY . .` (copying source code) *before* `RUN npm install`.
* **The Fix:** Copy `package.json` first, install dependencies, *then* copy source code. This ensures you don't re-download internet dependencies just because you changed a comment in your code.

---

## 2. Multi-Stage Builds (Size)
Production images should be tiny. You don't need the Go compiler or Maven in production; you just need the binary.
* **Stage 1 (Builder):** Compiles the code. Large size (e.g., 1GB).
* **Stage 2 (Runner):** Copies *only* the binary from Stage 1. Tiny size (e.g., 20MB).

---

## 3. Security
* **Don't Run as Root:** By default, Docker runs as root. If a hacker breaks out of the container, they have root on your server.
    * *Instruction:* `USER appuser`
* **Distroless Images:** Use base images like `gcr.io/distroless/static` which have NO shell (`/bin/bash`). Even if a hacker gets in, they can't run commands.

---

Class 4.1.3:
	Title: Docker Networking & Storage
	Description: Volumes and networking modes.
Content Type: text
Duration: 350 
Order: 3
		Text Content :
# Docker Networking & Storage

Docker provides flexible networking and storage options to enable container communication and data persistence. Understanding these is key to designing reliable containerized applications.

---

## 1. Networking Modes

Docker containers can connect to the network in multiple ways depending on isolation, performance, and cross-host requirements.

### Bridge (Default)
- Each container gets its own internal IP on a private network.
- Docker uses **NAT (Network Address Translation)** to allow outbound traffic.
- Suitable for single-host setups.
- Pros: Simple, isolated networking.
- Cons: Requires port mapping for external access.

### Host
- Container shares the **host's network namespace**.
- No NAT, so traffic is faster.
- Ports are directly exposed on the host.
- Pros: Maximum performance.
- Cons: Port conflicts are common; you cannot run multiple containers using the same port on the host.

### Overlay
- Connects containers across **different hosts**.
- Used in **Docker Swarm** and **Kubernetes** (via CNI plugins).
- Requires a key-value store (Swarm) or cluster networking backend (K8s).
- Pros: Enables multi-host service communication, scalable.
- Cons: Slightly higher network overhead due to encapsulation (VXLAN).

---

## 2. Data Persistence (Volumes)

Containers are **ephemeral**, so external storage is necessary for persistent data.

### Bind Mount
- Maps a **host directory** into the container.
- Example: `/home/user/code` → `/app` inside the container.
- Great for development and live reloading.
- Changes in the host directory are instantly visible inside the container.
- Cons: Less portable and depends on host filesystem.

### Named Volume
- Managed by Docker, typically stored in `/var/lib/docker/volumes`.
- Independent of host directory structure.
- Standard for production databases and stateful services.
- Pros: Portable, managed by Docker, survives container recreation.
- Supports volume drivers for cloud storage, encryption, and backups.

**Key Insight:**  
Use **bind mounts** for development convenience and **named volumes** for production-grade persistence.


---

Topic 4.2:
Title: Kubernetes Architecture
Order: 2

Class 4.2.1:
	Title: Kubernetes Control Plane
	Description: The brain of the cluster.
Content Type: text
Duration: 450 
Order: 1
		Text Content :
# Kubernetes Control Plane (The Brain)

The Kubernetes Control Plane orchestrates the entire cluster. It manages scheduling, state reconciliation, and communication between nodes and workloads. Understanding each component is critical to designing resilient and scalable clusters.

---

## 1. API Server (The Front Door)

- The **API Server** is the **central point of communication** for users (`kubectl`), controllers, schedulers, and nodes.
- Stateless and horizontally scalable:
  - Multiple API server instances can run behind a load balancer.
- Exposes the Kubernetes API over HTTPS.
- Validates requests and updates **etcd** accordingly.
- Handles authentication, authorization (RBAC), and admission control.

**Key Insight:** Every Kubernetes action flows through the API server.

---

## 2. etcd (The Memory)

- **etcd** is a distributed, consistent key-value store.
- Stores the **entire cluster state**:
  - Pod specs
  - ConfigMaps and Secrets
  - Node and service information
- High Availability:
  - Production clusters typically use 3 or 5 nodes
  - Raft consensus protocol ensures consistency
- **Critical:** Losing etcd without backups means losing cluster state.

**Operational Tip:** Always enable automated snapshots and offsite backups.

---

## 3. Scheduler

- Responsible for **pod placement**.
- Evaluates nodes based on:
  - Resource availability (CPU, memory, GPUs)
  - Affinity/anti-affinity rules
  - Taints and tolerations
  - Custom scheduling constraints
- Selects the most suitable node and informs the API server.

**Key Insight:** Scheduler enforces constraints while optimizing resource utilization.

---

## 4. Controller Manager

- Continuously monitors **Desired State vs Current State**.
- Runs controllers such as:
  - ReplicationController / ReplicaSet
  - Deployment
  - StatefulSet
  - Node and Service controllers
- **Example:** If a deployment specifies 3 replicas and one pod fails, the Controller Manager requests the API server to start a replacement pod.
- Ensures **self-healing** behavior of the cluster.

**Takeaway:** The Controller Manager automates the maintenance of the cluster state, ensuring workloads stay as intended.


---

Class 4.2.2:
	Title: Kubernetes Worker Nodes
	Description: Kubelet and Proxy.
Content Type: text
Duration: 350 
Order: 2
		Text Content :
# Kubernetes Worker Nodes (The Muscle)

Worker nodes are where your actual workloads run. While the Control Plane decides *what should happen*, worker nodes are responsible for *making it happen*. Each node runs a small but critical set of components that execute pods, handle networking, and report health.

---

## 1. Kubelet (The Captain)

The **kubelet** is the primary node-level agent in Kubernetes. Every worker node runs exactly one kubelet.

- It continuously watches the API Server for PodSpecs that are assigned to its node.
- Once it sees a Pod assignment, it:
  - Pulls container images
  - Creates containers via the container runtime
  - Ensures containers are running and healthy
- Periodically reports:
  - Node status (Ready / NotReady)
  - Pod status
  - Resource usage signals

Kubelet does **not** schedule pods. It only executes what has already been decided by the Scheduler.

**Failure Mode Insight:**  
If the kubelet stops running, the node becomes `NotReady`, and the Control Plane will eventually reschedule pods elsewhere.

---

## 2. Container Runtime (Implicit but Critical)

Although not always listed separately, the container runtime is essential to the worker node.

- Examples: `containerd`, `CRI-O` (Docker is deprecated)
- Responsible for:
  - Pulling images
  - Creating containers
  - Managing container lifecycle

Kubelet communicates with the runtime using the **Container Runtime Interface (CRI)**.

---

## 3. Kube-proxy (The Networking)

**kube-proxy** handles service-level networking on each node.

- Maintains network rules so traffic sent to a **Service IP (ClusterIP)** reaches the correct backend Pod.
- Implements Service load balancing using:
  - **iptables mode** (most common, simple, battle-tested)
  - **IPVS mode** (higher performance at large scale)
- Works at Layer 4 (TCP/UDP).

**Example Flow:**
A request to `10.96.0.10:80` (Service IP) is transparently redirected to one of the Pod IPs backing that service.

**Scaling Insight:**  
kube-proxy does not proxy traffic in userspace; it programs kernel rules, making it extremely efficient.

---

## 4. Node-Level Responsibility Summary

Worker nodes are responsible for:
- Running application containers
- Enforcing CPU and memory limits (via cgroups)
- Handling Pod networking and Service routing
- Reporting health and metrics upstream

**Mental Model:**  
If the Control Plane is the brain, worker nodes are the hands and legs that do the actual work.


---

Topic 4.3:
Title: Kubernetes Core Objects
Order: 3

Class 4.3.1:
	Title: Pods, Deployments, StatefulSets
	Description: Workload management.
Content Type: text
Duration: 500 
Order: 1
		Text Content :
# Core Objects: Managing Workloads

Kubernetes workloads are defined using higher-level objects that describe *how applications should run*, not *how to run them manually*. These objects enable self-healing, scaling, and safe updates.

---

## 1. Pods (The Atom)

A **Pod** is the smallest unit of scheduling in Kubernetes.

- A pod represents **one logical application instance**.
- Most pods run **one container**, but multiple containers are used when they must:
  - Share the same network namespace
  - Share the same storage
  - Start and stop together (sidecar pattern)

Key characteristics:
- All containers in a pod:
  - Share the same IP address
  - Share localhost networking
  - Can share volumes
- Pods are **ephemeral by design**:
  - If a pod dies, it is gone forever
  - Kubernetes creates a *new* pod, not the same one

**Critical Rule:**  
Never create pods directly for production workloads. Always use a controller (Deployment, StatefulSet, Job).

---

## 2. Deployments (For Stateless Applications)

A **Deployment** is the most common workload controller.

Primary use cases:
- Web servers
- REST APIs
- Microservices
- Any stateless application

How it works:
- You declare:
  - Container image
  - Number of replicas
  - Update strategy
- Kubernetes creates a **ReplicaSet**
- The ReplicaSet ensures the desired number of pods are always running

### Rolling Updates
Deployments support zero-downtime upgrades.

- New pods (v2) are created gradually
- Old pods (v1) are terminated gradually
- Traffic is shifted automatically via Services
- Rollbacks are instant if something goes wrong

**Operational Insight:**  
Deployments assume pods are interchangeable. If a pod dies, any other pod can replace it without data loss.

---

## 3. StatefulSets (For Stateful Applications)

A **StatefulSet** is designed for applications that need **identity and stable storage**.

Primary use cases:
- Databases
- Distributed systems (Kafka, Cassandra, Elasticsearch)
- Any system where order and identity matter

Key guarantees:
- **Stable Pod Identity**
  - Pod names are predictable: `db-0`, `db-1`, `db-2`
  - If `db-0` crashes, it restarts as `db-0`
- **Stable Storage**
  - Each pod gets its own PersistentVolume
  - Storage is reattached when the pod restarts
- **Ordered Operations**
  - Pods start in order (0 → 1 → 2)
  - Pods stop in reverse order

**Why this matters:**  
Databases rely on node identity, replication order, and persistent disks. Deployments cannot provide these guarantees.

---

## Choosing the Right Object

- Use **Pods** only for debugging or learning
- Use **Deployments** for stateless, horizontally scalable apps
- Use **StatefulSets** when identity, order, or persistent storage is required

**Mental Model:**  
Deployments treat pods like cattle.  
StatefulSets treat pods like named pets.


---

Class 4.3.2:
	Title: Services & Networking
	Description: Exposing applications.
Content Type: text
Duration: 450 
Order: 2
		Text Content :
 # Services: The Networking Abstraction

## 1. Service Types
Pods have dynamic IPs that change when they restart. A **Service** provides a stable IP (VIP) to access them.
* **ClusterIP (Default):** Internal only. For DBs or backend APIs.
* **NodePort:** Opens a port (30000-32767) on *every* worker node. Good for demos, bad for production security.
* **LoadBalancer:** Provisions a real Cloud Load Balancer (AWS ALB / Google LB). The standard for exposing apps to the internet.

---

## 2. Ingress (The Router)
* A Service works at Layer 4 (IP/Port). **Ingress** works at Layer 7 (HTTP/Hostname).
* Allows you to host multiple domains (`api.com`, `app.com`) on a single Load Balancer to save money.
* Requires an **Ingress Controller** (like Nginx or Traefik) to work.

Class 4.3.3:
	Title: ConfigMaps, Secrets, & Volumes
	Description: Configuration and storage.
Content Type: text
Duration: 400 
Order: 3
		Text Content :
 # Configuration & Storage

## 1. ConfigMaps vs. Secrets
* **ConfigMap:** Non-sensitive data (URLs, DB Hostnames).
* **Secret:** Sensitive data (Passwords, API Keys).
    * *Warning:* Kubernetes Secrets are just **Base64 encoded**, not encrypted by default. Anyone with API access can decode them.

---

## 2. Persistent Volumes (PV) & Claims (PVC)
* **PVC:** A developer's request: "I need 10GB of storage."
* **PV:** The actual storage (EBS Volume, Google Disk).
* **StorageClass:** Automatically creates the PV when a PVC is requested (Dynamic Provisioning).

Topic 4.4:
Title: Production Kubernetes
Order: 4

Class 4.4.1:
	Title: Health Checks & Probes
	Description: Liveness, Readiness, and Startup probes.
Content Type: text
Duration: 300 
Order: 1
		Text Content :
# Health Checks: Keeping Apps Alive

Kubernetes health probes allow the platform to understand the **real runtime state** of your application. They prevent broken containers from serving traffic and automatically recover applications without human intervention.

---

## 1. Liveness Probe (The Defibrillator)

The **Liveness Probe** answers one critical question:  
**“Is the application still alive, or is it stuck?”**

- Continuously checks the health of the container process
- If the probe fails repeatedly:
  - Kubelet **restarts the container**
- Used to detect:
  - Deadlocks
  - Infinite loops
  - Unresponsive processes

**When to use:**  
Applications that can appear “running” but internally stop making progress.

**Danger Zone:**  
A poorly configured liveness probe can cause restart loops.

---

## 2. Readiness Probe (The Traffic Light)

The **Readiness Probe** determines whether the application can **receive traffic**.

- If the probe fails:
  - The pod is **removed from Service endpoints**
  - No traffic is sent to it
  - The container is **not restarted**
- Common readiness failure scenarios:
  - Application startup
  - Cache warm-up
  - Database dependency unavailable
  - Temporary overload

**Key Insight:**  
Readiness controls traffic, not process lifecycle.

---

## 3. Startup Probe (The Legacy Helper)

The **Startup Probe** is designed for slow-starting applications.

- Temporarily disables:
  - Liveness probe
  - Readiness probe
- Gives the application enough time to boot
- Once startup probe succeeds:
  - Liveness and readiness probes take over

**Typical use case:**  
Large Java applications or monoliths with long initialization phases.

---

## Probe Comparison Table

| Probe Type       | Main Question                          | Failure Action                          | Typical Use Case                          |
|------------------|----------------------------------------|------------------------------------------|-------------------------------------------|
| Liveness Probe   | Is the app stuck or dead?               | Restart container                        | Deadlocks, infinite loops                 |
| Readiness Probe  | Can the app serve traffic?              | Remove from Service endpoints            | Startup, DB dependency, load shedding     |
| Startup Probe    | Has the app finished starting?          | Delay other probes                       | Slow-booting legacy applications          |

---

## Best Practice Summary

- Use **Readiness Probes** for traffic control
- Use **Liveness Probes** sparingly and carefully
- Use **Startup Probes** for slow-starting applications
- Never assume “container running” means “application healthy”

---

Class 4.4.2:
	Title: Resource Management
	Description: Requests, Limits, and QoS.
Content Type: text
Duration: 400 
Order: 2
		Text Content :
# Resource Management

Proper resource management is critical for cluster stability, predictable performance, and cost efficiency. Kubernetes enforces this through requests, limits, and autoscaling mechanisms.

---

## 1. Requests vs. Limits

Resources are defined at the **container level** and influence both scheduling and runtime behavior.

### Requests (Scheduling Guarantee)
- Represents the **minimum** resources a container requires.
- Used by the **Scheduler** to decide pod placement.
- If no node has sufficient available requested resources, the pod remains in `Pending`.

**Effect:**  
Requests define *capacity planning* for the cluster.

---

### Limits (Runtime Enforcement)
- Represents the **maximum** resources a container is allowed to consume.
- Enforced by the Linux kernel via **cgroups**.

Behavior differs by resource type:
- **CPU Limit**
  - CPU is throttled when the limit is reached.
  - Application slows down but keeps running.
- **Memory Limit**
  - Exceeding the limit triggers an **OOMKill**.
  - Container is terminated and restarted.

**Operational Rule:**  
Always set memory limits. Be cautious with CPU limits for latency-sensitive apps.

---

## 2. Horizontal Pod Autoscaler (HPA)

The **Horizontal Pod Autoscaler** scales the number of pods, not their size.

- Continuously monitors metrics such as:
  - CPU utilization
  - Memory utilization
  - Custom or external metrics (via Prometheus)
- Adjusts replica count within defined min/max bounds.

**Critical Dependency:**  
HPA relies on `resources.requests`.  
Without correct requests, utilization percentages are meaningless.

**Example:**  
If CPU request is `500m` and usage is `400m`, utilization = 80%.

---

# K8s Security: Zero Trust

Kubernetes follows a **secure-by-design but open-by-default** model. Security must be explicitly configured.

---

## 1. RBAC (Role-Based Access Control)

RBAC governs **who can do what** in the cluster.

- **Role**
  - Namespace-scoped
  - Example: Read-only access to pods in `dev`
- **ClusterRole**
  - Cluster-wide
  - Example: Read access to nodes or CRDs

**Best Practices:**
- Never use the default `admin` for applications
- Follow the **Principle of Least Privilege**
- Bind roles to Service Accounts, not users, for workloads

---

## 2. Network Policies (The Internal Firewall)

By default:
- Every pod can communicate with every other pod.

Network Policies allow you to:
- Restrict ingress and egress traffic
- Define allowed communication paths explicitly

**Recommended Pattern:**
- Apply a **Default Deny** policy
- Explicitly allow:
  - Frontend → Backend
  - Backend → Database

**Important Note:**  
Network Policies require a compatible CNI (Calico, Cilium, etc.).

---

# Autoscaling Comparison Table (HPA vs VPA vs Cluster Autoscaler)

| Feature                  | HPA (Horizontal Pod Autoscaler) | VPA (Vertical Pod Autoscaler) | CA (Cluster Autoscaler) |
|--------------------------|----------------------------------|--------------------------------|--------------------------|
| Scales What              | Number of pods                  | CPU/Memory per pod             | Number of nodes          |
| Scaling Direction        | Horizontal                      | Vertical                       | Infrastructure-level     |
| Trigger Metrics          | CPU, Memory, Custom metrics     | Historical usage analysis      | Pending pods             |
| Affects Pod Restart     | No                               | Yes (for updates)              | No                       |
| Used For                | Traffic-based scaling           | Right-sizing workloads         | Handling node shortages  |
| Common Use Case          | Web services, APIs               | Databases, batch jobs          | Burst traffic, HPA needs |
| Production Maturity     | Very stable                      | Use with caution               | Very stable              |

---

## Mental Model

- **HPA:** “Add more workers”
- **VPA:** “Give each worker more power”
- **Cluster Autoscaler:** “Buy more machines”


---

Class 4.4.4:
	Title: Kubernetes Troubleshooting
	Description: Debugging common failure states.
Content Type: text
Duration: 400 
Order: 4
		Text Content :
 # Troubleshooting: The Daily Grind

## 1. The "Big Three" Errors
* **CrashLoopBackOff:** The app started, crashed, and exited.
    * *Fix:* Check Application Logs (`kubectl logs`). It's usually a code error or missing env variable.
* **ImagePullBackOff:** K8s cannot download the Docker image.
    * *Fix:* Check image name spelling or Registry Authentication (ImagePullSecrets).
* **Pending:** The Scheduler cannot find a node to place the pod.
    * *Fix:* Check `kubectl describe pod`. Usually "Insufficient CPU/Memory" or Taint/Toleration mismatch.

---

## 2. Debugging Tools
* `kubectl describe pod <name>`: The first command you should run. Shows events and errors.
* `kubectl exec -it <name> -- /bin/bash`: SSH into the container to debug network/files.
---
Topic 4.5:
Title: Kubernetes - Challenge
Order: 5


Class 4.5.1:
	Title: Kubernetes Fundamentals - Challenge
	Description: Scenario-based K8s troubleshooting.
Content Type: text
Duration: 300 
Order: 1
		Text Content :
# Kubernetes Fundamentals – Challenge  
**Contest Format | 5 Questions**

These scenarios test your ability to **debug Kubernetes under pressure**, not just recall definitions.

---

## Question 1: Pod Troubleshooting (CrashLoopBackOff, ImagePullBackOff, Pending)

### Problem  
A production deployment has multiple pods in unhealthy states:
- Pod A: `CrashLoopBackOff`
- Pod B: `ImagePullBackOff`
- Pod C: `Pending`

**Tasks:**
1. Identify the root cause for each state.
2. Provide the correct debugging steps.
3. Propose fixes.

---

### Answer

**CrashLoopBackOff**
- Meaning: Container starts, crashes, and restarts repeatedly.
- Debug:
  ```bash
  kubectl logs pod-a
  kubectl describe pod pod-a


* Common causes:

  * Application crash
  * OOMKilled (memory limit too low)
* Fix:

  * Increase memory limit
  * Fix application startup error

**ImagePullBackOff**

* Meaning: Kubernetes cannot pull the container image.
* Debug:

  ```bash
  kubectl describe pod pod-b
  ```
* Common causes:

  * Wrong image name or tag
  * Missing imagePullSecrets for private registry
* Fix:

  * Correct image reference
  * Add proper registry credentials

**Pending**

* Meaning: Pod cannot be scheduled.
* Debug:

  ```bash
  kubectl describe pod pod-c
  ```
* Common causes:

  * Insufficient CPU/memory on nodes
  * Node selector or taint mismatch
* Fix:

  * Add capacity
  * Adjust resource requests or scheduling constraints

---

## Question 2: Service and Ingress Configuration

### Problem

A frontend application is deployed, but users cannot access it via the browser.

**Tasks:**

1. Identify what could be misconfigured.
2. Debug service and ingress issues.
3. Fix the exposure.

---

### Answer

**Debug Steps**

1. Check Service:

   ```bash
   kubectl get svc
   ```

   * Ensure correct `type` (ClusterIP / NodePort / LoadBalancer)
   * Validate `targetPort` matches container port

2. Verify Endpoints:

   ```bash
   kubectl get endpoints frontend-service
   ```

   * Empty endpoints indicate selector mismatch

3. Check Ingress:

   ```bash
   kubectl describe ingress frontend-ingress
   ```

   * Validate host/path rules
   * Ensure Ingress Controller is running

**Fix**

* Align Service selectors with pod labels
* Ensure Ingress routes to correct Service and port
* Confirm DNS points to Ingress Load Balancer

---

## Question 3: ConfigMaps and Secrets Management

### Problem

An application fails after deployment due to missing configuration and credentials.

**Tasks:**

1. Explain how ConfigMaps and Secrets should be used.
2. Debug common misconfigurations.
3. Fix securely.

---

### Answer

**ConfigMaps**

* Used for non-sensitive config (URLs, flags)
* Injected as:

  * Environment variables
  * Mounted files

**Secrets**

* Used for sensitive data (passwords, API keys)
* Base64-encoded, not encrypted by default

**Debug**

```bash
kubectl describe pod <pod>
kubectl get configmap
kubectl get secret
```

**Fix**

* Ensure correct key names
* Mount Secrets as environment variables or volumes
* Enable encryption at rest for Secrets

**Best Practice**
Never hardcode values in manifests or images.

---

## Question 4: Resource Requests and Limits Tuning

### Problem

Pods are getting evicted or throttled under load.

**Tasks:**

1. Explain how requests and limits work.
2. Identify misconfigurations.
3. Tune resources correctly.

---

### Answer

**Requests**

* Used by scheduler
* Determines pod placement

**Limits**

* Enforced by kubelet
* CPU: throttling
* Memory: OOMKill

**Debug**

```bash
kubectl describe pod <pod>
kubectl top pod
```

**Fix**

* Set realistic requests based on observed usage
* Avoid setting memory limit too close to average usage
* Ensure requests ≤ limits

**Production Tip**
Incorrect requests break HPA behavior.

---

## Question 5: RBAC and Network Policies Implementation

### Problem

A developer can delete resources they should not, and pods can talk to the database directly.

**Tasks:**

1. Lock down access using RBAC.
2. Restrict pod-to-pod communication.

---

### Answer

**RBAC**

* Use least privilege
* Create Roles instead of ClusterRoles when possible

```yaml
kind: Role
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]
```

**Network Policies**

* Kubernetes is open by default
* Implement default deny

```yaml
policyTypes:
- Ingress
```

* Explicitly allow:

  * Frontend → Backend
  * Backend → Database

**Result**

* Reduced blast radius
* Zero-trust inside the cluster

---

## Contest Evaluation Criteria

* Correct debugging flow
* Understanding of scheduler vs runtime issues
* Secure configuration practices
* Ability to explain cause and fix clearly

These scenarios represent **real on-call Kubernetes incidents**, not certification questions.

---

Topic 4.6:
Title: Advanced Kubernetes Topics
Order: 6

Class 4.6.1:
	Title: Extensibility - CRDs and Operators
	Description: Custom resources, validation, and operator patterns.
Content Type: text
Duration: 550 
Order: 1
		Text Content :
 # Kubernetes Extensibility: Building on the Platform

Kubernetes is not a monolith. It's a **platform** you can extend with Custom Resources and Operators.

---

## 1. Custom Resource Definitions (CRDs)

A CRD lets you define **new resource types** that behave like built-in Kubernetes resources.

### Creating a CRD

```yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: databases.myapp.io
spec:
  group: myapp.io
  names:
    kind: Database
    plural: databases
  scope: Namespaced
  versions:
  - name: v1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object
        properties:
          spec:
            type: object
            properties:
              engine:
                type: string
                enum: [postgres, mysql, mongodb]
              version:
                type: string
              storageSize:
                type: string
                pattern: '^\d+(Gi|Ti|Mi)$'
              backupEnabled:
                type: boolean
              maxConnections:
                type: integer
                minimum: 10
                maximum: 10000
            required: [engine, version, storageSize]
          status:
            type: object
            properties:
              phase:
                type: string
                enum: [Pending, Creating, Ready, Failed]
              endpoint:
                type: string
              lastBackup:
                type: string
                format: date-time
```

### Using the CRD

```yaml
apiVersion: myapp.io/v1
kind: Database
metadata:
  name: prod-db
  namespace: production
spec:
  engine: postgres
  version: "14.5"
  storageSize: 100Gi
  backupEnabled: true
  maxConnections: 500
```

### Validating Custom Resources

```bash
# The CRD enforces validation
kubectl apply -f database.yaml

# If spec is invalid:
# error validating "database.yaml": error validating: 
# storageSize: Invalid value: "100GB": must match pattern

# Validation rules can also prevent:
- Conflicting configurations
- Deprecated fields
- Resource limits
```

### CRD vs ConfigMap

| Aspect | CRD | ConfigMap |
| :--- | :--- | :--- |
| **Type Safety** | Strong (validates schema) | Weak (free-form) |
| **Kubectl Support** | Full (like native resources) | Limited |
| **Status Tracking** | Can have status subresource | No |
| **RBAC** | Fine-grained control | Less granular |
| **Use Case** | Complex domain objects | Simple configuration |

---

## 2. Kubernetes Operators

An Operator is a **pattern** that uses CRDs + custom controllers to manage complex applications.

### The Operator Pattern

```
CRD (What you want)
    ↓
Controller (Desired state reconciliation)
    ↓
Kubernetes Resources (Actual implementation)
```

### Example: Database Operator

```yaml
# User declares desired database
apiVersion: mydb.io/v1
kind: Database
metadata:
  name: mydb
spec:
  engine: postgres
  version: "14.5"
  replicas: 3
  storage: 100Gi

# Operator reconciles:
# 1. Creates StatefulSet with 3 replicas
# 2. Creates PVC for storage
# 3. Creates ConfigMap for configuration
# 4. Creates Secret for credentials
# 5. Sets up replication between replicas
# 6. Monitors health
# 7. Handles upgrades
```

### Operator SDK

The Operator SDK simplifies building operators.

```bash
# Install Operator SDK
brew install operator-sdk

# Create operator project
operator-sdk init --domain myapp.io --repo github.com/myorg/mydb-operator

# Create API and controller
operator-sdk create api --group mydb --version v1 --kind Database --resource --controller

# This generates:
# - CRD definition
# - Controller reconciliation logic
# - Example Custom Resource
```

### Reconciliation Loop (Heart of Operator)

```go
func (r *DatabaseReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
    // Get the Database resource
    db := &mydbv1.Database{}
    r.Get(ctx, req.NamespacedName, db)

    // Step 1: Create StatefulSet if it doesn't exist
    sts := &appsv1.StatefulSet{}
    if errors.IsNotFound(r.Get(ctx, ..., sts)) {
        newSts := r.constructStatefulSet(db)
        r.Create(ctx, newSts)
    }

    // Step 2: Create PVC for storage
    pvc := &corev1.PersistentVolumeClaim{}
    if errors.IsNotFound(r.Get(ctx, ..., pvc)) {
        newPvc := r.constructPVC(db)
        r.Create(ctx, newPvc)
    }

    // Step 3: Update status
    db.Status.Phase = "Ready"
    db.Status.Endpoint = "postgres.default.svc"
    r.Status().Update(ctx, db)

    // Requeue after 5 minutes to check status
    return ctrl.Result{RequeueAfter: 5 * time.Minute}, nil
}
```

---

## 3. Operator Lifecycle Manager (OLM)

OLM manages operator installation, updates, and dependencies.

### Installing an Operator via OLM

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: operators

---

apiVersion: operators.coreos.com/v1alpha1
kind: OperatorGroup
metadata:
  name: default
  namespace: operators

---

apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: postgres-operator
  namespace: operators
spec:
  channel: stable
  installPlanApproval: Automatic  # Auto-approve updates
  name: postgres-operator
  source: operatorhub  # From OperatorHub.io catalog
  sourceNamespace: olm
```

**Result:** Operator is installed and automatically updated.

---

## 4. Common Operators

| Operator | Purpose | Example Use |
| :--- | :--- | :--- |
| **PostgreSQL Operator** | Database provisioning | `kubectl apply -f postgres.yaml` |
| **MongoDB Enterprise Operator** | Database provisioning | `kubectl apply -f mongodb.yaml` |
| **Prometheus Operator** | Monitoring stack | Declarative monitoring |
| **Cert-Manager** | TLS certificates | Automatic HTTPS |
| **Nginx Ingress Controller** | Ingress management | Dynamic routing rules |

---

Class 4.6.2:
	Title: Advanced Networking and Service Mesh
	Description: CNI plugins, service mesh, and traffic management.
Content Type: text
Duration: 500 
Order: 2
		Text Content :
 # Advanced Kubernetes Networking

## 1. CNI (Container Network Interface) Plugins

CNI is the standard for Kubernetes networking. Different plugins offer different capabilities.

### Flannel (Simple)

```bash
# Install Flannel
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```

**Characteristics:**
- Simple, lightweight
- Flat network (all pods can reach each other)
- No advanced policies
- Good for small clusters

**Architecture:**
```
┌──────────┐    ┌──────────┐
│ Pod A    │    │ Pod B    │
│ 10.1.1.2 │    │ 10.1.2.3 │
└────┬─────┘    └────┬─────┘
     │               │
     └───────┬───────┘
             │
          VXLAN tunnel
          (Flannel backend)
             │
     ┌───────┴───────┐
     │   Etcd        │
     │  (Mapping)    │
     └───────────────┘
```

### Calico (Production)

```bash
# Install Calico
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/master/manifests/tigera-operator.yaml
```

**Characteristics:**
- **BGP-based** routing (scalable)
- Network policies (security)
- High performance
- Enterprise-grade

**Features:**
```yaml
# NetworkPolicy enforcement
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
```

### Cilium (Advanced)

```bash
# Install Cilium
helm repo add cilium https://helm.cilium.io
helm install cilium cilium/cilium --namespace kube-system
```

**Characteristics:**
- **eBPF-based** (kernel level, very fast)
- Fine-grained security policies
- Service mesh integration
- Observable

**Advantages:**
- Lower latency than iptables-based plugins
- Real-time visibility
- Cluster mesh (multi-cluster)

### CNI Comparison

| Aspect | Flannel | Calico | Cilium |
| :--- | :--- | :--- | :--- |
| **Routing** | VXLAN | BGP | eBPF |
| **Performance** | Good | Excellent | Excellent+ |
| **Policies** | No | Yes | Yes (advanced) |
| **Complexity** | Low | Medium | High |
| **Scale** | ~100 nodes | 1000+ nodes | 1000+ nodes |
| **Multi-cluster** | No | Yes | Yes (mesh) |

---

## 2. Service Mesh: Istio

A service mesh manages service-to-service communication.

### Why Service Mesh?

**Without Service Mesh:**
```
App A → App B (network reliability is app's problem)
      ↓
   App must handle:
   - Retries
   - Timeouts
   - Circuit breaker
   - Load balancing
   - Observability
```

**With Service Mesh:**
```
App A → Envoy (sidecar) → Envoy (sidecar) → App B
                 ↓              ↓
        All resilience handled by mesh!
```

### Installing Istio

```bash
# Download and install
curl -L https://istio.io/downloadIstio | sh
cd istio-*
export PATH=$PWD/bin:$PATH

# Install control plane
istioctl install --set profile=demo

# Enable sidecar injection (automatic)
kubectl label namespace default istio-injection=enabled
```

### Istio Resources

#### VirtualService (Traffic Management)

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: booking-vs
spec:
  hosts:
  - booking  # Service DNS name
  http:
  # Canary deployment: 10% to v2, 90% to v1
  - match:
    - uri:
        prefix: /api/v2
    route:
    - destination:
        host: booking
        subset: v2
  - route:
    - destination:
        host: booking
        subset: v1
      weight: 90
    - destination:
        host: booking
        subset: v2
      weight: 10
```

#### DestinationRule (Load Balancing)

```yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: booking-dr
spec:
  host: booking
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        maxRequestsPerConnection: 2
    outlierDetection:
      consecutive5xxErrors: 5
      interval: 30s
      baseEjectionTime: 30s
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
```

#### SecurityPolicy (mTLS)

```yaml
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
spec:
  mtls:
    mode: STRICT  # Require mTLS for all traffic
```

---

## 3. Observability in Service Mesh

Istio automatically integrates with observability tools.

```bash
# Enable metrics collection
kubectl apply -f - <<EOF
apiVersion: telemetry.istio.io/v1alpha1
kind: Telemetry
metadata:
  name: all-metrics
spec:
  metrics:
  - providers:
    - name: prometheus
    dimensions:
    - request_path
    - response_code
    - source_principal
EOF
```

**Metrics Available:**
- Request rate, latency, errors
- Service dependencies
- mTLS certificate expiration
- Sidecar resource usage

---

Class 4.6.3:
	Title: Cluster Operations and Scaling
	Description: Upgrades, multi-cluster, autoscaling, and scheduling.
Content Type: text
Duration: 550 
Order: 3
		Text Content :
 # Cluster Operations at Scale

## 1. Kubernetes Cluster Upgrades

Upgrading a Kubernetes cluster requires careful planning.

### Version Skew Policy

Kubernetes has strict version compatibility rules:

- **Kubelet** can be 2 versions behind API server
- **API Server** components must be within 1 version
- **etcd** must be within 1 version

```
Supported version combinations:
API Server: 1.25
kubelet:    1.25, 1.24, 1.23 (up to 2 versions behind)
kube-proxy: 1.25, 1.24
etcd:       3.5, 3.4
```

### Upgrade Strategy: Blue-Green

```
┌──────────────────────────────────────┐
│ Old Cluster (Blue) - v1.24           │
│ ✓ Running production traffic         │
└──────────────────────────────────────┘

            ↓ (Setup new)

┌──────────────────────────────────────┐
│ New Cluster (Green) - v1.25          │
│ ✓ Ready, waiting                     │
└──────────────────────────────────────┘

            ↓ (Migrate DNS)

┌──────────────────────────────────────┐
│ Old Cluster (Blue) - v1.24           │
│ Decommissioned                       │
└──────────────────────────────────────┘

┌──────────────────────────────────────┐
│ New Cluster (Green) - v1.25          │
│ ✓ Running production traffic         │
└──────────────────────────────────────┘
```

**Pros:** Instant rollback, zero downtime
**Cons:** Double infrastructure cost, data migration complexity

### Upgrade Strategy: Rolling (In-Place)

```bash
# 1. Cordon the node (no new pods)
kubectl cordon node-1

# 2. Drain existing pods (graceful shutdown)
kubectl drain node-1 --ignore-daemonsets

# 3. Upgrade kubelet
ssh node-1
apt-get upgrade kubeadm kubelet
systemctl restart kubelet

# 4. Uncordon
kubectl uncordon node-1

# Repeat for each node
```

**Pros:** Single cluster, minimal cost
**Cons:** Risk of compatibility issues, longer upgrade window

### Pre-Upgrade Checklist

```bash
# 1. Backup etcd
kubectl get --raw=/api/v1/nodes > nodes-backup.json

# 2. Check node readiness
kubectl get nodes

# 3. Check pod disruption budgets
kubectl get pdb -A

# 4. Verify storage
kubectl get pvc -A

# 5. Test on staging first!
```

---

## 2. Multi-Cluster Management

Managing multiple Kubernetes clusters brings new challenges.

### Federation (Kubernetes Federation v2 - KubeFed)

```yaml
apiVersion: core.kubefed.io/v1beta1
kind: KubeFederatedCluster
metadata:
  name: cluster-us-east
spec:
  clusterRef:
    name: us-east-1
  secretRef:
    name: us-east-1-secret
---
apiVersion: multiclusterdns.kubefed.io/v1alpha1
kind: IngressDNSRecord
metadata:
  name: global-app
spec:
  hosts:
  - global-app.example.com
  recordTTL: 60
```

**Result:** Traffic automatically routed to healthy clusters.

### GitOps Across Clusters

```bash
# Use ArgoCD with multiple clusters
---
apiVersion: v1
kind: Secret
metadata:
  name: cluster-us-east
  namespace: argocd
type: Opaque
data:
  name: dXMtZWFzdC0x  # us-east-1
  server: aHR0cHM6...  # https://api.us-east-1...

---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: app-multi-cluster
spec:
  project: default
  destination:
    server: "{{ .server }}"  # Dynamically set per cluster
  source:
    repoURL: https://github.com/myorg/app
    path: manifests
```

---

## 3. Cluster Autoscaler Deep-Dive

Cluster Autoscaler automatically adds/removes nodes based on demand.

### How It Works

```
Pending Pod
    ↓
Scheduler can't find node
    ↓
Pod stays Pending
    ↓
Cluster Autoscaler detects
    ↓
Adds node from node group
    ↓
Pod is scheduled
    ↓
Node healthy, ready for more pods
```

### Configuration

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-autoscaler-status
  namespace: kube-system
data:
  # Scale up 10% if utilization > 80%
  scale-down-delay-after-add: 10m
  scale-down-unneeded-time: 5m
  # Don't remove node if utilization > 65%
  scale-down-utilization-threshold: 0.65
```

### Node Groups

```bash
# AWS ASG example
# Create node group with specific instance type
aws autoscaling create-auto-scaling-group \
  --auto-scaling-group-name k8s-nodes \
  --min-size 1 \
  --max-size 10 \
  --desired-capacity 3 \
  --launch-template LaunchTemplateName=k8s-node
```

### Scale-Down Policies

```yaml
# Don't scale down a node if:
# 1. Pod has local storage
apiVersion: v1
kind: Pod
metadata:
  name: app-with-local-storage
spec:
  containers:
  - name: app
    volumeMounts:
    - name: cache
      mountPath: /cache
  volumes:
  - name: cache
    emptyDir: {}

# 2. Pod prevents disruption
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: app-pdb
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: critical-app
```

---

## 4. Container Storage Interface (CSI)

CSI standardizes how Kubernetes integrates with storage providers.

### CSI Drivers

```bash
# Install EBS CSI Driver (AWS)
helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver
helm install aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver \
  --namespace kube-system
```

### Dynamic Provisioning

```yaml
# StorageClass (template for volumes)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ebs-fast
provisioner: ebs.csi.aws.com  # CSI driver
parameters:
  type: gp3
  iops: "3000"
  throughput: "125"  # MB/s
  encrypted: "true"
allowVolumeExpansion: true

---

# PVC (request storage)
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: app-data
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: ebs-fast
  resources:
    requests:
      storage: 100Gi

---

# Pod (use the storage)
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - name: app
    image: myapp:latest
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: app-data
```

### Volume Snapshots

```yaml
# Create snapshot
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: app-data-snapshot
spec:
  volumeSnapshotClassName: ebs-snapshot
  source:
    persistentVolumeClaimName: app-data

---

# Restore from snapshot
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: app-data-restored
spec:
  dataSource:
    name: app-data-snapshot
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  resources:
    requests:
      storage: 100Gi
```

---

## 5. Kubernetes Scheduler Deep-Dive

The scheduler places pods on nodes based on constraints and preferences.

### Scheduling Queue

```
Unscheduled Pod
    ↓
Active Queue
    ↓
Filtering (remove unsuitable nodes)
    ↓
Scoring (rank remaining nodes)
    ↓
Binding (assign pod to best node)
```

### Filtering (Yes/No Decisions)

```
Node Requirements:
✓ Enough CPU/memory
✓ Tolerates taints
✓ Node selector matches
✓ Pod affinity satisfied
✓ Storage available
```

### Scoring (Ranking)

```
Pod affinity (prefer same zone as other pods): +50 points
Least packed (spread pods across nodes): +40 points
Node affinity (prefer specific node): +30 points
... (other factors)
Winner: node with highest score
```

### Custom Scheduler

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  schedulerName: my-custom-scheduler  # Use custom scheduler
  containers:
  - name: app
    image: myapp:latest
```

### Pod Priority and Preemption

```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000  # Higher number = higher priority
globalDefault: false
description: "Critical workloads"

---

apiVersion: v1
kind: Pod
metadata:
  name: critical-pod
spec:
  priorityClassName: high-priority
  containers:
  - name: app
    image: critical-app:latest
```

**When cluster is full:**
- Low-priority pods are evicted
- High-priority pod is scheduled

---

## 6. Leader Election

Used in controllers to ensure only one is active.

### ConfigMap-Based (Simple but Slow)

```go
import "k8s.io/client-go/tools/leaderelection"

lock := &corev1.ConfigMap{
    ObjectMeta: metav1.ObjectMeta{
        Name:      "controller-leader",
        Namespace: "default",
    },
}

leaderelection.RunOrDie(ctx, leaderelection.LeaderElectionConfig{
    Lock:            lock,
    LeaseDuration:   15 * time.Second,
    RenewDeadline:   10 * time.Second,
    RetryPeriod:     2 * time.Second,
    Callbacks: leaderelection.LeaderCallbacks{
        OnStartedLeading: func(ctx context.Context) {
            log.Info("Starting leadership")
            r.Reconcile(ctx)
        },
        OnStoppedLeading: func() {
            log.Info("Lost leadership")
        },
    },
})
```

### Lease-Based (Modern, Recommended)

```yaml
apiVersion: coordination.k8s.io/v1
kind: Lease
metadata:
  name: my-controller-lease
  namespace: default
spec:
  leaseDurationSeconds: 15
  acquireTime: "2024-01-15T10:00:00Z"
  renewTime: "2024-01-15T10:00:05Z"
```

---

---

Module 5:
Title: CI/CD & Automation
Description: Master continuous integration and deployment pipelines. Learn to automate build, test, and deployment workflows using industry-standard tools.
Order: 5
Learning Outcomes:
Design and implement CI/CD pipelines
Master Jenkins, GitLab CI, GitHub Actions
Understand deployment strategies
Automate testing and quality checks

Topic 5.1:
Title: CI/CD Fundamentals
Order: 1

Class 5.1.1:
	Title: CI/CD Concepts & Best Practices
	Description: Understanding the pipeline, version control strategies, and DORA metrics.
Content Type: text
Duration: 500 
Order: 1
		Text Content :
 # CI/CD: The Engine of DevOps

## 1. What is CI/CD?
In an interview, do not just expand the acronym. Explain the **workflow**.

* **Continuous Integration (CI):** Developers merge code into a shared repository frequently (multiple times a day). Each merge triggers an automated build and test sequence.
    * *Goal:* Detect bugs early ("Fail Fast").
    * **Real impact:** A bug caught in CI takes minutes to fix. The same bug in production takes hours to debug, deploy, and recover from.
* **Continuous Delivery (CD):** The code is built, tested, and ready to release to production at any time. The final deployment to production is a **manual decision** (click a button).
    * **Human gate:** A person decides "Yes, ship it"
    * Reduces risk by introducing a decision point
* **Continuous Deployment (CD):** Every change that passes the automated tests is deployed to production **automatically**. No human intervention.
    * *Risk:* Requires extremely robust automated testing.
    * *Benefit:* Fastest feedback loop. Users see changes in minutes.

---

## 2. The CI/CD Pipeline Flow

```
Developer commits code
        ↓
Webhook triggers CI/CD
        ↓
┌─────────────────────────────┐
│ Continuous Integration (CI) │
├─────────────────────────────┤
│ 1. Checkout code            │
│ 2. Build artifact           │
│ 3. Run unit tests           │
│ 4. Static code analysis     │
│ 5. Security scanning        │
└────────┬────────────────────┘
         ↓
    Tests pass?
    ├─ NO → Notify dev, stop
    └─ YES ↓
┌─────────────────────────────┐
│ Continuous Delivery (CD)    │
├─────────────────────────────┤
│ 1. Deploy to staging        │
│ 2. Run integration tests    │
│ 3. Run E2E tests            │
│ 4. Performance tests        │
└────────┬────────────────────┘
         ↓
    Ready for prod?
    ├─ NO → Halt, wait for manual approval
    └─ YES (Manual gate or auto) ↓
┌─────────────────────────────┐
│ Continuous Deployment       │
├─────────────────────────────┤
│ 1. Deploy to production     │
│ 2. Health checks            │
│ 3. Smoke tests              │
│ 4. Monitor metrics          │
└─────────────────────────────┘
        ↓
Users see the change
```

---

## 3. Version Control Strategies

Your pipeline strategy depends on your branching strategy. This choice directly impacts deployment frequency and risk.

### Git Flow (Safe but Slow)

```
main (production) ← releases only
   ↑
develop (staging)
   ↑
feature/xxx (developer)
   ↑
hotfix/xxx (emergency fixes)
```

**Characteristics:**
- Two long-lived branches: `main` and `develop`
- Feature branches live for days/weeks
- Multiple merge steps before production
- Explicit versioning

**Pros:**
- Clear separation of concerns
- Safe for large teams
- Release planning is visible

**Cons:**
- Slow lead time (days to production)
- Merge conflicts are common
- Not suitable for high-frequency deployments

**Best for:** Large enterprises with strict release gates

---

### Trunk-Based Development (Fast and Lean)

```
main (production + staging)
 ↑ ↑ ↑ ↑ ↑ ↑
↓ ↓ ↓ ↓ ↓ ↓ (feature branches, live ≤ 1 day)
feature/xxx (many developers)
```

**Characteristics:**
- Single long-lived branch: `main`
- Feature branches are **short-lived** (hours to a day)
- Frequent merges (multiple per day)
- Relies on **feature flags** to control rollout

**Pros:**
- Low merge conflict risk
- Fast lead time (minutes to production)
- Enables continuous deployment
- Scales with large teams

**Cons:**
- Requires robust automated testing
- Feature flags add complexity
- Main branch must always be releasable

**Feature Flags in Action:**
```python
# Feature flag library
if is_feature_enabled('new_checkout'):
    # Use new checkout flow
    return new_checkout()
else:
    # Use old checkout flow
    return old_checkout()
```

**The DevOps Gold Standard:**
High-performing DevOps teams use **Trunk-Based Development + Feature Flags + Continuous Deployment**.

---

## 4. Key Metrics (DORA Metrics)

Google's DORA (DevOps Research and Assessment) research identified 4 metrics that predict organizational performance. These are **interview gold**.

### The Four Key Metrics

#### 1. Deployment Frequency
**Question:** How often do you deploy to production?

```
Elite performers:   On-demand (multiple per day, per hour)
High performers:    Weekly
Medium performers:  Monthly
Low performers:     Quarterly
```

**Why it matters:**
- Fast deployments = quick feedback loops
- Users see features sooner
- Recovery from failures is faster

**Interview Q:** "What's your current deployment frequency?"

---

#### 2. Lead Time for Changes
**Question:** How long from commit to production?

```
Elite performers:   < 1 hour
High performers:    < 1 day
Medium performers:  < 1 week
Low performers:     > 1 month
```

**Calculation:**
```
Lead Time = (Time code merged) to (Time in production)
```

**Why it matters:**
- Shorter lead time = faster innovation
- Quicker response to production issues
- Feedback to developers is immediate

---

#### 3. Change Failure Rate
**Question:** What percentage of deployments cause production incidents?

```
Elite performers:   0-15%
High performers:    15-45%
Medium performers:  45-60%
Low performers:     > 60%
```

**Formula:**
```
CFR = (Failed deployments) / (Total deployments) × 100%
```

**Example:**
- Deployed 100 times this month
- 5 caused incidents
- CFR = 5%

**Why it matters:**
- Indicates testing quality
- Reveals risk management effectiveness
- Lower CFR = safer deployments

---

#### 4. Mean Time to Recovery (MTTR)
**Question:** When production breaks, how fast can you fix it?

```
Elite performers:   < 1 hour
High performers:    < 1 day
Medium performers:  < 1 week
Low performers:     > 1 month
```

**MTTR Calculation:**
```
MTTR = (Detection time) + (Remediation time)
```

**Example Incident:**
- Bug discovered at 10:00 AM
- Fixed and deployed at 10:45 AM
- MTTR = 45 minutes

**Why it matters:**
- Incidents are inevitable
- Recovery speed minimizes user impact
- Enables safer, faster deployments

---

## 5. Relationship Between Metrics

```
High Deployment Frequency
        ↓
More chances to catch bugs
        ↓
Lower Change Failure Rate
        ↓
Faster MTTR (each change is smaller)
        ↓
More deployments possible
        ↓
Positive feedback loop!
```

**Counterintuitive truth:**
Deploying *more often* with smaller changes is *safer* than deploying infrequently with large changes.

---

Class 5.1.2:
	Title: Pipeline Design and Best Practices
	Description: Building reliable, fast, and secure pipelines.
Content Type: text
Duration: 450 
Order: 2
		Text Content :
 # Designing Production-Grade Pipelines

## 1. Pipeline Stages

A well-designed pipeline has clear stages, each with a specific purpose.

### Stage 1: Build (Minutes)

```bash
# Compile source code
# Run syntax checks
# Create artifact (binary, JAR, Docker image)
```

**Gate:** Compilation succeeds, artifact is created

**Failure Handling:** Notify developer immediately

---

### Stage 2: Unit Tests (5-10 minutes)

```bash
# Run fast, isolated unit tests
# No external dependencies (mock databases)
# Tests in parallel to save time
```

**Acceptance Criteria:**
- Coverage > 80%
- All tests pass
- No flaky tests

**Pro Tip:** If a unit test takes > 1 second, it's not really a unit test.

---

### Stage 3: Static Analysis (5-10 minutes)

```bash
# SAST: Scan source code for vulnerabilities
# Linting: Check code style (eslint, pylint)
# Complexity analysis: Cyclomatic complexity
# Dependency scanning: Known CVEs in libraries
```

**Tools:**
- SonarQube (comprehensive)
- Semgrep (fast, rules-based)
- Snyk (dependency scanning)

**Gate:** No critical/high-severity issues

---

### Stage 4: Build Artifact (5-15 minutes)

```bash
# Create docker image / JAR / binary
# Tag with commit SHA
# Push to artifact repository (Docker Hub, Artifactory)
```

**Best Practice:** Tag with both commit SHA and branch name

```bash
docker build -t myapp:sha-abc123 .
docker build -t myapp:main .
docker push myapp:sha-abc123
docker push myapp:main
```

---

### Stage 5: Deploy to Staging (10-30 minutes)

```bash
# Pull artifact
# Deploy to staging cluster
# Wait for health checks to pass
# Smoke tests
```

**Staging Environment = Production Copy**

Staging should mirror production exactly:
- Same infrastructure
- Same data (anonymized)
- Same monitoring
- Same security policies

---

### Stage 6: Integration & E2E Tests (20-60 minutes)

```bash
# Run tests against staging deployment
# Test full user flows (login, checkout, etc.)
# Load testing (simulate user traffic)
# Smoke tests (basic health checks)
```

**Example E2E Test:**
```python
def test_user_checkout_flow():
    # 1. Login
    response = login("user@example.com", "password")
    assert response.status == 200
    
    # 2. Add item to cart
    response = add_to_cart("product-123")
    assert response.status == 200
    
    # 3. Checkout
    response = checkout()
    assert response.status == 200
    assert order_created()
```

**Gate:** All tests pass, error rates acceptable

---

### Stage 7: Manual Approval (Explicit Gate)

```
PR approved by 2 engineers
 ↓
All tests passing
 ↓
Deploy to Production [APPROVED/REJECTED]
```

**Who approves?**
- Release manager
- On-call engineer
- Product lead

**Why explicit gate?**
- Accountability
- One last chance to catch issues
- Business decision (not just tech)

---

### Stage 8: Deploy to Production (5-30 minutes)

```bash
# Pull artifact (same one that passed staging)
# Deploy to prod
# Gradual rollout (canary or blue-green)
# Health checks
# Automatic rollback on failure
```

**Strategies:**
- **Canary:** 10% of traffic → 50% → 100%
- **Blue-Green:** Old version running alongside new, switch traffic
- **Rolling:** Gradually replace old pods with new ones

---

## 2. Pipeline Performance

Slow pipelines discourage frequent commits and feedback loops.

### Metrics
- Build time: Should be < 10 minutes for most projects
- Test time: Parallel execution is key
- Total lead time: < 30 minutes from commit to production-ready

### Optimization Techniques

**Parallel Execution**
```yaml
stages:
  build:
    jobs:
      - compile
      - unit_tests        # These run in parallel
      - lint_code         # Doesn't wait for compile
      - scan_dependencies
```

**Caching**
```yaml
cache:
  paths:
    - node_modules/     # Don't reinstall on every run
    - .gradle/          # Cache Gradle dependencies
```

**Matrix Strategy** (Run same job with different inputs)
```yaml
test:
  strategy:
    matrix:
      python-version: ['3.8', '3.9', '3.10', '3.11']
      # Runs test job 4 times in parallel
```

---

## 3. Pipeline Security

### Secret Management

**Never commit secrets to Git:**
```bash
# WRONG
export AWS_SECRET_ACCESS_KEY=AKIA...  # In Jenkinsfile
export DB_PASSWORD=secret123           # In .gitlab-ci.yml

# RIGHT
export AWS_SECRET_ACCESS_KEY=${AWS_SECRET}  # From CI/CD secrets
export DB_PASSWORD=${DB_PASSWORD}           # From vault
```

**Use Platform Secrets:**
- GitHub: Settings → Secrets
- GitLab: CI/CD → Variables
- Jenkins: Credentials plugin

---

### Artifact Signing

```bash
# Sign the docker image
cosign sign myapp:sha-abc123

# Verify before deployment
cosign verify myapp:sha-abc123
```

**Benefits:**
- Prove artifact authenticity
- Prevent tampering
- Satisfy compliance requirements

---

Topic 5.2:
Title: CI/CD Tools
Order: 2

Class 5.2.1:
	Title: Jenkins - The Industry Standard
	Description: Master-Agent architecture and Declarative Pipelines.
Content Type: text
Duration: 500 
Order: 1
		Text Content :
# Jenkins: The Old Guard

Jenkins is one of the oldest and most widely adopted CI/CD tools. While newer platforms offer managed alternatives, Jenkins remains dominant due to its flexibility, massive plugin ecosystem, and deep enterprise adoption.

---

## 1. Jenkins Architecture

Jenkins follows a **controller–agent** (master–slave) architecture and should be treated as a distributed system.

### Controller (Master Node)
- Acts as the **control plane** of Jenkins.
- Responsibilities:
  - Scheduling jobs
  - Managing pipelines
  - Serving the web UI and REST API
  - Coordinating agents
- Should be kept **lightweight**:
  - No builds
  - No heavy scripts
  - No Docker builds

**Why:**  
Overloading the controller risks UI freezes and pipeline instability.

---

### Agents (Worker Nodes)

Agents execute the actual CI/CD workloads.

- Run builds, tests, and deployments
- Can be:
  - Static VMs
  - Docker containers
  - Kubernetes Pods
- Scale independently from the controller

**Best Practice:**  
Agents should be disposable and stateless.

---

## 2. Pipeline as Code (Jenkinsfile)

Modern Jenkins pipelines are defined entirely as code.

### Jenkinsfile
- Stored alongside application code in Git
- Enables:
  - Version control
  - Code review
  - Reproducible pipelines

### Declarative Pipeline (Recommended)

- Opinionated and structured
- Built-in error handling and validation
- Easier for teams to maintain

```groovy
pipeline {
    agent any
    stages {
        stage('Build') {
            steps { sh 'make' }
        }
        stage('Test') {
            steps { sh 'make test' }
        }
    }
}
```

---

Class 5.2.2:
	Title: GitLab CI/CD
	Description: The integrated approach with .gitlab-ci.yml.
Content Type: text
Duration: 450 
Order: 2
		Text Content :
 # GitLab CI/CD: The Modern Challenger

## 1. The .gitlab-ci.yml
Unlike Jenkins, GitLab CI is integrated directly into the source control platform.
* Configuration is defined in a YAML file at the root of the repo.
* **Stages:** Define the order (Build -> Test -> Deploy).
* **Jobs:** The actual scripts to run.

---

## 2. Runners
* **Shared Runners:** Hosted by GitLab (free minutes).
* **Specific Runners:** You install the GitLab Runner agent on your own EC2 instance. Secure and faster for heavy workloads.
* **Executors:**
    * *Shell Executor:* Runs commands directly on the server (simple but risky).
    * *Docker Executor:* Runs every job in a clean, isolated Docker container. **This is the industry standard.**

---

## 3. Docker-in-Docker (DinD)
To build a Docker image *inside* a CI pipeline (which is itself running in Docker), you need DinD.
* *The Trick:* You mount the `/var/run/docker.sock` from the host to the container. This allows the inner container to talk to the outer Docker Daemon.

---

Class 5.2.3:
	Title: GitHub Actions
	Description: Event-driven workflows and marketplace actions.
Content Type: text
Duration: 450 
Order: 3
		Text Content :
# GitHub Actions: The Developer's Favorite

GitHub Actions (GHA) is a modern CI/CD platform built directly into GitHub. Its tight integration with repositories, pull requests, and issues makes it extremely attractive to development teams.

---

## 1. Workflows & Events

GitHub Actions is **event-driven by design**. A workflow is a YAML file stored in `.github/workflows/` that defines *when* and *how* automation runs.

- Workflows react to events generated by GitHub.
- Each workflow can have:
  - One or more triggers
  - One or more jobs
  - One or more steps per job

This model aligns CI/CD directly with developer activity.

---

## 2. Triggers (`on`)

Triggers define **when** a workflow should run.

- `push`
  - Runs on every commit pushed to a branch
  - Commonly used for continuous integration
- `pull_request`
  - Runs when a PR is opened, updated, or synchronized
  - Used for validation before merging
- `schedule`
  - Runs workflows on a cron-based schedule
  - Commonly used for nightly tests or security scans
- `workflow_dispatch`
  - Allows manual execution from the GitHub UI
  - Useful for production deployments or ad-hoc tasks

---

## 3. The Marketplace

The GitHub Actions Marketplace is a major differentiator.

- Thousands of pre-built, reusable actions
- Reduces boilerplate scripting
- Encourages best practices through community-vetted actions

Example: Authenticating to AWS without custom scripts

```yaml
- uses: aws-actions/configure-aws-credentials@v1
  with:
    aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
    aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
    aws-region: us-east-1
````

**Key Advantage:**
You compose pipelines instead of writing glue code.

---

## Trigger Types Explained

| Trigger               | When It Runs                       | Typical Use Case                           |
| --------------------- | ---------------------------------- | ------------------------------------------ |
| `push`                | Code is pushed to a branch         | CI builds, unit tests                      |
| `pull_request`        | PR opened or updated               | Code validation, linting, reviews          |
| `schedule`            | Based on cron expression           | Nightly tests, backups, security scans     |
| `workflow_dispatch`   | Manually triggered via UI          | Deployments, hotfixes, one-off jobs        |
| `release`             | GitHub release is published        | Packaging and publishing artifacts         |
| `repository_dispatch` | External API triggers the workflow | Cross-repo automation, custom integrations |

---

## Mental Model

* **Jenkins:** You manage the system that runs pipelines
* **GitHub Actions:** GitHub runs pipelines *for you*

**Takeaway:**
GitHub Actions shifts CI/CD closer to developers while reducing operational overhead.

---

Topic 5.3:
Title: Deployment Strategies
Order: 3

Class 5.3.1:
	Title: Deployment Patterns and Rollback Strategies
	Description: Canary, Blue-Green, Rolling, and Feature Flags.
Content Type: text
Duration: 500 
Order: 1
		Text Content :
 # Deployment Strategies: Shipping Safely

The strategy you choose directly impacts your ability to respond to failures. Picking the right strategy for your application is critical.

---

## 1. Rolling Deployment (Traditional)

Gradually replace old pods with new ones.

```
V1 V1 V1 V1 (4 running)
   ↓
V1 V1 V1 V2 (1 new)
   ↓
V1 V1 V2 V2 (2 new)
   ↓
V1 V2 V2 V2 (3 new)
   ↓
V2 V2 V2 V2 (all new)
```

**Kubernetes:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 4
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1        # Max 1 extra pod during update
      maxUnavailable: 1  # Max 1 pod down during update
  template:
    spec:
      containers:
      - name: app
        image: myapp:v2
```

**Pros:**
- Simple, no extra infrastructure
- Gradual traffic shift
- Automatic rollback on health check failure

**Cons:**
- No easy rollback (old code is gone)
- Can't test old + new version together
- Database migrations must be backward compatible

---

## 2. Blue-Green Deployment (Safest)

Two identical production environments. Switch traffic between them.

```
┌─────────────────────┐
│ Blue (v1)           │
│ ✓ Running traffic   │
└─────────────────────┘
         ↓
┌─────────────────────┐
│ Green (v2)          │
│ ✓ Ready, not in use │
└─────────────────────┘
         ↓ (Switch DNS/LB)
┌─────────────────────┐
│ Blue (v1) - stopped │
│ Green (v2) running  │
└─────────────────────┘
```

**Kubernetes Implementation:**
```yaml
# Blue environment
apiVersion: v1
kind: Service
metadata:
  name: myapp
spec:
  selector:
    version: blue  # Points to blue deployment
  ports:
  - port: 80
    targetPort: 8080

---

# Blue deployment (current)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-blue
spec:
  selector:
    matchLabels:
      version: blue
  template:
    metadata:
      labels:
        version: blue
    spec:
      containers:
      - name: app
        image: myapp:v1

---

# Green deployment (new)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-green
spec:
  selector:
    matchLabels:
      version: green
  template:
    metadata:
      labels:
        version: green
    spec:
      containers:
      - name: app
        image: myapp:v2
```

**Switching Traffic:**
```bash
# Verify green is healthy
kubectl get pods -l version=green

# Update service to point to green
kubectl patch service myapp -p '{"spec":{"selector":{"version":"green"}}}'

# If issue detected, instant rollback
kubectl patch service myapp -p '{"spec":{"selector":{"version":"blue"}}}'
```

**Pros:**
- Instant rollback (one kubectl command)
- Can test new version fully before switching
- Zero downtime
- Supports database migrations

**Cons:**
- Requires double infrastructure (2x cost)
- Both versions must coexist in production

---

## 3. Canary Deployment (Balanced)

Send a small percentage of traffic to the new version. Gradually increase.

```
Traffic: 100% → 95% v1, 5% v2
         ↓
         95% v1, 10% v2
         ↓
         90% v1, 10% v2
         ↓
         50% v1, 50% v2
         ↓
         0% v1, 100% v2
```

**With Istio:**
```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: myapp
spec:
  hosts:
  - myapp.example.com
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        host: myapp
        subset: v1
      weight: 95
    - destination:
        host: myapp
        subset: v2
      weight: 5

---

apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: myapp
spec:
  host: myapp
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 100
        http2MaxRequests: 100
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
```

**Automated Canary (with Flagger):**
```yaml
apiVersion: flagger.app/v1beta1
kind: Canary
metadata:
  name: myapp
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  service:
    port: 8080
  analysis:
    interval: 30s
    threshold: 5
    maxWeight: 50
    stepWeight: 5
  metrics:
  - name: error-rate
    thresholdRange:
      max: 1
    interval: 1m
  - name: latency
    thresholdRange:
      max: 500
    interval: 1m
```

**Pros:**
- Risk-limited (only 5% affected)
- Automatic rollback on metrics breach
- Detects issues before full rollout
- Minimal extra infrastructure

**Cons:**
- More complex to set up
- Requires good metrics/observability
- Slower rollout (takes time)

---

## 4. Feature Flags (Application-Level)

```python
# Feature flag in code
def checkout_page():
    if is_feature_enabled('new_checkout'):
        return new_checkout()
    else:
        return old_checkout()
```

**Deploy Without Restart:**
- Change flag = instant behavior change
- No redeployment needed
- Rollback is instant

**Tools:**
- LaunchDarkly
- Flagsmith
- Unleash
- ConfigCat

---

Module 6:
Title: Infrastructure as Code
Description: Learn to manage infrastructure through code using Terraform, Ansible, and other IaC tools. Master declarative infrastructure management and configuration at scale.
Order: 6
Learning Outcomes:
Master Terraform for infrastructure provisioning
Implement configuration management with Ansible
Understand IaC best practices
Manage infrastructure state effectively

Topic 6.1:
Title: Terraform Fundamentals
Order: 1

Class 6.1.1:
	Title: Terraform Core Concepts
	Description: Declarative infrastructure and the workflow.
Content Type: text
Duration: 450 
Order: 1
		Text Content :
 # Terraform: The Industry Standard

## 1. IaC Philosophy
In the old days, we clicked buttons in the AWS Console. This was "ClickOps." It is unrepeatable, error-prone, and slow.
* **Infrastructure as Code (IaC):** We define infrastructure in text files. We version control it (Git). We review it (PRs).
* **Declarative (Terraform) vs. Imperative (Python/Bash):**
    * *Imperative:* "Make a server. Then add a disk. Then start the network." (Focus on *How*).
    * *Declarative:* "I want 1 Server with 1 Disk and 1 Network." (Focus on *What*). Terraform figures out the "How."

---

## 2. The Workflow
This is the "Red-Green-Refactor" of DevOps.
1.  `terraform init`: Downloads the providers (drivers) for AWS/Azure.
2.  `terraform plan`: **The Dry Run.** It compares your code to the live cloud and tells you what *will* change. Always read the plan!
3.  `terraform apply`: Makes the API calls to build the infrastructure.
4.  `terraform destroy`: Tears it all down.

---

## 3. HCL (HashiCorp Configuration Language)
HCL is designed to be human-readable.
* **Provider:** Who are we talking to? (AWS, Azure).
* **Resource:** What are we building? (EC2, S3).
    ```hcl
    resource "aws_s3_bucket" "my_bucket" {
      bucket = "my-unique-bucket-name"
    }
    ```
* **Data Source:** Read-only. "Go fetch the ID of the existing VPC."

---

Class 6.1.2:
	Title: State Management
	Description: The most critical concept in Terraform.
Content Type: text
Duration: 450 
Order: 2
		Text Content :
# Terraform State: The Holy Grail

Terraform state is the backbone of how Terraform understands and manages real infrastructure. Without state, Terraform cannot safely create, update, or destroy resources.

---

## 1. What is the State File? (`terraform.tfstate`)

Terraform is **declarative**, but it is not omniscient.

- The state file is Terraform’s **source of truth** for:
  - What resources exist
  - Their real-world identifiers
  - Their current attributes
- It maps configuration to reality:
  - Code: `resource "aws_instance" "web"`
  - Reality: `i-1234567890abcdef0`

Why this matters:
- Terraform compares:
  - **Desired state** (your code)
  - **Current state** (tfstate)
- Then generates an execution plan.

### Security Implications
- The state file may contain:
  - Database passwords
  - API tokens
  - Private IPs and metadata
- Data is stored in **plain text JSON**.

**Rule:** Treat the state file like production secrets.

---

## 2. Remote State (S3 + DynamoDB)

Local state does not scale beyond a single engineer.

### Problems with Local State
- **Security risk:** Local machines and Git repos are not secure secret stores.
- **Concurrency risk:** Two engineers running `terraform apply` at the same time can corrupt state.
- **Lack of visibility:** No shared source of truth.

---

### The Production-Grade Solution

Terraform supports **remote backends**.

- **S3**
  - Stores the state file centrally
  - Supports encryption at rest
  - Highly durable
- **DynamoDB**
  - Provides **state locking**
  - Prevents concurrent `apply` operations

**How Locking Works:**
- When `terraform apply` starts:
  - A lock record is written to DynamoDB
- While the lock exists:
  - Other applies are blocked
- Lock is released after completion

**Result:** Safe, collaborative infrastructure changes.

---

## 3. Importing Existing Infrastructure

Not all infrastructure starts as code.

### The Reality
- Many AWS accounts are built via:
  - Console clicks
  - Manual CLI commands
- Terraform does not know about these resources.

---

### `terraform import`

- Imports existing resources into the state file
- Does **not** generate Terraform code automatically
- Requires:
  - Writing the resource block manually
  - Importing the resource ID

Example:
```bash
terraform import aws_instance.web i-1234567890abcdef0
```



---

Class 6.1.3:
	Title: Advanced Terraform
	Description: Modules, Workspaces, and Logic.
Content Type: text
Duration: 500 
Order: 3
		Text Content :
# Advanced Terraform: Scaling Up

As infrastructure grows, Terraform usage must evolve from single-file configs into modular, reusable, and environment-aware systems. These patterns are essential for managing production-scale infrastructure safely.

---

## 1. Modules (Don’t Repeat Yourself)

Modules are Terraform’s primary abstraction mechanism.

- A **module** is a reusable collection of Terraform configuration files.
- Conceptually similar to a function:
  - Input variables = function arguments
  - Resources = function body
  - Outputs = return values

### Module Types
- **Root Module**
  - The directory where `terraform init/plan/apply` is executed
  - Orchestrates infrastructure composition
- **Child Modules**
  - Encapsulate reusable infrastructure patterns
  - Examples:
    - Standard VPC
    - ECS cluster
    - RDS instance

**Why modules matter:**
- Eliminate copy-paste
- Enforce standards
- Enable centralized updates

**Production Pattern:**  
Version modules and consume them like dependencies.

---

## 2. Workspaces

Workspaces allow multiple **state files** to be managed from the same codebase.

- Each workspace:
  - Has its own `terraform.tfstate`
  - Shares the same configuration
- Commonly used for:
  - Dev
  - Staging
  - Production

Example:
```bash
terraform workspace new dev
terraform workspace select dev
````

### Limitations of Workspaces

* Easy to apply changes to the wrong environment
* All environments share the same backend configuration
* Harder to reason about blast radius

**Enterprise Best Practice:**
Prefer separate directories or repositories per environment:

```
/env/dev
/env/staging
/env/prod
```

Workspaces are best suited for **lightweight or non-critical environments**.

---

## 3. Dynamic Blocks & Loops

Terraform supports controlled iteration to generate resources programmatically.

---

### `count`

* Creates multiple identical resources
* Indexed numerically

Example:

```hcl
resource "aws_instance" "web" {
  count = 5
}
```

**Downside:**
Removing an item in the middle shifts indexes and forces resource recreation.

---

### `for_each`

* Iterates over a map or set
* Keys provide stable identity

Example:

```hcl
resource "aws_instance" "web" {
  for_each = {
    app1 = "t3.micro"
    app2 = "t3.small"
  }
}
```

**Why `for_each` is better:**

* Predictable resource lifecycle
* Safer refactoring
* Cleaner diffs

---

## Key Takeaways

* Modules enable reuse and standardization
* Workspaces isolate state, not configuration
* Prefer directory-based environment isolation at scale
* Use `for_each` over `count` whenever possible

**Mental Model:**
Terraform scales best when treated like a software project, not a script.

---

Class 6.1.4:
	Title: Terraform Best Practices
	Description: Security, Testing, and CI/CD.
Content Type: text
Duration: 350 
Order: 4
		Text Content :
 # Terraform Best Practices

## 1. Code Organization
* **Small State Files:** Don't put the entire company infrastructure in one state file. If that file corrupts, the company stops.
* **Layering:** Separate state for Networking (VPC) vs. Apps (EC2). The Apps read the Networking data using `data sources`.

---

## 2. Testing (Terratest)
* **`terraform validate`:** Checks syntax.
* **`terraform plan`:** Checks intent.
* **Terratest (Go):** Deploys real infrastructure, pings it to see if it works, and then destroys it.

---

## 3. CI/CD Integration (Atlantis)
Stop running `terraform apply` from your laptop.
* **Atlantis:** A GitOps tool.
    1.  You open a Pull Request.
    2.  Atlantis runs `terraform plan` and comments the output on the PR.
    3.  Your boss approves the PR.
    4.  You comment `atlantis apply` on the PR.
    5.  Atlantis applies changes and merges the PR.

---

Topic 6.2:
Title: Configuration Management
Order: 2

Class 6.2.1:
	Title: Ansible Fundamentals
	Description: Architecture, Inventory, and Ad-hoc commands.
Content Type: text
Duration: 450 
Order: 1
		Text Content :

# Ansible: Automating the Operating System

Ansible is a configuration management and automation tool designed to manage **operating systems and applications after infrastructure exists**. It excels at repeatable, idempotent system configuration without introducing heavy operational overhead.

---

## 1. Terraform vs. Ansible

Terraform and Ansible solve different problems and are often used together.

- **Terraform (Provisioning)**
  - Creates infrastructure resources
  - Works at the cloud/API level
  - Manages lifecycle of servers, networks, load balancers, and disks
- **Ansible (Configuration)**
  - Configures what runs *inside* the servers
  - Installs packages
  - Manages services
  - Edits configuration files

**Mental Model:**
- Terraform builds the house
- Ansible sets up the furniture, plumbing, and appliances

---

## 2. Architecture (Agentless)

Ansible’s biggest architectural advantage is that it is **agentless**.

- No software needs to be installed on target machines
- Uses existing system tools:
  - **SSH** for Linux/Unix
  - **WinRM** for Windows
- Execution model:
  - Ansible runs from a **Control Node**
  - Connects to managed nodes over the network
  - Executes tasks remotely

**Why this matters:**
- Lower operational complexity
- No agent version drift
- Easy to bootstrap new servers

---

## 3. Inventory

The inventory defines **which hosts Ansible manages**.

---

### Static Inventory

- Defined in simple text files (`INI` or `YAML`)
- Suitable for:
  - Small environments
  - Fixed infrastructure

Example:
```ini
[web]
10.0.1.10
10.0.1.11
````

---

### Dynamic Inventory

* Inventory is generated dynamically at runtime
* Commonly used with cloud providers
* Example:

  * Query AWS EC2 for instances with `Role=Web`
  * Automatically adapts to scaling events

**Why dynamic inventory is critical:**

* Auto Scaling Groups add/remove servers
* Manual inventory becomes obsolete instantly

---

## Key Takeaways

* Ansible is for **configuration**, not infrastructure creation
* Agentless architecture simplifies operations
* Dynamic inventory enables cloud-native automation
* Best used alongside Terraform in modern DevOps stacks

**Mental Model:**
Terraform defines *what exists*.
Ansible defines *how it is configured*.


---

Class 6.2.2:
	Title: Ansible Advanced Topics
	Description: Playbooks, Roles, and Vault.
Content Type: text
Duration: 450 
Order: 2
		Text Content :
# Ansible Power User

Once basic automation is in place, Ansible becomes a powerful framework for building reliable, reusable, and secure operating system workflows. These concepts separate casual usage from production-grade automation.

---

## 1. Playbooks (YAML)

Playbooks define **what state the system should be in**, not how to get there step by step.

- Written in YAML for readability
- Executed top-down across one or more hosts
- Designed to be **idempotent**

### Idempotency (Core Principle)

Idempotency ensures:
- Running a playbook once or 100 times produces the same result
- Automation is safe and repeatable

Examples:
- Non-idempotent
  - Appends data repeatedly
  - Breaks system state over time
- Idempotent
  - Ensures configuration exists exactly once
  - Makes no change if the system is already compliant

**Why it matters:**  
Idempotent automation enables continuous configuration enforcement without fear.

---

## 2. Roles & Galaxy

As playbooks grow, structure becomes mandatory.

### Roles

Roles enforce a **standard directory layout** for automation components.

Typical role structure:
- `tasks/` – main automation logic
- `handlers/` – restart/reload actions
- `templates/` – Jinja2 templates
- `files/` – static files
- `vars/` / `defaults/` – configuration values

**Benefit:**  
Encapsulation, reuse, and team collaboration.

---

### Ansible Galaxy

Galaxy is the community ecosystem for Ansible roles.

- Thousands of reusable roles
- Covers common infrastructure patterns:
  - Web servers
  - Databases
  - Monitoring agents
- Example: `geerlingguy.nginx` is an industry-standard role

**Best Practice:**  
Reuse vetted roles instead of reinventing foundational automation.

---

## 3. Ansible Vault

Secrets do not belong in plaintext YAML files.

### Vault Capabilities

- Encrypts:
  - Variables
  - Files
  - Entire playbooks
- Requires a password or key to decrypt at runtime

**Use Cases:**
- Database credentials
- API tokens
- Private keys

**Security Principle:**  
Configuration should be version-controlled. Secrets should be encrypted.

---

## Key Takeaways

- Idempotency is non-negotiable
- Roles enable scalable automation
- Galaxy accelerates delivery
- Vault protects sensitive data

**Mental Model:**  
Ansible is safe automation only when it is structured, repeatable, and secure.


---

Class 6.2.3:
	Title: Configuration Management at Scale
	Description: Comparison with Chef and Puppet.
Content Type: text
Duration: 400 
Order: 3
		Text Content :
# The Configuration Wars: Ansible vs. The Rest

Configuration management tools solve the problem of keeping thousands of systems in a **known, desired state**. The core difference between these tools lies in *how* configuration changes are delivered and enforced.

---

## 1. Push vs. Pull Models

The architectural model defines scalability, operational complexity, and control.

### Push Model (Ansible)

In a push-based system, changes are initiated from a central control node.

- The Control Node:
  - Connects to target machines
  - Executes configuration tasks immediately
- No agent runs continuously on the managed servers

**Advantages:**
- Full control over when changes happen
- Immediate feedback
- Simple to reason about and debug

**Limitations:**
- Control node can become a bottleneck at very large scale
- Requires network reachability to all targets at execution time

---

### Pull Model (Chef / Puppet)

In a pull-based system, each server is responsible for keeping itself compliant.

- Each node runs an agent
- The agent periodically:
  - Polls a central server
  - Pulls configuration
  - Applies changes automatically

**Advantages:**
- Extremely scalable
- Automatic drift correction
- Well-suited for massive fleets

**Limitations:**
- Agent lifecycle management
- Complex control-plane infrastructure
- Slower feedback loop

---

## 2. Why Learn Ansible?

Ansible occupies a unique position in the DevOps ecosystem.

- Simple learning curve
- Minimal infrastructure requirements
- Works across:
  - Linux servers
  - Network devices
  - Cloud platforms
  - Containers
- Frequently used as:
  - Deployment orchestrator
  - Migration tool
  - One-time automation runner

**Reality Check:**  
Terraform is superior for cloud provisioning, but Ansible excels at orchestration and configuration tasks that fall outside pure infrastructure management.

---

## Key Takeaways

- Push vs Pull defines operational philosophy
- Ansible favors control and simplicity
- Chef/Puppet favor scale and continuous enforcement
- Ansible remains the most versatile “glue” tool in DevOps

**Mental Model:**  
Ansible tells servers *what to do now*.  
Pull-based systems tell servers *what they should always be*.

---
Topic 6.3:
Title: Infrastructure as Code - Challenge
Order: 3

Class 6.3.1:
	Title: Infrastructure as Code - Challenge
	Description: Questions and Their respective answers
Content Type: text
Duration: 600 
Order: 1
		Text Content :

# Infrastructure Automation – Terraform & Ansible Challenge
**Contest Format | 5 Questions**

These questions focus on **real-world IaC failures, collaboration issues, and recovery patterns** seen in production.

---

## Question 1: Terraform State Management and Drift Resolution

### Problem  
Infrastructure managed by Terraform behaves differently from what is defined in code. A recent `terraform plan` shows unexpected changes, even though no one modified the Terraform files.

**Tasks:**
1. Define Terraform state and drift.
2. Identify common causes of drift.
3. Explain how to detect and resolve it safely.

---

### Answer

**Terraform State**
- State maps Terraform resources to real-world infrastructure.
- Stored locally or remotely as `terraform.tfstate`.

**Drift**
- Occurs when infrastructure is modified **outside Terraform** (console, CLI, scripts).

**Detection**
```bash
terraform plan
terraform refresh
```


**Common Causes**

* Manual AWS console changes
* Auto-scaling or policy-based updates
* Partial apply failures

**Resolution**

* If manual change is desired: update Terraform code to match reality.
* If not: `terraform apply` to reconcile.
* Never edit state files manually unless in recovery mode.

**Best Practice**
Terraform must be the single source of truth.

---

## Question 2: Remote State Configuration with S3 and DynamoDB

### Problem

Multiple engineers are working on the same Terraform project and frequently encounter state corruption and overwrite issues.

**Tasks:**

1. Explain why remote state is required.
2. Design a safe remote backend using AWS.
3. Explain state locking.

---

### Answer

**Why Remote State**

* Enables collaboration
* Prevents state overwrite
* Enables locking and recovery

**Recommended Backend**

* S3 for state storage
* DynamoDB for state locking

```hcl
backend "s3" {
  bucket         = "terraform-state-prod"
  key            = "vpc/terraform.tfstate"
  region         = "us-east-1"
  dynamodb_table = "terraform-locks"
  encrypt        = true
}
```

**State Locking**

* DynamoDB prevents concurrent `apply`
* Avoids race conditions and corruption

**Production Rule**
Never use local state for shared infrastructure.

---

## Question 3: Terraform Modules and Workspace Management

### Problem

Your Terraform codebase is duplicated across `dev`, `staging`, and `prod`, causing inconsistencies and maintenance overhead.

**Tasks:**

1. Explain Terraform modules.
2. Explain Terraform workspaces.
3. Show how they solve environment sprawl.

---

### Answer

**Terraform Modules**

* Reusable, parameterized infrastructure units
* Promote DRY and consistency

```hcl
module "vpc" {
  source = "../modules/vpc"
  cidr   = var.vpc_cidr
}
```

**Workspaces**

* Isolate state per environment

```bash
terraform workspace new prod
terraform workspace select prod
```

**Use Together**

* Modules define structure
* Workspaces separate state

**Warning**
Workspaces are not a replacement for proper environment isolation in large orgs.

---

## Question 4: Ansible Playbook Idempotency and Handlers

### Problem

An Ansible playbook restarts services on every run, even when nothing has changed.

**Tasks:**

1. Define idempotency.
2. Explain handlers.
3. Fix the playbook behavior.

---

### Answer

**Idempotency**

* Running a playbook multiple times produces the same system state.
* No unnecessary changes.

**Handlers**

* Triggered only when a task reports `changed`.

```yaml
tasks:
  - name: Update config
    template:
      src: app.conf.j2
      dest: /etc/app.conf
    notify: restart app

handlers:
  - name: restart app
    service:
      name: app
      state: restarted
```

**Fix**

* Use proper modules (`template`, `package`, `service`)
* Avoid `shell` unless required

**Interview Insight**
Idempotency is the core Ansible design principle.

---

## Question 5: Dynamic Inventory Configuration for AWS

### Problem

Your infrastructure scales dynamically, but Ansible inventory files are static and outdated.

**Tasks:**

1. Explain dynamic inventory.
2. Configure it for AWS.
3. Explain tagging-based targeting.

---

### Answer

**Dynamic Inventory**

* Inventory generated at runtime from cloud APIs.
* Reflects real-time infrastructure.

**AWS Inventory Plugin**

```yaml
plugin: aws_ec2
regions:
  - us-east-1
keyed_groups:
  - key: tags.Environment
```

**Usage**

```bash
ansible-inventory --graph
ansible-playbook -i aws_ec2.yaml deploy.yml
```

**Targeting**

* Use EC2 tags for grouping:

  * Environment=prod
  * Role=web

**Result**

* Zero manual inventory management
* Scales naturally with autoscaling groups

---

## Contest Evaluation Criteria

* Understanding of IaC failure modes
* State safety and collaboration awareness
* Correct use of modules and idempotent automation
* Ability to reason about production-scale automation

These scenarios reflect **real Terraform outages and Ansible misconfigurations**, not theory.


Topic 6.4:
Title: Advanced Terraform Techniques
Order: 4

Class 6.4.1:
	Title: State Management and Organization
	Description: Workspaces, drift detection, import, and monorepo patterns.
Content Type: text
Duration: 500 
Order: 1
		Text Content :
 # Advanced Terraform: State and Organization

## 1. Terraform Workspaces

Workspaces allow multiple **separate state files** within one directory.

### When to Use Workspaces

**Scenario:** Deploy same infrastructure across dev, staging, prod.

```bash
# Create workspaces
terraform workspace new dev
terraform workspace new staging
terraform workspace new prod

# Switch workspace
terraform workspace select prod

# Apply (uses prod.tfstate)
terraform apply

# View all workspaces
terraform workspace list
# * default
#   dev
#   staging
#   prod
```

### The tfvars Pattern (Better Than Workspaces)

```hcl
# prod.tfvars
environment = "production"
instance_count = 10
instance_type = "t3.large"
enable_detailed_monitoring = true

# dev.tfvars
environment = "development"
instance_count = 1
instance_type = "t3.micro"
enable_detailed_monitoring = false
```

```bash
# Deploy with different variables
terraform apply -var-file=prod.tfvars
terraform apply -var-file=dev.tfvars
```

**Why tfvars is better:**
- Variables are visible (not hidden in separate state)
- Easier to version control
- Clearer inheritance hierarchy

### Workspace Limitations

```bash
# Workspaces are NOT isolated!
# All workspaces share:
# - .tf files
# - Remote state bucket
# - Team permissions

# If someone has access to one workspace, they see all
```

**Recommendation:** Use workspaces for dev environments, **separate directories** or **separate repos** for prod.

---

## 2. State Drift Detection

State drift = actual infrastructure differs from Terraform state.

### Detecting Drift

```bash
# terraform plan -refresh-only shows drift without applying
terraform plan -refresh-only -json | jq '.resource_changes[] | select(.change.before != .change.after)'

# Example drift:
# Security group rule was manually added
# S3 bucket was manually deleted
# RDS password was rotated outside Terraform
```

### Common Causes

1. **Manual changes** via AWS console
2. **External tools** (CloudFormation, Ansible, scripts)
3. **Third-party services** (e.g., Datadog agent installing itself)
4. **Expired credentials** (state is stale)

### Remediation

```bash
# Option 1: Accept drift (update state)
terraform refresh

# Option 2: Revert to state (danger!)
terraform apply  # Overwrites actual infra

# Option 3: Investigate then decide
terraform state show aws_security_group.main
# Then manually fix in console OR in Terraform
```

### Drift Monitoring (Automated)

```bash
# Daily drift check (CI/CD)
#!/bin/bash
terraform init
terraform plan -refresh-only -json > drift.json

if jq '.resource_changes | length > 0' drift.json; then
  echo "DRIFT DETECTED"
  curl -X POST https://hooks.slack.com/services/... \
    -d @drift.json
  exit 1
fi
```

---

## 3. terraform import

Importing existing resources into Terraform state.

### Scenario: Existing RDS Cluster

Someone created RDS in console, now need to manage it with Terraform.

```bash
# 1. Write the resource block (empty)
cat > main.tf << 'EOF'
resource "aws_db_instance" "prod_database" {
  # To be filled by import
}
EOF

# 2. Import the actual resource
terraform import aws_db_instance.prod_database mydb-prod-01

# 3. terraform inspect the imported resource
terraform state show aws_db_instance.prod_database

# 4. Fill in the configuration based on state
resource "aws_db_instance" "prod_database" {
  identifier           = "mydb-prod-01"
  engine               = "mysql"
  engine_version       = "8.0.28"
  instance_class       = "db.t3.micro"
  allocated_storage    = 20
  storage_encrypted    = true
  skip_final_snapshot  = false
  db_subnet_group_name = aws_db_subnet_group.main.name
  
  # ... etc
}
```

### Challenges

- **Finding the resource ID** (varies by resource type)
- **Incomplete configuration** (import doesn't create config, only state)
- **Secrets not imported** (password, API keys must be added manually)

### Best Practice

```bash
# Don't import production resources into your main code
# Instead:

# 1. Create separate module for imported resources
terraform {
  required_version = ">= 1.0"
}

module "legacy_infra" {
  source = "./modules/legacy"
  # Configuration
}

# 2. Once stable, migrate to main code
# 3. Version and test thoroughly before merging
```

---

## 4. Remote State and Data Sources

### Remote State Data Source (Cross-Stack References)

```hcl
# Stack 1: VPC (state stored in terraform-vpc-prod.tfstate)
resource "aws_vpc" "main" {
  cidr_block = "10.0.0.0/16"
}

output "vpc_id" {
  value = aws_vpc.main.id
}

# Stack 2: App (needs VPC ID from Stack 1)
data "terraform_remote_state" "vpc" {
  backend = "s3"
  config = {
    bucket = "terraform-state-prod"
    key    = "vpc/terraform.tfstate"
    region = "us-east-1"
  }
}

resource "aws_instance" "app" {
  ami           = "ami-12345"
  instance_type = "t3.micro"
  subnet_id     = data.terraform_remote_state.vpc.outputs.vpc_id
}
```

### Output Dependencies

```hcl
# Stack 1 publishes outputs
output "database_endpoint" {
  value = aws_db_instance.main.endpoint
}

output "database_port" {
  value = aws_db_instance.main.port
}

# Stack 2 consumes outputs
locals {
  db_host = data.terraform_remote_state.db.outputs.database_endpoint
  db_port = data.terraform_remote_state.db.outputs.database_port
}

# In app code
resource "aws_ssm_parameter" "db_connection_string" {
  name  = "/app/database/connection"
  value = "postgresql://${local.db_host}:${local.db_port}/mydb"
}
```

---

## 5. Monorepo vs Multirepo

### Monorepo Pattern

```
terraform/
├── environments/
│   ├── dev/
│   │   ├── main.tf
│   │   ├── variables.tf
│   │   └── terraform.tfvars
│   ├── staging/
│   └── prod/
├── modules/
│   ├── vpc/
│   ├── rds/
│   └── eks/
└── shared/
    ├── variables.tf
    └── providers.tf
```

**Pros:**
- Single repository to clone
- Easy code reuse (shared modules)
- Consistent versioning

**Cons:**
- Large monorepo (slow operations)
- Accidental changes to prod from dev PR
- Need strict code review process

---

### Multirepo Pattern

```
terraform-vpc/          (separate repo)
terraform-rds/          (separate repo)
terraform-app/          (separate repo, uses vpc/rds outputs)
```

**Pros:**
- Smaller, faster repos
- Independent versioning
- Clear separation of concerns
- Easier to grant access (different teams)

**Cons:**
- Multiple repos to maintain
- Dependency management complex
- Less code reuse

**Recommendation:**
- **Small teams (<10 people):** Monorepo with modules
- **Large teams (>10 people):** Multirepo with clear dependencies

---

Class 6.4.2:
	Title: Policy as Code and Testing
	Description: Sentinel, OPA, and Terraform testing frameworks.
Content Type: text
Duration: 450 
Order: 2
		Text Content :
 # Infrastructure Testing and Policy Enforcement

## 1. Sentinel: Policy Language for Terraform Enterprise

Sentinel enforces policies on Terraform runs.

### Example Policies

```sentinel
# Enforce tagging
import "tfplan/v2" as tfplan

mandatory_tags = ["Environment", "Owner", "CostCenter"]

resources = tfplan.resource_changes

violations = []

for resources as address, rc {
  for rc.change.after.tags as tag, _ {
    if mandatory_tags not contains tag {
      append(violations, address)
    }
  }
}

main = length(violations) == 0
```

```sentinel
# Prevent public S3 buckets
import "tfplan/v2" as tfplan

buckets = tfplan.resource_changes["aws_s3_bucket"]

main = all buckets as address, bucket {
  bucket.change.after.acl != "public-read" and
  bucket.change.after.acl != "public-read-write"
}
```

### Policy Sets

```hcl
# Add to Terraform Enterprise
resource "tfe_policy_set" "aws_security" {
  name         = "aws-security-policies"
  organization = "my-org"
  
  policy_ids = [
    tfe_sentinel_policy.no_public_s3.id,
    tfe_sentinel_policy.require_tags.id,
    tfe_sentinel_policy.instance_size_limit.id
  ]
}
```

### Enforcement Levels

| Level | Behavior |
| :--- | :--- |
| **advisory** | Warn, but allow override |
| **soft-mandatory** | Warn, require manager approval |
| **hard-mandatory** | Fail, no override possible |

---

## 2. OPA with Terraform

Using Open Policy Agent for IaC validation.

### conftest: OPA for IaC

```bash
# Install conftest
brew install conftest

# Write policy
cat > policy.rego << 'EOF'
package main

deny[msg] {
    input.resource.aws_instance[_].instance_type == "t2.micro"
    msg = "Instances must be t3 or larger"
}

deny[msg] {
    input.resource.aws_security_group[sg].ingress[_].cidr_blocks[_] == "0.0.0.0/0"
    msg = sprintf("Security group %s allows open access", [sg])
}
EOF

# Test against Terraform plan
terraform plan -json | conftest test -
```

### Advantages over Sentinel

| Aspect | Sentinel | OPA |
| :--- | :--- | :--- |
| **Language** | Sentinel (proprietary) | Rego (general-purpose) |
| **Requires** | Terraform Enterprise | CLI tool |
| **Reusable** | TFE-specific | Any tool (Kubernetes, Docker, etc.) |
| **Community** | Limited | Large (CNCF) |

---

## 3. Terraform Testing Frameworks

### terraform-compliance: BDD Testing

```gherkin
# compliance.feature
Feature: Ensure all AWS resources are properly tagged

  Scenario: EC2 instances must have required tags
    Given I have aws_instance defined
    Then it must contain tags
    And its tags.Environment must exist
    And its tags.Owner must exist
```

```bash
terraform plan -json | terraform-compliance -f compliance.feature
```

### Terratest: Go-Based Integration Testing

```go
// test/aws_test.go
package test

import (
  "testing"
  "github.com/gruntwork-io/terratest/modules/terraform"
)

func TestTerraform(t *testing.T) {
  opts := &terraform.Options{
    TerraformDir: "../",
    Vars: map[string]interface{}{
      "environment": "test",
    },
  }

  terraform.InitAndApply(t, opts)
  defer terraform.Destroy(t, opts)

  // Assert outputs
  vpcId := terraform.Output(t, opts, "vpc_id")
  if vpcId == "" {
    t.Fatal("VPC ID not found")
  }

  // Assert actual infrastructure
  subnets := terraform.GetRandomSubnets(t, &ec2.Client{}, opts)
  if len(subnets) == 0 {
    t.Fatal("No subnets created")
  }
}
```

```bash
go test -v test/aws_test.go
```

---

## 4. Advanced Terraform Patterns

### Dynamic Blocks

```hcl
# Generate ingress rules dynamically
resource "aws_security_group" "main" {
  name = "dynamic-sg"

  dynamic "ingress" {
    for_each = var.allowed_ports
    content {
      from_port   = ingress.value.from
      to_port     = ingress.value.to
      protocol    = ingress.value.protocol
      cidr_blocks = ingress.value.cidrs
    }
  }
}

# Usage
allowed_ports = [
  { from = 80,   to = 80,   protocol = "tcp", cidrs = ["0.0.0.0/0"] },
  { from = 443,  to = 443,  protocol = "tcp", cidrs = ["0.0.0.0/0"] },
  { from = 22,   to = 22,   protocol = "tcp", cidrs = ["10.0.0.0/8"] }
]
```

### for_each with Resource Attributes

```hcl
# Create multiple resources with specific names
resource "aws_instance" "workers" {
  for_each = {
    web1 = { instance_type = "t3.micro",  az = "us-east-1a" },
    web2 = { instance_type = "t3.small",  az = "us-east-1b" },
    web3 = { instance_type = "t3.micro",  az = "us-east-1c" }
  }

  instance_type           = each.value.instance_type
  availability_zone       = each.value.az
  ami                     = data.aws_ami.ubuntu.id
  iam_instance_profile    = aws_iam_instance_profile.main.name

  tags = {
    Name = "worker-${each.key}"
  }
}

# Reference specific instance
aws_instance.workers["web1"].id
```

### Conditional Logic

```hcl
resource "aws_db_instance" "main" {
  # ... common config ...

  # Only create replica in production
  replicate_source_db = var.environment == "prod" ? aws_db_instance.primary.id : null

  # Backup retention varies by environment
  backup_retention_period = var.environment == "prod" ? 30 : 7

  # Enable enhanced monitoring only in prod
  enabled_cloudwatch_logs_exports = var.environment == "prod" ? ["postgresql"] : []
}
```

---

## 5. Module Design Patterns

### Composition (Recommended)

```hcl
# Low-level module: VPC with subnets
module "vpc" {
  source = "./modules/vpc"
  cidr   = "10.0.0.0/16"
}

# Mid-level module: Database in VPC
module "database" {
  source          = "./modules/rds"
  vpc_id          = module.vpc.id
  subnet_ids      = module.vpc.private_subnets
}

# High-level: Complete application stack
module "app" {
  source = "./modules/app"
  
  vpc_module  = module.vpc
  db_endpoint = module.database.endpoint
}
```

**Pros:** Small, focused, reusable
**Cons:** Many modules to manage

### Monolithic Module (Not Recommended)

```hcl
# One giant module for "complete-app"
module "app" {
  source = "./modules/app"
  
  # Requires 50+ variables
  environment = var.environment
  vpc_config = var.vpc_config
  db_config = var.db_config
  # ... many more
}
```

**Cons:** Hard to reuse, difficult to test

### Module Versioning

```hcl
# Reference module by version tag
module "vpc" {
  source = "git::https://github.com/myorg/terraform-vpc.git?ref=v2.1.0"
  cidr   = "10.0.0.0/16"
}

# Or use private registry
module "rds" {
  source = "app.terraform.io/myorg/rds/aws"
  version = "~> 3.0"  # >= 3.0, < 4.0
}
```

---

Module 7:
Title: Monitoring & Observability
Description: Build comprehensive monitoring and alerting systems. Master metrics, logs, and traces with Prometheus, Grafana, ELK Stack, and distributed tracing tools.
Order: 7
Learning Outcomes:
Implement comprehensive monitoring solutions
Master Prometheus and Grafana
Build centralized logging with ELK
Understand distributed tracing

Topic 7.1:
Title: Monitoring Fundamentals
Order: 1

Class 7.1.1:
	Title: The Three Pillars of Observability
	Description: Metrics, Logs, and Traces explained.
Content Type: text
Duration: 400 
Order: 1
		Text Content :
 # The Three Pillars of Observability

In an interview, "Monitoring" is pass/fail. "Observability" is understanding **why**.

---

## 1. Metrics (Is it healthy?)
* **Definition:** Numerical data measured over time (Time-Series).
* **Characteristics:** Cheap to store, fast to query. Great for "What" and "When."
* *Example:* "CPU Usage is 99%."

---

## 2. Logs (Why is it sick?)
* **Definition:** A discrete record of an event.
* **Characteristics:** Expensive to store, heavy to search. Great for "Why."
* *Example:* "Exception: NullPointerException at UserLogin.java:42."

---

## 3. Traces (Where is the problem?)
* **Definition:** The lifecycle of a request as it hops across microservices.
* **Characteristics:** Essential for microservices.
* *Example:* "The User Service called the Payment Service, which timed out after 500ms."

---

## 4. SLI vs. SLO vs. SLA
* **SLI (Indicator):** The actual measurement (e.g., "Current Latency is 120ms").
* **SLO (Objective):** The internal goal (e.g., "We want 99% of requests to be under 200ms").
* **SLA (Agreement):** The legal contract (e.g., "If we are down for >1 hour, we refund you 10%").

Class 7.1.2:
	Title: Golden Signals & Key Metrics
	Description: What you should actually measure.
Content Type: text
Duration: 300 
Order: 2
		Text Content :
 # Golden Signals & Key Metrics

You cannot measure everything. Google SRE defined the "Golden Signals" as the absolute minimum.

---

## 1. The Four Golden Signals
1.  **Latency:** How long does it take? (Distinguish between successful requests and failed ones).
2.  **Traffic:** How much demand is there? (Requests per Second).
3.  **Errors:** The rate of requests failing (HTTP 500s).
4.  **Saturation:** How "full" is the service? (Memory usage, Queue depth).

---

## 2. Methodologies
* **USE Method (For Infrastructure):** Focus on **U**tilization, **S**aturation, and **E**rrors.
    * *Target:* CPU, Disk, RAM.
* **RED Method (For Services):** Focus on **R**ate, **E**rrors, and **D**uration (Latency).
    * *Target:* APIs, Microservices.

Topic 7.2:
Title: Metrics & Time-Series Databases
Order: 2

Class 7.2.1:
	Title: Prometheus
	Description: Architecture and PromQL.
Content Type: text
Duration: 500 
Order: 1
		Text Content :
 # Prometheus: The King of Metrics

## 1. Architecture: The Pull Model
Unlike traditional monitoring (where agents push data to the server), **Prometheus Pulls (Scrapes)** data.
* **Target:** Your application exposes a `/metrics` HTTP endpoint.
* **Scraper:** Prometheus visits that endpoint every 15 seconds and stores the data.
* *Advantage:* If your app is under load, it doesn't get slowed down by trying to push metrics. Prometheus just fails to scrape, which is safer.

---

## 2. Metric Types
* **Counter:** Only goes up (e.g., `http_requests_total`).
* **Gauge:** Goes up and down (e.g., `memory_usage_bytes`).
* **Histogram:** Buckets data (e.g., "How many requests took <0.1s, <0.5s, <1s?").

---

## 3. PromQL (Prometheus Query Language)
* **Rate:** Calculates per-second speed.
    * `rate(http_requests_total[5m])` -> "Average requests per second over the last 5 minutes."
* *Interview Q:* "Why use `rate` instead of `increase`?"
    * *A:* `rate` handles counter resets (server restarts) gracefully.

Class 7.2.2:
	Title: Grafana
	Description: Visualization and Dashboards.
Content Type: text
Duration: 450 
Order: 2
		Text Content :
 # Grafana: Making Data Beautiful

## 1. Data Source Agnostic
Grafana does not store data. It visualizes data from **Prometheus**, **CloudWatch**, **Elasticsearch**, and **InfluxDB** all in one dashboard.

---

## 2. Dashboard Best Practices
* **Top Row:** High-level "Traffic Lights" (Uptime, Error Rate).
* **Middle:** RED Metrics (Rate, Error, Duration) graphs.
* **Bottom:** Deep dive infrastructure metrics (CPU per pod).
* **Variables:** Use Templating (Dropdowns) to switch between `Production` and `Staging` without duplicating dashboards.

---

## 3. Alerting
Grafana can send alerts to Slack/PagerDuty.
* *Pro Tip:* Don't alert on "CPU > 80%." Alert on "Error Rate > 1%." (User pain is more important than server pain).

Topic 7.3:
Title: Log Management
Order: 3

Class 7.3.1:
	Title: ELK Stack (Elasticsearch, Logstash, Kibana)
	Description: The centralized logging standard.
Content Type: text
Duration: 500 
Order: 1
		Text Content :


# ELK Stack (Elasticsearch, Logstash, Kibana)

## Overview

The **ELK Stack** is the industry-standard solution for **centralized logging, search, and observability**.
In real systems, logs are generated across hundreds or thousands of nodes (VMs, containers, Kubernetes pods). ELK provides a **scalable pipeline** to collect, process, store, and analyze this data in near real time.

At scale, ELK is not just a logging tool—it becomes a **debugging, auditing, and incident-response platform**.

---

## 1. Elasticsearch – Distributed Search & Storage Engine

Elasticsearch is a **distributed, document-oriented search engine** built on Apache Lucene.

### Core Characteristics

* Stores data as **JSON documents**
* Schema is defined using **Mappings**
* Data is indexed for **full-text search and aggregations**
* Horizontally scalable via **shards**
* Fault tolerant via **replicas**

### Key Concepts

* **Index**
  Logical namespace for documents (e.g., `nginx-logs-2026.01.02`)

* **Document**
  A single log/event stored as JSON

* **Shard**
  A physical partition of an index

  * Primary shard: holds original data
  * Replica shard: copy for HA and read scaling

* **Cluster State**
  Metadata about indices, shards, nodes
  Excessive shard counts directly impact cluster stability

### Production Considerations

* **Shard sizing matters** (10–50 GB per shard is a common guideline)
* Too many small indices cause:

  * High heap usage
  * Slow cluster state updates
* Retention is managed via **Index Lifecycle Management (ILM)**

---

## 2. Logstash – Ingestion and Transformation Layer

Logstash is a **data processing pipeline** that ingests, parses, enriches, and forwards data.

### Why Logstash Exists

Raw logs are inconsistent:

* Different formats
* Mixed timestamps
* Unstructured text

Logstash normalizes this data into **structured events** before storage.

### Pipeline Structure

```text
Input → Filter → Output
```

### Common Filters

* `grok` – Parse unstructured logs (regex-based)
* `date` – Normalize timestamps
* `mutate` – Rename/remove fields
* `geoip` – Enrich IP addresses with location
* `json` – Parse embedded JSON logs

### Example Use Case

* Parse Nginx access logs
* Extract:

  * HTTP method
  * Status code
  * Response time
* Convert timestamp to UTC
* Send structured output to Elasticsearch

### Operational Notes

* Logstash is **CPU and memory intensive**
* Not ideal on every node
* Often deployed as:

  * Centralized service
  * Kubernetes Deployment
  * Autoscaled group

---

## 3. Kibana – Visualization and Analysis Layer

Kibana is the **UI and analytics interface** for Elasticsearch.

### Core Capabilities

* **Discover** – Explore raw logs in real time
* **Dashboards** – Visualize metrics (latency, errors, traffic)
* **Lens / Visualize** – Build charts without writing queries
* **Alerting** – Trigger alerts based on query results
* **Saved Searches** – Reusable filtered views

### Real-World Usage

* Identify error spikes after deployments
* Correlate latency with specific services
* Drill down from metrics → logs during incidents

### Important Note

Kibana **does not store data**.
It queries Elasticsearch directly. Performance depends entirely on index design and query efficiency.

---

## 4. Beats – Lightweight Data Shippers

Beats are **small, purpose-built agents** that collect data and forward it downstream.

### Common Beats

* **Filebeat** – Log files
* **Metricbeat** – System and service metrics
* **Heartbeat** – Service availability checks
* **Auditbeat** – Security and audit data

---

## Why Beats Matter

* Extremely **low resource usage**
* Fast startup (important for autoscaling and containers)
* Designed for **large fleets**
* Ideal for Kubernetes and ephemeral workloads

Beats typically run:

* As a **DaemonSet** in Kubernetes
* As an **agent** on VMs

They forward data to:

* Logstash (for heavy processing), or
* Directly to Elasticsearch (for simple pipelines)

---

## 5. Data Flow Architecture

```text
Application Logs
        ↓
     Beats
        ↓
   Logstash (optional)
        ↓
  Elasticsearch
        ↓
     Kibana
```

Not all pipelines use Logstash.
At scale, **Beats → Elasticsearch** is common when logs are already structured (JSON).

---

## 6. Scaling and Reliability Concerns

### Elasticsearch

* Use **dedicated master nodes**
* Separate **hot / warm / cold** data tiers
* Monitor:

  * Heap usage
  * GC pauses
  * Shard counts
* Enable **ILM** for retention and rollover

### Logstash

* Horizontal scaling via multiple pipelines
* Use persistent queues for burst handling
* Monitor pipeline latency and backpressure

### Beats

* Backpressure aware
* Can buffer data when downstream is unavailable

---

## Key Takeaways

* **Elasticsearch** provides fast, distributed storage and search
* **Logstash** enables complex parsing and enrichment
* **Kibana** is the exploration and visualization layer
* **Beats** are optimized shippers for scale and containers

### Mental Model


Beats collect
→ Logstash transforms
→ Elasticsearch stores
→ Kibana visualizes

This pipeline forms the backbone of **production observability and incident response** in modern systems.




---

Class 7.3.2:
	Title: Centralized Logging Strategies
	Description: Sidecars vs. DaemonSets.
Content Type: text
Duration: 400 
Order: 2
		Text Content :
 # Logging Strategies in Kubernetes

## 1. Structured Logging (JSON)
* **The Golden Rule:** Never log plain text (`System.out.println("User failed")`).
* **Do this:** Log JSON (`{"level": "error", "user_id": "123", "msg": "failed"}`).
* *Why?* Machines can parse JSON instantly. Regex parsing plain text is slow and brittle.

---

## 2. Collection Patterns
* **Node Agent (DaemonSet):** Run one Filebeat pod per Node. It reads `/var/log/containers/*.log`.
    * *Pros:* Resource efficient. The standard way.
* **Sidecar Pattern:** Add a logging container *inside* every application pod.
    * *Pros:* Can handle custom log locations.
    * *Cons:* Heavy resource usage. Avoid if possible.

Topic 7.4:
Title: Advanced Monitoring Tools
Order: 4

Class 7.4.1:
	Title: Application Performance Monitoring
	Description: Datadog, New Relic, and Code-Level insights.
Content Type: text
Duration: 500 
Order: 1
		Text Content :
# APM: Code-Level Visibility

## Overview

Traditional monitoring answers **“Is the system up?”**9.1.1
APM (Application Performance Monitoring) answers **“Why is it slow?”**

Metrics and logs operate at the **infrastructure and service level**.
APM operates at the **code execution level**, tracing individual requests as they move through functions, databases, queues, and downstream services.

APM becomes critical once:

* Systems are microservice-based
* Latency issues are non-obvious
* Failures are intermittent and data-dependent

---

## 1. What is APM?

APM instruments application code to measure:

* Request latency
* Call paths
* Dependency behavior
* Error propagation

Instead of guessing where time is spent, APM shows **exact execution paths**.

### Example Insight

* API request latency: **3.2s**
* Breakdown:

  * App logic: 40ms
  * External API call: 120ms
  * Database query: **3.0s**
* Root cause:

  * Missing index on `order_id`

Without APM, this appears as a “slow server.”
With APM, it becomes a **specific, actionable fix**.

---

## 2. Core APM Concepts

### Transactions

A **transaction** represents a single request or job:

* HTTP request
* Background job
* Message queue consumer

Each transaction has:

* Total latency
* Success/failure state
* Breakdown by component

---

### Spans

A **span** is a timed operation within a transaction.

Examples:

* HTTP handler
* SQL query
* Redis call
* External API request

Transactions are composed of multiple spans, forming a **trace**.

---

### Distributed Tracing

In microservices, a single user request may touch:

* API Gateway
* Auth service
* Checkout service
* Inventory service
* Database

APM propagates **trace IDs** across services to reconstruct the **end-to-end request path**.

This enables:

* Cross-service latency analysis
* Dependency bottleneck identification
* Root cause isolation across teams

---

## 3. Auto-Instrumentation

Modern APM tools rely heavily on **auto-instrumentation**.

### How It Works

* An agent or library is added to the runtime
* Common frameworks and libraries are automatically wrapped
* No manual code changes required for basic visibility

### Examples

* Python: `ddtrace`, `newrelic`
* Java: Java Agent (`-javaagent`)
* Node.js: `dd-trace`, `elastic-apm-node`
* Go: Middleware-based instrumentation

Captured automatically:

* HTTP request timing
* SQL query latency
* Cache access (Redis/Memcached)
* External service calls

Manual instrumentation is still used for:

* Business-critical functions
* Custom logic timing
* Domain-specific metrics

---

## 4. APM Tools (Datadog / New Relic / Elastic APM)

### Common Capabilities

* Distributed tracing
* Code-level profiling
* Error tracking with stack traces
* Dependency analysis
* Correlation with logs and metrics

### Runtime Visibility

APM agents often capture:

* Function execution time
* CPU usage per request
* Memory allocations
* Thread or event-loop blocking

This helps diagnose issues such as:

* Slow garbage collection
* Blocking calls in async systems
* Thread pool exhaustion

---

## 5. Service Maps

APM tools automatically generate **service maps** by observing traffic flow.

### What Service Maps Show

* Services as nodes
* Requests as edges
* Latency and error rates per connection

### Practical Use Cases

* Identifying unexpected dependencies
* Detecting circular service calls
* Understanding blast radius of failures
* Spotting over-coupled services

Service maps are especially valuable in:

* Large organizations
* Poorly documented systems
* Rapidly evolving microservice architectures

---

## 6. APM vs Metrics vs Logs

Each observability pillar answers a different question:

* **Metrics:** Is something wrong?
* **Logs:** What happened?
* **APM:** Why did it happen?

APM complements—not replaces—metrics and logs.

### Example Incident Flow

1. Alert fires: latency spike (metrics)
2. Logs show no obvious errors
3. APM trace reveals:

   * One slow database query
   * Triggered only for specific input
4. Root cause fixed with an index or query rewrite

---

## 7. Performance and Cost Considerations

APM is powerful but not free.

### Overhead

* Additional CPU and memory usage
* Network overhead for trace data
* Storage costs for retained traces

### Common Mitigations

* Sampling (e.g., 10% of requests)
* Higher sampling for errors
* Shorter retention for high-volume services

APM configuration is a **trade-off between visibility and cost**.

---

## Key Takeaways

* APM provides **code-level observability**
* It traces requests across services and dependencies
* Auto-instrumentation enables fast adoption
* Service maps expose real system architecture
* APM turns performance issues into actionable fixes

---

## Mental Model

Metrics show symptoms
Logs show events
APM shows execution paths and root causes

APM is the bridge between **application code and operational reality**.


Class 7.4.2:
	Title: Distributed Tracing
	Description: Tracking requests across microservices.
Content Type: text
Duration: 350 
Order: 2
		Text Content :
 # Distributed Tracing

## 1. The Microservice Problem
In a monolith, a stack trace tells you everything. In microservices, a request hits 10 different servers. Standard logs are useless here.

---

## 2. How Tracing Works
* **Trace ID:** A unique ID generated at the Ingress. It is passed in the HTTP Headers (`X-Trace-ID`) to every downstream service.
* **Span:** A specific unit of work (e.g., "Database Query" or "External API Call").
* **Visualization:** A "Waterfall" or "Flame Graph" showing exactly where time was spent.

---

## 3. OpenTelemetry (The Standard)
* Formerly OpenTracing/OpenCensus.
* Vendor-neutral. You instrument your code with OpenTelemetry, and you can send the data to Jaeger, Datadog, or Honeycomb.

Topic 7.5:
Title: Alerting Strategies
Order: 5

Class 7.5.1:
	Title: Effective Alerting
	Description: Avoiding Alert Fatigue.
Content Type: text
Duration: 350 
Order: 1
		Text Content :
 # Effective Alerting: Don't Wake Me Up

## 1. Alert Fatigue
If your phone buzzes 50 times a day for non-critical issues, you will ignore it when the real outage happens.
* **Rule:** If an alert doesn't require immediate human action, it shouldn't be an alert. It should be a ticket or a log entry.

---

## 2. Alertmanager (Prometheus Ecosystem)
* **Grouping:** Don't send 100 emails if 100 servers go down. Send 1 email saying "Cluster X is down (100 servers affected)."
* **Inhibition:** If the "Data Center Power" alert fires, suppress the "Server Down" alerts (because we know why they are down).
* **Silencing:** "I am doing maintenance, don't page me for the next hour."

---

## 3. On-Call Best Practices
* **Runbooks:** Every alert MUST link to a document explaining:
    1.  What is the impact?
    2.  How to verify it?
    3.  How to fix it?
* If there is no Runbook, delete the alert.

---

Topic 7.6:
Title: Advanced Observability Topics
Order: 6

Class 7.6.1:
	Title: Advanced Monitoring and Performance Optimization
	Description: Custom metrics, advanced query optimization, and observability at scale.
Content Type: text
Duration: 500 
Order: 1
		Text Content :
 # Advanced Observability: From Metrics to Action

## 1. Custom Metrics and Application Instrumentation

Beyond the Golden Signals, applications emit custom business metrics.

### Emitting Custom Metrics

```python
# Python with Prometheus client
from prometheus_client import Counter, Histogram, Gauge, generate_latest

# Counter: only increases
orders_placed = Counter('orders_placed_total', 'Total orders placed')
orders_placed.inc()

# Gauge: can increase or decrease
active_connections = Gauge('active_connections', 'Currently active connections')
active_connections.set(42)

# Histogram: buckets latency
checkout_duration = Histogram('checkout_duration_seconds', 'Checkout duration')
checkout_duration.observe(2.5)

# Custom labels
orders_by_region = Counter(
    'orders_placed_by_region_total',
    'Orders by region',
    ['region', 'currency']
)
orders_by_region.labels(region='us-east', currency='usd').inc()
```

### Exposing Metrics

```python
# Flask app with Prometheus metrics
from flask import Flask
from prometheus_client import generate_latest, CollectorRegistry, CONTENT_TYPE_LATEST

app = Flask(__name__)
registry = CollectorRegistry()

@app.route('/metrics')
def metrics():
    return generate_latest(registry), 200, {'Content-Type': CONTENT_TYPE_LATEST}

@app.route('/api/orders', methods=['POST'])
def create_order():
    orders_placed.inc()
    # ... business logic ...
    checkout_duration.observe(duration)
    return jsonify(order)
```

### What to Measure

**Business Metrics:**
- Orders placed, revenue, conversion rate
- User signups, active users
- Feature usage

**Application Metrics:**
- Request rate per endpoint
- Cache hit rate
- Queue depth
- Batch job duration

**Resource Metrics:**
- Database connections used
- Kafka lag per topic
- Memory usage per service

---

## 2. PromQL Advanced Queries

PromQL is powerful for sophisticated monitoring.

### Aggregation Over Time

```promql
# Last 5-minute error rate across all services
sum(rate(http_requests_total{status=~"5.."}[5m]))
/
sum(rate(http_requests_total[5m]))

# Per-service error rate
sum by (service) (rate(http_requests_total{status=~"5.."}[5m]))
/
sum by (service) (rate(http_requests_total[5m]))
```

### Complex Joins

```promql
# Correlate request latency with CPU usage
histogram_quantile(0.95, request_duration_seconds) 
/ ignoring (instance)
(cpu_usage_percent / 100)
# Shows: is high CPU causing slow requests?
```

### Prediction and Trending

```promql
# Is disk usage growing dangerously?
# If this trend continues, when will disk be full?
predict_linear(node_filesystem_avail_bytes[1h], 3600)

# If positive trend continues, alert when reaching 90% full
ALERTS when disk fills at current rate
```

### Alert Examples

```yaml
# Alert if error rate is increasing (trend detection)
- alert: IncreasingErrorRate
  expr: |
    (
      rate(errors_total[5m]) 
      > 
      rate(errors_total offset 10m)
    )
    and
    rate(errors_total[5m]) > 0.01
  for: 5m
  annotations:
    summary: "Error rate is increasing"

# Alert if service is degrading (composite)
- alert: ServiceDegraded
  expr: |
    (histogram_quantile(0.95, request_duration) > 1)
    and
    (rate(http_requests_total[5m]) > 100)
  annotations:
    summary: "Service is slow under load"
```

---

## 3. Tracing at Scale

Distributed tracing becomes essential with many services.

### Sampling Strategies

**Uniform Sampling (10% of requests):**
```yaml
sampler:
  type: const
  param: 0.1  # Sample 10%
```

**Issues:** Miss rare errors

**Error-Based Sampling:**
```go
// Always trace errors, sample successes
func shouldSample(span *Span) bool {
    if span.Status == ERROR {
        return true  // 100% of errors
    }
    return rand.Float64() < 0.01  // 1% of successes
}
```

**Adaptive Sampling:**
```yaml
sampler:
  type: probabilistic
  param: 0.01  # Base 1%
  
# Increase to 10% if error rate > 1%
adaptive_sampler:
  initial_sampling_rate: 0.01
  sampling_rate_limit: 0.1
```

### Trace Storage and Query

```bash
# Jaeger backend options:
# 1. Elasticsearch (good for scale, searchable)
# 2. Cassandra (write-optimized, expensive)
# 3. Badger (embedded database, dev only)

# Query: Find all traces of failing checkout
traces = jaeger.query(
    service="checkout-service",
    tag="error=true",
    time_range="1h"
)

# Drill down into slowest trace
trace = traces.sort_by_duration().last()
spans = trace.spans  # See service breakdown
```

---

## 4. Observability Platforms (Beyond Open Source)

### Datadog: All-in-One

```yaml
# Datadog agent (stateless, cloud-native)
apiVersion: v1
kind: Pod
metadata:
  name: app
  annotations:
    ad.datadoghq.com/app.check_names: '["prometheus"]'
    ad.datadoghq.com/app.init_configs: '[{}]'
    ad.datadoghq.com/app.instances: |
      [{
        "prometheus_url": "http://%%host%%:8080/metrics"
      }]
spec:
  containers:
  - name: app
    image: myapp:latest
  - name: datadog-agent
    image: datadog/agent:latest
```

**Features:**
- Automatic dashboard generation
- Anomaly detection (ML-based)
- Service dependency maps
- Cost attribution

### New Relic: Language-Native

```java
// Java agent (automatic instrumentation)
java -javaagent:/opt/newrelic/newrelic.jar \
  -Dnewrelic.config.file=/etc/newrelic/newrelic.yml \
  -jar app.jar

// No code changes needed:
// - HTTP latency tracked
// - Database queries monitored
// - Error tracking automatic
```

### Honeycomb: Event-Driven

```javascript
// Send structured events
const tracer = new Tracer({
  apiKey: "YOUR_KEY"
});

tracer.startSpan("checkout").then(span => {
  span.addField("user_id", 123);
  span.addField("items", 5);
  span.addField("total", 99.99);
  
  // Automatic context propagation
  callPaymentService(span);
  
  span.end();
});
```

---

## 5. Observability Best Practices

### Know Your Baselines

```
Before going on-call, know what "normal" looks like:
- Request latency: p50=50ms, p95=200ms, p99=1s
- Error rate: <0.1%
- Disk usage: 45%
- Database connections: 50/200 available
```

### Correlation Analysis

```promql
# When request latency spikes, what else changes?
1. Check error rate (errors causing slow responses?)
2. Check database latency (DB issue?)
3. Check GC pauses (garbage collection?)
4. Check network metrics (network saturation?)
```

### Cardinality Management

```yaml
# DON'T do this (too many labels):
metric{user_id, session_id, request_id, ...}

# DO this (bounded cardinality):
metric{region, service, endpoint, status}

# Cardinality explosion detection:
# If a metric has >1000 unique label combinations, investigate!
```

---

Class 7.6.2:
	Title: Database Performance and Optimization
	Description: Query tuning, migrations, and replication strategies.
Content Type: text
Duration: 500 
Order: 2
		Text Content :
 # Database Performance at Scale

## 1. Query Optimization with EXPLAIN

Every slow query starts with EXPLAIN.

### EXPLAIN Output (PostgreSQL)

```sql
EXPLAIN ANALYZE
SELECT o.id, o.total, c.name
FROM orders o
JOIN customers c ON o.customer_id = c.id
WHERE o.created_at > '2024-01-01';

-- Output:
-- Seq Scan on orders o (cost=0.00..50000.00 rows=10000)
--   Filter: created_at > '2024-01-01'
--   -> Index Scan using idx_order_customer on customers c
```

**Red Flags:**
- `Seq Scan` on large table → missing index
- `Nested Loop` with millions of rows → join performance issue
- High actual rows vs estimated rows → statistics out of date

### Index Strategy

```sql
-- Wrong: Single-column index
CREATE INDEX idx_user ON orders(user_id);

-- Better: Multi-column index (covers more queries)
CREATE INDEX idx_order_user_date 
  ON orders(user_id, created_at DESC)
  INCLUDE (total);  -- Include without sorting

-- Best: Covering index (query doesn't touch table)
-- Result: "Index Only Scan"
```

### Query Analysis Tools

```sql
-- PostgreSQL: Detailed stats
SELECT query, mean_exec_time, calls
FROM pg_stat_statements
WHERE mean_exec_time > 1000  -- Queries taking >1 second
ORDER BY mean_exec_time DESC;

-- MySQL: Performance schema
SELECT * FROM performance_schema.events_statements_summary_by_digest
WHERE SUM_TIMER_WAIT > 1000000000000  -- 1 second in picoseconds
ORDER BY SUM_TIMER_WAIT DESC;
```

---

## 2. Database Migrations (Zero-Downtime)

Migrations must work without downtime.

### Flyway (Version-Based)

```sql
-- V1__create_users_table.sql
CREATE TABLE users (
  id BIGINT PRIMARY KEY,
  name VARCHAR(255) NOT NULL,
  email VARCHAR(255) NOT NULL
);

-- V2__add_phone_column.sql
ALTER TABLE users ADD COLUMN phone VARCHAR(20);

-- V3__add_unique_email.sql
ALTER TABLE users ADD CONSTRAINT uq_email UNIQUE (email);
```

```bash
# Flyway automatically tracks migrations
flyway info  # See migration status
flyway migrate  # Apply pending migrations
flyway validate  # Check for conflicts
```

### Liquibase (Declarative)

```yaml
# changelog.yaml
databaseChangeLog:
  - changeSet:
      id: 1
      author: alice
      changes:
        - createTable:
            tableName: users
            columns:
              - column:
                  name: id
                  type: BIGINT
                  constraints:
                    primaryKey: true
  
  - changeSet:
      id: 2
      author: bob
      changes:
        - addColumn:
            tableName: users
            columns:
              - column:
                  name: phone
                  type: VARCHAR(20)
```

```bash
liquibase update
liquibase rollback --count 1  # Undo last change
```

### Zero-Downtime Pattern (Backwards Compatibility)

```sql
-- Step 1: Add new column (no constraint)
ALTER TABLE users ADD COLUMN phone_new VARCHAR(20);

-- Step 2: App writes to both old and new columns
INSERT INTO users (name, phone, phone_new) VALUES (...)

-- Step 3: Backfill existing data
UPDATE users SET phone_new = phone WHERE phone_new IS NULL;

-- Step 4: Add constraint (after backfill complete)
ALTER TABLE users ALTER COLUMN phone_new SET NOT NULL;

-- Step 5: Drop old column
ALTER TABLE users DROP COLUMN phone;

-- Step 6: Rename
ALTER TABLE users RENAME COLUMN phone_new TO phone;
```

---

## 3. Replication Topologies

### Primary-Replica (Master-Slave)

```
Write ─→ Primary (PostgreSQL)
             ↓ (WAL streaming)
         Replica (Read-Only)
```

```sql
-- Primary: Enable replication
ALTER SYSTEM SET wal_level = replica;
ALTER SYSTEM SET max_wal_senders = 3;

-- Replica: Connect to primary
SELECT * FROM pg_create_physical_replication_slot('slot1');
```

**Monitoring:**
```sql
-- Check replication lag
SELECT client_addr, state,
       (pg_wal_lsn_diff(pg_current_wal_lsn(), flush_lsn) / 1024 / 1024)::int AS flush_lag_mb
FROM pg_stat_replication;
```

### Primary-Primary (Multi-Master)

**Challenges:**
- Write conflicts (both masters wrote to same row)
- Replication loops (need identifier to prevent)
- Split-brain scenarios

**Solutions:**
- Conflict resolution (last-write-wins, custom logic)
- Pglogical (PostgreSQL extension with conflict handling)
- MongoDB replication sets (built-in)

---

## 4. Backup and Disaster Recovery

### Backup Types

| Type | Duration | Restore Time | Size | Use Case |
| :--- | :--- | :--- | :--- | :--- |
| **Physical (pg_basebackup)** | 10 min | 5 min | 50GB | Fast restore |
| **Logical (pg_dump)** | 1 hour | 2 hours | 5GB | Portable, smaller |
| **Continuous WAL** | Real-time | Varies | Minimal | Point-in-time recovery |
| **Cloud Snapshots** | Instant | 5 min | Large | Disaster recovery |

### Point-in-Time Recovery (PITR)

```bash
# Backup: base + WAL
pg_basebackup -D /backup/base -Ft -z -P
# WAL files: /var/lib/postgresql/pg_wal/*

# To restore to 2024-01-15 14:00:00
mkdir /restore/data
tar -xzf /backup/base/base.tar.gz -C /restore/data

# Create recovery config
cat > /restore/data/recovery.conf << EOF
restore_command = 'cp /backup/wal/%f %p'
recovery_target_time = '2024-01-15 14:00:00'
EOF

# Start PostgreSQL
pg_ctl -D /restore/data start
```

---

## 5. Sharding and Partitioning

Splitting data across multiple databases or tables.

### Horizontal Partitioning (Sharding)

```
User IDs 1-1000    → Shard 1
User IDs 1001-2000 → Shard 2
User IDs 2001-3000 → Shard 3
```

```sql
-- Shard key: user_id
INSERT INTO shard_1.users VALUES (500, 'Alice', ...);
INSERT INTO shard_2.users VALUES (1500, 'Bob', ...);
```

**Challenges:**
- **Shard key selection** is critical (hot shards?)
- **Resharding** when adding shards
- **Cross-shard joins** are expensive

### Vertical Partitioning (Decomposition)

```
users table:
id, name, email (frequently accessed)

users_profile table:
id, bio, avatar, preferences (accessed less)
```

**Benefits:**
- Better cache hit rate (smaller hot data)
- Easier to scale hot data independently

### Hash Partitioning (Built-in)

```sql
-- PostgreSQL declarative partitioning
CREATE TABLE orders (
  id BIGINT,
  user_id BIGINT,
  total DECIMAL
) PARTITION BY HASH (user_id);

CREATE TABLE orders_1 PARTITION OF orders
  FOR VALUES WITH (MODULUS 4, REMAINDER 0);
CREATE TABLE orders_2 PARTITION OF orders
  FOR VALUES WITH (MODULUS 4, REMAINDER 1);
-- ... more partitions

-- Queries automatically go to correct partition
SELECT * FROM orders WHERE user_id = 12345;
-- PostgreSQL routes to appropriate partition
```

---

Module 8:
Title: Security & Compliance (DevSecOps)
Description: Implement security-first architecture and DevSecOps practices. Learn vulnerability management, secrets handling, compliance, and security automation.
Order: 8
Learning Outcomes:
Implement DevSecOps practices
Manage secrets securely
Understand compliance requirements
Automate security scanning

Topic 8.1:
Title: DevSecOps Fundamentals
Order: 1

Class 8.1.1:
	Title: Security-First Mindset
	Description: Shift-Left, Defense in Depth, and Zero Trust.
Content Type: text
Duration: 400 
Order: 1
		Text Content :
# DevSecOps: Security is Everyone's Job

DevSecOps integrates security into every phase of the software delivery lifecycle. Instead of treating security as a final approval step, it becomes a continuous, shared responsibility across development, operations, and security teams.

---

## 1. Shift-Left Security

Traditional security models introduced security checks **late** in the process, leading to last-minute rework and delayed releases.

Shift-left security moves security controls **earlier** in the lifecycle.

- Security checks begin during:
  - Development
  - Code review
  - Continuous Integration
- Vulnerabilities are caught when they are:
  - Cheaper to fix
  - Easier to understand
  - Less risky to remediate

**Practical Actions:**
- Run static code analysis in the IDE
- Scan dependencies on every commit
- Enforce security gates in CI pipelines

**Outcome:**  
Faster delivery with fewer security surprises.

---

## 2. Defense in Depth

No single control is sufficient to protect modern systems. Security must be layered.

- Perimeter controls reduce attack surface
- Application-level controls enforce behavior
- Identity controls restrict access
- Data-level controls protect sensitive information

Each layer assumes the previous one can fail.

**Core Principle:**  
An attacker should have to break multiple, independent systems to succeed.

---

## 3. Zero Trust Architecture

Zero Trust abandons implicit trust based on network location.

- Network presence does not imply trust
- Every request must be authenticated and authorized
- Identity becomes the primary security boundary

### Traditional Model
- Trust is granted once inside the network
- Assumes internal traffic is safe

### Zero Trust Model
- Trust is evaluated continuously
- Applies equally to:
  - Users
  - Services
  - APIs

**Common Implementations:**
- Mutual TLS (mTLS) between services
- Short-lived credentials
- Strong identity verification

**Why this matters:**  
Modern systems are distributed, cloud-based, and constantly changing. Network-based trust no longer scales.

---

## Key Takeaways

- Security must start early, not at the end
- Layered defenses reduce blast radius
- Zero Trust treats every interaction as untrusted by default

**Mental Model:**  
Security is not a team.  
Security is a behavior.

---

Topic 8.2:
Title: Container & Cloud Security
Order: 2

Class 8.2.1:
	Title: Container Security
	Description: Image scanning, Distroless, and Runtime protection.
Content Type: text
Duration: 450 
Order: 1
		Text Content :
# Container Security: Hardening the Box

Container security focuses on reducing risk across the entire container lifecycle: build time, deploy time, and runtime. The goal is to minimize attack surface and detect malicious behavior as early as possible.

---

## 1. Image Scanning (Supply Chain Security)

Most applications are built on top of public base images such as `node:14` or `python:3.9`. These images become part of your software supply chain.

**The Risk:**
- Base images may contain known vulnerabilities (CVEs)
- A vulnerable dependency becomes your vulnerability

**Tools:**
- Trivy
- Clair
- Snyk

**CI/CD Best Practice:**
- Scan images during build
- Define severity thresholds
- Fail the pipeline immediately if a **Critical** or **High** CVE is detected

**Principle:**  
Never deploy what you would not be willing to defend.

---

## 2. Minimal Base Images (Distroless)

### The Bloat Problem
Traditional base images like `ubuntu` or `alpine` include:
- Shells (`bash`, `sh`)
- Package managers (`apt`, `apk`)
- Network tools (`curl`, `wget`)

These tools are extremely useful to attackers once they gain access.

### The Solution: Distroless Images
- Developed by Google
- Contain only:
  - Your application
  - Required runtime libraries
- No shell
- No package manager

**Security Benefit:**
- No interactive shell to exploit
- No ability to install additional tools
- Significantly reduced attack surface

**Trade-off:**
- Debugging requires better observability and logging
- You debug via metrics, logs, and tracing, not SSH

---

## 3. Runtime Security (Falco)

Image scanning is static. It tells you what *might* be vulnerable. Runtime security tells you what is *actually happening*.

### Falco
- Kernel-level runtime security tool
- Monitors system calls inside containers
- Detects abnormal behavior in real time

**Example Alerts:**
- A shell spawned inside a production container
- Unexpected outbound network connections
- File modifications in immutable containers

**Why this matters:**
- Containers should be predictable
- Any deviation from expected behavior is a potential breach

---

## Key Takeaways

- Secure the supply chain before deployment
- Reduce the attack surface as much as possible
- Assume breaches can happen and monitor aggressively

**Mental Model:**  
If an attacker gets in, make sure there is nothing useful they can do.

---

---

## Base Image Comparison

| Base Image   | Size (Approx) | Includes Shell | Package Manager | Use Case | Security Posture |
|-------------|---------------|----------------|------------------|----------|------------------|
| **Alpine** | ~5–7 MB | Yes (`sh`) | Yes (`apk`) | Lightweight general-purpose base image | Better than Ubuntu, but still exploitable at runtime |
| **Slim** (e.g., `python:3.11-slim`) | ~40–60 MB | Yes | Yes (`apt`) | Balance between usability and size | Reduced bloat, but still a broad attack surface |
| **Distroless** | ~10–20 MB | No | No | Production workloads where security matters | Very strong – minimal runtime attack surface |
| **Scratch** | ~0 MB | No | No | Static binaries (Go, Rust) | Maximum security, zero tooling |

---

### Practical Guidance
- **Alpine:** Good for development and quick experiments.
- **Slim:** Acceptable when debugging tools are still required.
- **Distroless:** Recommended default for production containers.
- **Scratch:** Best option when your application can be statically compiled.

**Rule of Thumb:**  
If your container needs a shell in production, you are already taking on extra risk.

---

Class 8.2.2:
	Title: Kubernetes Security
	Description: RBAC, Network Policies, and OPA.
Content Type: text
Duration: 450 
Order: 2
		Text Content :
# Kubernetes Security: Locking Down the Cluster

## 1. Pod Security Standards (PSS)
Pod Security Standards define **how privileged a Pod is allowed to be**.

* **Privileged Mode**
  * Running a container as `--privileged` gives it near-root access to the **host node**.
  * This breaks container isolation.
  * **Rule:** Never allow privileged containers unless absolutely required (e.g., low-level CNI plugins).

* **RunAsNonRoot**
  * Enforces containers to run as a non-root user.
  * Prevents privilege escalation inside the container.
  * **Best Practice:**  
    ```yaml
    securityContext:
      runAsNonRoot: true
      runAsUser: 1000
    ```

* **Production Baseline**
  * Disallow privileged containers
  * Disallow hostPath volumes
  * Enforce non-root execution

---

## 2. Network Policies (Micro-Segmentation)
By default, Kubernetes networking is **flat and permissive**.
Any Pod can talk to any other Pod across namespaces.

* **NetworkPolicy**
  * Acts as an internal firewall.
  * Controls traffic using **Pod selectors**, **Namespaces**, and **Ports**.

* **Example Rule**
  * Allow only backend Pods to access the database:
    * Source: `backend`
    * Destination: `db`
    * Port: `5432`
  * Everything else is denied.

* **Default Deny Strategy**
  1. Apply a default deny-all policy
  2. Explicitly allow required traffic paths  
     `frontend → backend → database`

---

## 3. Admission Controllers (OPA / Kyverno)
Prevention is better than detection.

Admission Controllers intercept requests **before** objects are created.

* **Policy as Code**
  * Security rules are written as versioned code.
  * Enforced automatically on every `kubectl apply`.

* **Common Policies**
  * Disallow `Service.type=LoadBalancer` in Dev
  * Enforce resource requests and limits
  * Block images not coming from approved registries
  * Enforce `runAsNonRoot=true`

* **OPA / Kyverno Flow**
  1. Developer runs `kubectl apply`
  2. Admission Controller evaluates policy
  3. Request is either **accepted** or **rejected**
  4. Insecure workloads never reach the cluster

---

## Admission Controller Comparison

| Tool     | Policy Language | Kubernetes-Native | Ease of Use | Typical Use |
|---------|-----------------|-------------------|-------------|-------------|
| **OPA Gatekeeper** | Rego | No (CRDs) | Medium–Hard | Complex compliance rules |
| **Kyverno** | YAML | Yes | Easy | Kubernetes-native security policies |

---

### Production Security Rule
If a security rule can be automated, **never rely on documentation or reviews**.  
Enforce it with Admission Controllers.

---

Topic 8.3:
Title: Secrets Management
Order: 3

Class 8.3.1:
	Title: Secrets Management Solutions
	Description: HashiCorp Vault and External Secrets.
Content Type: text
Duration: 450 
Order: 1
		Text Content :
 # Secrets Management: Protecting the Keys

## 1. The Problem with Kubernetes Secrets
* K8s Secrets are stored in `etcd` in **Base64** (encoding, not encryption). If anyone can read `etcd`, they have your passwords.
* *Best Practice:* Enable "Encryption at Rest" for K8s Secrets.

---

## 2. HashiCorp Vault (The Gold Standard)
Vault is a centralized fortress for secrets.
* **Dynamic Secrets:** This is the killer feature.
    * App asks Vault: "I need to access the DB."
    * Vault creates a **temporary** username/password on the DB valid for 1 hour.
    * Vault gives it to the App.
    * After 1 hour, Vault automatically deletes the user.
    * *Result:* Even if the password leaks, it is already useless.

---

## 3. External Secrets Operator (ESO)
Most teams use AWS Secrets Manager or Azure Key Vault.
* **ESO:** A K8s operator that syncs secrets from AWS/Azure *into* Kubernetes Secrets automatically.
* *Benefit:* Developers manage secrets in the Cloud Provider UI; K8s consumes them natively.

---

Topic 8.4:
Title: Vulnerability Management
Order: 4

Class 8.4.1:
	Title: Security Scanning in CI/CD
	Description: SAST, DAST, and SCA.
Content Type: text
Duration: 400 
Order: 1
		Text Content :
# The Security Testing Triad

Security testing is not one tool or one stage. It is a **layered approach** applied across the SDLC.

## 1. SAST (Static Application Security Testing)
* **What:** Scans **source code** without running the application (White Box).
* **When:** Runs on every Pull Request or commit.
* **Catches:**
  * SQL Injection
  * Hardcoded secrets
  * Insecure crypto usage
  * Buffer overflows
* **Strength:** Finds issues early, cheap to fix.
* **Limitation:** Cannot detect runtime or environment-specific issues.
* *Tools:* SonarQube, Checkmarx, Semgrep

---

## 2. DAST (Dynamic Application Security Testing)
* **What:** Scans the **running application** (Black Box).
* **When:** Runs against a deployed app (Staging/Pre-prod).
* **Catches:**
  * Auth bypass
  * Broken access control
  * Session and cookie manipulation
* **Strength:** Sees the app exactly like an attacker would.
* **Limitation:** Slower, noisy, and requires a running environment.
* *Tools:* OWASP ZAP, Burp Suite

---

## 3. SCA (Software Composition Analysis)
* **What:** Scans **third-party dependencies**.
* **Reality:** ~90% of modern apps are open-source code.
* **Catches:**
  * Known CVEs (e.g., Log4j, OpenSSL)
  * License violations
* **Strength:** Prevents supply-chain attacks.
* **Limitation:** Cannot detect custom code vulnerabilities.
* *Tools:* Snyk, Dependabot, WhiteSource

---

## SAST vs DAST vs SCA

| Dimension | SAST | DAST | SCA |
|--------|------|------|-----|
| Analysis Type | Static | Dynamic | Static |
| Visibility | Source Code | Running App | Dependencies |
| Testing Model | White Box | Black Box | Metadata / CVE DB |
| Pipeline Stage | PR / Build | Pre-release | PR / Build |
| Finds Zero-Days | Yes | Yes | No |
| Supply Chain Coverage | No | No | Yes |

---

### Production Rule
No single tool is enough.  
**SAST + SCA run early, DAST runs late.**  
Together, they form a complete application security baseline.


---

Topic 8.5:
Title: Compliance & Auditing
Order: 5

Class 8.5.1:
	Title: Compliance Standards
	Description: SOC 2, GDPR, and PCI-DSS.
Content Type: text
Duration: 450 
Order: 1
		Text Content :
 # Compliance: The Necessary Evil

## 1. SOC 2 (Service Organization Control)
* **Target:** SaaS companies storing customer data.
* **Focus:** Security, Availability, and Confidentiality.
* **The Audit:** You must prove you have processes (e.g., "Show me the list of everyone who offboarded last month and prove their access was revoked within 24 hours").

---

## 2. GDPR (General Data Protection Regulation)
* **Target:** Anyone with EU users.
* **Key Right:** "Right to be Forgotten." If a user deletes their account, you must scrub their data from the Primary DB **and** backups.
* **Data Residency:** German data often must stay in Germany.

---

## 3. PCI-DSS (Payment Card Industry)
* **Target:** Anyone handling Credit Cards.
* **Rule:** Never store the CVV code.
* **Network:** The Payment Environment must be completely isolated (firewalled) from the rest of the company network.

---

Class 8.5.2:
	Title: Audit Logging & Monitoring
	Description: CloudTrail and SIEM.
Content Type: text
Duration: 350 
Order: 2
		Text Content :
 # Audit Logging: Who Did What?

## 1. The Audit Trail
If you get hacked, the first question is "How?" The answer is in the logs.
* **Immutable Logs:** Hackers try to delete logs to cover their tracks. Send your logs to a "Write-Once-Read-Many" (WORM) storage bucket that even Root cannot delete.

---

## 2. Cloud Audit Logs (CloudTrail)
* **AWS CloudTrail:** Records every single API call.
* *Example:* "User 'Bob' called `DeleteBucket` on 'Prod-Data' at 3:00 AM from IP 1.2.3.4."
* *Alerting:* You should have an alarm that fires instantly if CloudTrail is turned off.

---

## 3. SIEM (Security Information and Event Management)
* **The Problem:** You have logs from AWS, Linux, K8s, and Firewalls.
* **The Solution:** A SIEM (like Splunk or Datadog Security) aggregates them all to find patterns.
* *Scenario:* "5 failed login attempts on VPN" + "1 successful login on AWS" = **Account Takeover**.

---

Topic 8.6:
Title: Secrets Management Deep-Dive
Order: 6

Class 8.6.1:
	Title: HashiCorp Vault Advanced Topics
	Description: Architecture, engines, authentication, and dynamic secrets.
Content Type: text
Duration: 600 
Order: 1
		Text Content :
 # HashiCorp Vault: The Secrets Fortress

Vault is the industry standard for centralized secret management. It goes beyond simple password storage—it provides dynamic secret generation, automatic rotation, and fine-grained access control.

---

## 1. Architecture: The Foundation

### Core Components

* **Vault Server:** The API endpoint that stores and distributes secrets. Stateless and scalable.
* **Storage Backend:** Where secrets are actually stored (encrypted).
  * Integrated Storage (Raft)
  * External: Consul, S3, DynamoDB, PostgreSQL
* **Secrets Engines:** Plugins that generate or store different types of secrets.
* **Authentication Methods:** How clients prove their identity (tokens, AWS IAM, K8s, LDAP).
* **Policies:** Fine-grained access control rules written in HCL.

### High Availability Setup

```
                    ┌─────────────────────┐
                    │   Load Balancer     │
                    └──────────┬──────────┘
                               │
            ┌──────────┬────────┼────────┬──────────┐
            │          │        │        │          │
        ┌───▼──┐   ┌──▼───┐ ┌─▼────┐ ┌──▼───┐  ┌──▼───┐
        │Vault │   │Vault │ │Vault │ │Vault │  │Vault │
        │ 1    │   │ 2    │ │ 3    │ │ 4    │  │ 5    │
        │(Lead)│   │(Seal)│ │(Seal)│ │(Seal)│  │(Seal)│
        └──┬───┘   └──┬───┘ └─┬────┘ └──┬───┘  └──┬───┘
           │          │      │         │       │
           └──────────┼──────┼─────────┼───────┘
                      │
                ┌─────▼─────────┐
                │Integrated Raft│
                │ Storage       │
                └───────────────┘
```

**Election Process:**
- Nodes form a Raft cluster
- One node is elected "Lead"
- Others are "Standbys"
- On Lead failure, new election occurs within seconds

---

## 2. Secrets Engines (The Powerhouses)

### KV (Key-Value) - Simple Storage

```bash
vault secrets enable kv-v2

# Store a secret
vault kv put secret/database/prod \
  username="admin" \
  password="$(openssl rand -base64 32)"

# Retrieve a secret
vault kv get secret/database/prod
```

**Versions & Metadata:**
- All writes are versioned
- Can recover old secrets: `vault kv get -version=5 secret/database/prod`
- Useful for rotation and disaster recovery

---

### Database Engine - Dynamic Credentials

**The Revolution:** Stop storing static passwords. Generate temporary credentials on demand.

```bash
vault secrets enable database

# Configure connection to PostgreSQL
vault write database/config/postgres \
  plugin_name=postgresql-database-plugin \
  allowed_roles="readonly" \
  connection_url="postgresql://admin:password@postgres.local:5432/postgres" \
  username="vault" \
  password="vaultpass"

# Define a role
vault write database/roles/readonly \
  db_name=postgres \
  creation_statements="CREATE ROLE \"{{name}}\" WITH LOGIN PASSWORD '{{password}}' VALID UNTIL '{{expiration}}'; GRANT SELECT ON ALL TABLES IN SCHEMA public TO \"{{name}}\";" \
  default_ttl="1h" \
  max_ttl="24h"

# Request temporary credentials
vault read database/creds/readonly
```

**Output:**
```
Key                Value
---                -----
lease_id           database/creds/readonly/AbCdEfGhIjKlMnOpQrStUv
lease_duration     3600s
lease_renewable    true
password           randomPassword123!
username           v-token-readonly-AbCdEfGhI
```

**Why This Rocks:**
- Credentials expire automatically (no manual revocation needed)
- Each credential is unique (audit trail is clear)
- Database can revoke individual users without affecting others
- Works with: PostgreSQL, MySQL, MongoDB, Cassandra, etc.

---

### AWS Secrets Engine - Cloud-Native Credentials

Generate AWS access keys with limited permissions.

```bash
vault secrets enable aws

vault write aws/config/root \
  access_key=AKIAIOSFODNN7EXAMPLE \
  secret_key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY \
  region=us-east-1

# Define a role with specific IAM policy
vault write aws/roles/s3-readonly \
  credential_type=iam_user \
  iam_groups=S3ReadOnly

# Request credentials
vault read aws/creds/s3-readonly
```

**Other Engines:**
- **SSH:** Generate SSH certificates
- **PKI:** Create TLS certificates on-the-fly
- **LDAP:** Authenticate against directory servers
- **Kubernetes Auth:** Pods authenticate with their service account

---

## 3. Authentication Methods (How Secrets Get Out)

### Token Authentication (The Default)

```bash
# Create a token with specific policies
vault token create \
  -policy="app-policy" \
  -ttl=24h \
  -display-name="app-token-prod"

# Output: hvs.CAESIF1nTsFhdFS...
```

**Token Leasing & Renewal:**
- Tokens have TTL (time-to-live)
- Apps can renew tokens before expiration
- Max TTL enforced by policy (child tokens can't exceed parent TTL)

---

### Kubernetes Authentication

Vault trusts the Kubernetes API server. Pods authenticate using their Service Account.

```bash
vault auth enable kubernetes

vault write auth/kubernetes/config \
  token_reviewer_jwt="@/var/run/secrets/kubernetes.io/serviceaccount/token" \
  kubernetes_host="https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT" \
  kubernetes_ca_cert="@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"

vault write auth/kubernetes/role/app \
  bound_service_account_names=app \
  bound_service_account_namespaces=default \
  policies="app-policy" \
  ttl=24h
```

**Pod Login:**
```bash
curl --request POST \
  --data @jwt.json \
  http://vault.vault.svc.cluster.local:8200/v1/auth/kubernetes/login
```

---

### AWS IAM Authentication

EC2 instances or Lambda functions authenticate using their IAM identity.

```bash
vault auth enable aws

vault write auth/aws/config/client \
  access_key=AKIAIOSFODNN7EXAMPLE \
  secret_key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY

vault write auth/aws/role/app-role \
  auth_type=iam \
  bound_iam_principal_arn="arn:aws:iam::123456789012:role/AppRole" \
  policies="app-policy"
```

**From EC2:**
```bash
# EC2 auto-signs AWS SigV4 request
vault login -method=aws -path=auth/aws/login
```

**Advantage:** No credentials in environment variables. IAM role = automatic trust.

---

### LDAP/Active Directory

Enterprise organizations centralize identity.

```bash
vault auth enable ldap

vault write auth/ldap/config \
  url="ldap://ldap.company.com" \
  userdn="cn=users,dc=company,dc=com" \
  groupdn="cn=groups,dc=company,dc=com"

vault write auth/ldap/groups/devops \
  policies="devops-policy,default"
```

---

## 4. Policies: The Access Control

Policies are written in HCL and define what a user/app can do.

```hcl
# Example policy
path "secret/data/app/*" {
  capabilities = ["read", "list"]
}

path "secret/metadata/app/*" {
  capabilities = ["read", "list"]
}

path "database/creds/app-role" {
  capabilities = ["read"]
}

path "aws/creds/s3-access" {
  capabilities = ["read"]
}

path "auth/token/renew-self" {
  capabilities = ["update"]
}
```

**Capabilities:**
- `read` – Fetch a secret
- `create` – Create a new secret (overwrite not allowed)
- `update` – Modify an existing secret
- `delete` – Remove a secret
- `list` – Enumerate paths
- `sudo` – Bypass policy (admin only)
- `deny` – Explicitly deny access

**Policy Best Practices:**
1. **Least Privilege:** Users should only access what they need.
2. **Wildcards:** Use `secret/app/*` instead of `secret/*`
3. **Metadata vs Data:** Separate read permissions for metadata and actual values
4. **Service Accounts:** Create specific policies for each application

---

## 5. Secret Rotation & Automation

### Manual Rotation (Don't Do This)

```bash
# Old way: manually generate new password
vault kv put secret/database/prod \
  username="admin" \
  password="$(openssl rand -base64 32)"
# Hope the app picked it up... (it didn't)
```

### Automated Rotation (Vault's Way)

Vault can rotate database passwords automatically.

```bash
vault write -f database/rotate-root/postgres
```

Vault generates a new password, updates it in the database, and stores it. Applications never know.

### Secret Sync (Enterprise Feature)

Vault can sync secrets to external systems:
- AWS Secrets Manager
- Azure Key Vault
- Kubernetes Secrets
- GitHub Secrets

---

## 6. Vault Agent (The Sidecar)

Vault Agent runs as a daemon and handles authentication/secret delivery.

```hcl
# /etc/vault/agent.hcl
pid_file = "/tmp/pidfile"

vault {
  address = "https://vault.service.consul:8200"
}

auto_auth {
  method {
    type = "kubernetes"
    
    config = {
      role = "app"
    }
  }

  sink {
    type = "file"
    config = {
      path = "/tmp/.vault-token"
    }
  }
}

cache {
  use_auto_auth_token = true
}

listener "unix" {
  address = "/tmp/vault.sock"
  tls_disable = true
}
```

**Features:**
- Auto-authentication (handles token renewal)
- Caching (reduces load on Vault server)
- Template rendering (inject secrets into config files)
- Listener (proxy to Vault for client requests)

---

## 7. Kubernetes Integration (The Real-World Setup)

### Helm Deployment

```bash
helm repo add hashicorp https://helm.releases.hashicorp.com
helm install vault hashicorp/vault \
  --set server.ha.enabled=true \
  --set server.ha.replicas=3 \
  --set server.dataStorage.size=10Gi
```

### Vault Agent Injector (Automatic Secret Injection)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app-pod
  annotations:
    vault.hashicorp.com/agent-inject: "true"
    vault.hashicorp.com/role: "app"
    vault.hashicorp.com/agent-inject-secret-database: "secret/data/database/prod"
    vault.hashicorp.com/agent-inject-template-database: |
      {{- with secret "secret/data/database/prod" -}}
      export DB_USER="{{ .Data.data.username }}"
      export DB_PASSWORD="{{ .Data.data.password }}"
      {{- end }}
spec:
  serviceAccountName: app
  containers:
  - name: app
    image: myapp:latest
    env:
    - name: VAULT_ADDR
      value: "http://vault.vault.svc.cluster.local:8200"
```

**Magic:**
- Mutating webhook intercepts pod creation
- Injects Vault Agent sidecar
- Agent authenticates with K8s ServiceAccount
- Secrets rendered into mounted files or env vars
- App never talks to Vault directly

---

## 8. Common Interview Questions

**Q: How does Vault prevent a compromised app from reading all secrets?**
A: Policies are tied to authentication identity. If an app authenticates as `app-role`, it can only access secrets listed in the `app-role` policy.

**Q: What happens if a Vault node crashes?**
A: In HA mode, another node becomes Lead immediately. Clients retry and connect to the new Lead.

**Q: Can I store secrets in Vault and Git?**
A: No. Vault is the source of truth. Git should have only: infrastructure code, Vault addresses, role names—not secrets.

---

## Production Checklist

- [ ] **HA Enabled:** At least 3 nodes with Raft/Integrated Storage
- [ ] **Backups:** Regularly backup the storage backend
- [ ] **Audit Logging:** All secret accesses logged to CloudTrail/Datadog
- [ ] **Authentication:** Kubernetes (k8s) or AWS IAM, not token-based
- [ ] **Policies:** Least privilege, regularly audited
- [ ] **Rotation:** Database engines used for dynamic credentials
- [ ] **Monitoring:** Alert on failed auth attempts, HA leader changes
- [ ] **Sealed/Unsealed:** Automatic unsealing (Cloud KMS, not manual)

---

Class 8.6.2:
	Title: SOPS and Sealed Secrets
	Description: Encrypting secrets in Git.
Content Type: text
Duration: 400 
Order: 2
		Text Content :
 # Encrypting Secrets in Git (SOPS & Sealed Secrets)

## 1. The Problem: Secrets in Git

Git repositories are version controlled, shared, and often public on GitHub. Storing secrets here is a disaster waiting to happen.

**Common Mistakes:**
- `export AWS_SECRET_ACCESS_KEY=AKIA...` in shell scripts
- Database passwords hardcoded in config files
- SSH keys accidentally committed

Once in Git history, they are there forever (even if you delete them).

---

## 2. SOPS (Secrets Operations) - AWS/GCP/Azure

SOPS encrypts specific fields in YAML/JSON files using cloud KMS.

### Installation & Setup

```bash
# Install SOPS
brew install sops

# Install AWS plugin
brew install sops-aws-kms-plugin
```

### Encrypt a File (Using AWS KMS)

```bash
# Create a secrets file
cat > secrets.yaml << EOF
database:
  username: admin
  password: supersecret
  host: db.example.com
EOF

# Encrypt using AWS KMS
export AWS_PROFILE=prod
sops --encrypt \
  --kms "arn:aws:kms:us-east-1:123456789012:key/12345678-1234-1234-1234-123456789012" \
  secrets.yaml > secrets.enc.yaml

# Now safe to commit!
git add secrets.enc.yaml
git commit -m "Add encrypted secrets"
```

### The Encrypted File (Human-Readable!)

```yaml
database:
  username: admin
  password: ENC[AES256_GCM,data:abcd1234...,iv:xyz,tag:abc,type:str]
  host: db.example.com
sops:
  kms:
  - arn: arn:aws:kms:us-east-1:123456789012:key/12345678...
    created_at: '2024-01-15T10:00:00Z'
    enc: AES_encryption_key_encrypted_by_KMS
```

**Magic:** SOPS shows you the plaintext fields but encrypts only the values.

### Decrypt (Automatic During Deployment)

```bash
# Developers decrypt locally
sops -d secrets.enc.yaml

# In CI/CD pipelines
sops -d secrets.enc.yaml | kubectl apply -f -
```

### Git Workflow with SOPS

```bash
# 1. Developer edits secret
sops secrets.enc.yaml
# SOPS automatically decrypts, opens in $EDITOR, re-encrypts on save

# 2. Diff before committing
sops diff secrets.enc.yaml
# Shows human-readable diffs of changes

# 3. Commit
git add secrets.enc.yaml
git commit -m "Update database password"

# 4. CI/CD decrypts with KMS key (role-based access)
sops -d secrets.enc.yaml | kubectl apply -f -
```

### KMS Key Access Control

```hcl
# AWS IAM Policy: Only specific roles can decrypt
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::123456789012:role/GitHubActionsRole"
      },
      "Action": [
        "kms:Decrypt",
        "kms:DescribeKey"
      ],
      "Resource": "arn:aws:kms:us-east-1:123456789012:key/*"
    }
  ]
}
```

---

## 3. GCP Google Cloud KMS

```bash
# Encrypt using GCP KMS
sops --encrypt \
  --gcp-kms "projects/my-project/locations/us/keyRings/sops/cryptoKeys/main" \
  secrets.yaml > secrets.enc.yaml
```

**Advantage:** Seamless GCP integration, no separate credentials needed.

---

## 4. Azure Key Vault

```bash
# Encrypt using Azure Key Vault
sops --encrypt \
  --azure-kv https://myvault.vault.azure.net/keys/sops/version \
  secrets.yaml > secrets.enc.yaml
```

---

## 5. Sealed Secrets (Kubernetes-Native)

Sealed Secrets is a Kubernetes controller that encrypts secrets so they can safely be committed to Git.

### Installation

```bash
helm repo add sealed-secrets https://kubernetes.github.io/sealed-secrets
helm install sealed-secrets -n kube-system sealed-secrets/sealed-secrets
```

### Seal a Secret

```bash
# 1. Create a normal Kubernetes secret
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  password: c3VwZXJzZWNyZXQ=  # base64 encoded
EOF

# 2. Export and seal it
kubectl get secret mysecret -o yaml | \
kubeseal -f - > mysealedsecret.yaml

# 3. Delete the original secret
kubectl delete secret mysecret
```

### The Sealed Secret (Safe for Git!)

```yaml
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: mysecret
spec:
  encryptedData:
    password: AgBvA3FgK7x8Kp... (encrypted blob)
  template:
    metadata:
      name: mysecret
    type: Opaque
```

### How It Works

1. **Sealing:** Uses the Sealed Secrets controller's public key (stored in the cluster)
2. **Unsealing:** Controller decrypts using its private key (never leaves cluster)
3. **Result:** A normal K8s Secret is created (apps see plaintext)

### Deployment

```bash
# Commit to Git
git add mysealedsecret.yaml
git commit -m "Add sealed secret"

# Deploy
kubectl apply -f mysealedsecret.yaml

# Controller automatically decrypts and creates the Secret
kubectl get secret mysecret
```

---

## 6. SOPS vs Sealed Secrets

| Aspect | SOPS | Sealed Secrets |
| :--- | :--- | :--- |
| **Encryption** | AWS/GCP/Azure KMS | K8s-native private key |
| **Portability** | High (cloud provider agnostic) | Low (tied to cluster) |
| **Ease of Use** | Medium (requires CLI) | High (kubectl-native) |
| **Disaster Recovery** | Easy (KMS key backup) | Hard (must backup cluster key) |
| **Multi-Environment** | Excellent (different KMS keys per env) | Difficult (each cluster is separate) |
| **Use Case** | Git workflow, CI/CD, multi-cluster | Single cluster, audit requirements |

---

## 7. Production Checklist

- [ ] Secrets encrypted before Git commit
- [ ] KMS keys rotated annually
- [ ] Access to decrypt logged and monitored
- [ ] Separate keys for dev/staging/prod
- [ ] Backup of encryption keys stored safely
- [ ] No plaintext secrets in Git history (use `git-secrets` to prevent)

---

Topic 8.7:
Title: Container Security
Order: 7

Class 8.7.1:
	Title: Image Scanning and Supply Chain Security
	Description: Trivy, Clair, Snyk and vulnerability management.
Content Type: text
Duration: 500 
Order: 1
		Text Content :
 # Container Image Scanning: Your First Defense

Container images are your supply chain. If you deploy a vulnerable image, you are shipping a backdoor to production.

---

## 1. Trivy: The Fast Scanner

Trivy is the industry standard for **fast, accurate vulnerability scanning**. It scans:
- **OS packages** (apt, yum, apk)
- **Application dependencies** (pip, npm, maven, go mod)
- **Container configuration** (Dockerfile misconfigurations)
- **IaC** (Terraform, CloudFormation, K8s manifests)

### Installation & Basic Usage

```bash
# Install Trivy
brew install trivy

# Scan a local image
trivy image myapp:latest

# Scan a running container
trivy image --input /path/to/image.tar

# Output example:
# myapp:latest (debian 11.6)
# Found 12 vulnerabilities
# 
# CRITICAL: CVE-2024-1234 in openssl
# Library: openssl (1.1.1)
# Severity: CRITICAL
# Fix Version: 1.1.1w
```

### Scanning in CI/CD

```bash
# Fail build if critical CVEs found
trivy image \
  --severity CRITICAL,HIGH \
  --exit-code 1 \
  myapp:$GITHUB_SHA

# Generate SBOM (Software Bill of Materials)
trivy image \
  --format cyclonedx \
  --output sbom.json \
  myapp:latest
```

### Trivy Configuration

```yaml
# .trivy.yaml
severity:
  - CRITICAL
  - HIGH

scanners:
  - vuln
  - config
  - secret

skip-dirs:
  - tests
  - vendor

ignorefile: .trivyignore
```

### Ignoring False Positives

```
# .trivyignore
# Format: CVE-YYYY-XXXX expires YYYY-MM-DD

CVE-2024-1234
CVE-2024-5678 expires 2024-12-31  # Temporary ignore, expires in 1 year
```

---

## 2. Clair: The Static Analysis Engine

Clair is a vulnerability scanner that runs as a service. It powers Docker Hub, Quay.io, and harbor.

### Architecture

```
┌─────────────────────────┐
│  Image Repository       │
│  (Docker Hub, Quay)     │
└──────────────┬──────────┘
               │
        (Webhook)
               │
         ┌─────▼──────┐
         │  Clair API │
         └─────┬──────┘
               │
         ┌─────▼──────────┐
         │ Vulnerability  │
         │ DB (Updated    │
         │ hourly)        │
         └────────────────┘
```

### Clair vs Trivy

| Aspect | Trivy | Clair |
| :--- | :--- | :--- |
| **Deployment** | CLI tool | Service/API |
| **Speed** | Very fast (offline) | Slower (needs DB lookups) |
| **Accuracy** | High | Very high (curated DB) |
| **Integration** | CI/CD pipelines | Container registries |
| **Database** | Built-in (updates automatically) | PostgreSQL backend |
| **Real-time Scanning** | No | Yes (continuous monitoring) |

**When to Use:**
- Trivy: In CI/CD, local dev, quick scans
- Clair: Registry integration, continuous monitoring

---

## 3. Snyk: The Developer-First Scanner

Snyk focuses on **developer experience** and **dependency vulnerability management**.

### Key Features

* Scans dependencies (npm, pip, maven, etc.)
* Provides remediation advice (upgrade specific packages)
* Monitors Git repos continuously
* Provides fix PRs automatically

### Snyk in a GitHub Workflow

```bash
# Install Snyk
npm install -g snyk

# Authenticate
snyk auth

# Test your project
snyk test
# Output: Found 15 vulnerabilities in dependencies
#   - Express 4.17.1 has ReDOS vulnerability
#   - Mongoose 5.1.0 has prototype pollution
#   Recommendations:
#   - Upgrade express to 4.18.0
#   - Upgrade mongoose to 5.13.0

# Fix automatically (where possible)
snyk fix

# Monitor continuously (syncs with GitHub)
snyk monitor
```

### Snyk in CI/CD

```yaml
# .github/workflows/snyk.yml
name: Snyk Test
on: [push, pull_request]

jobs:
  snyk:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - uses: snyk/actions/setup@master
    - run: snyk test --severity-threshold=high
      env:
        SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
```

---

## 4. Policy Enforcement: Scanning Strategy

### Shift-Left: Scan Early

```yaml
# CI/CD Pipeline stages
Build:
  Lint & Test:
    - Run unit tests
  Scan for Vulnerabilities:
    - Trivy: Local dependency scan
    - npm audit: Direct dependencies
    - SAST: Source code analysis
  Build Image:
    - Docker build
  Scan Image:
    - Trivy image scan
    - FAIL if CRITICAL found
  Push to Registry:
    - Push only if scan passed
  
Deploy:
  Run Clair/Registry Scanner:
    - Continuous monitoring
    - Alert on new CVEs in deployed images
```

### Policy: Fix or Explain

**Rule 1: No deployment with CRITICAL vulnerabilities.**
```bash
trivy image --severity CRITICAL --exit-code 1 myapp:latest
# If this fails, deployment is blocked
```

**Rule 2: HIGH vulnerabilities require exception.**
```bash
# Document why the CVE is acceptable
# (e.g., "Only affects CLI mode, not server mode")
echo "CVE-2024-1234" >> .trivyignore-justification.md
```

**Rule 3: Alert on new vulnerabilities in production.**
```bash
# Weekly Clair scans of production images
# Alert Slack if new CVE found
```

---

## 5. SBOM (Software Bill of Materials)

An SBOM lists all components in your image.

```bash
# Generate SBOM with Trivy
trivy image --format cyclonedx --output sbom.json myapp:latest

# Output (cyclonedx format):
{
  "bomFormat": "CycloneDX",
  "specVersion": "1.4",
  "components": [
    {
      "type": "library",
      "name": "openssl",
      "version": "1.1.1",
      "purl": "pkg:deb/debian/openssl@1.1.1?arch=amd64&distro=debian-11.6"
    },
    {
      "type": "library",
      "name": "python",
      "version": "3.9.2"
    }
  ]
}
```

**Why SBOM Matters:**
- **Compliance:** SOC 2, PCI-DSS require component tracking
- **Response:** If CVE announced, quickly check if SBOM is affected
- **Transparency:** Know exactly what's in your images

---

## 6. Signature Verification (Cosign)

Ensure images are built by trusted parties.

```bash
# Install Cosign
brew install sigstore/tap/cosign

# Sign an image
cosign sign --key cosign.key myapp:latest

# Verify signature before deployment
cosign verify --key cosign.pub myapp:latest

# In Kubernetes (enforce with policy)
# Only allow images signed by trusted keys
```

---

## Production Scanning Strategy

```
┌──────────────┐
│ Developer    │
│ Commits Code │
└──────┬───────┘
       │
┌──────▼──────────────────────────────┐
│ Trivy Scan (npm/pip dependencies)   │
│ SAST Scan (code vulnerabilities)    │
│ License Check                       │
└──────┬───────────────────────────────┘
       │ FAIL if CRITICAL/HIGH + no exception
       │
┌──────▼──────────────────────────────┐
│ Build Container Image               │
└──────┬───────────────────────────────┘
       │
┌──────▼──────────────────────────────┐
│ Trivy Scan Image                    │
│ Check for OS vulns, config issues   │
└──────┬───────────────────────────────┘
       │ FAIL if CRITICAL
       │
┌──────▼──────────────────────────────┐
│ Sign Image (Cosign)                 │
│ Generate SBOM                       │
└──────┬───────────────────────────────┘
       │
┌──────▼──────────────────────────────┐
│ Push to Registry                    │
│ Registry enables Clair scanning     │
└──────┬───────────────────────────────┘
       │
┌──────▼──────────────────────────────┐
│ Deployment                          │
│ Verify signature + policy admission │
└──────────────────────────────────────┘
```

---

Class 8.7.2:
	Title: Runtime Security (Falco)
	Description: Detecting malicious behavior in containers.
Content Type: text
Duration: 400 
Order: 2
		Text Content :
 # Runtime Security: Detecting the Breach

Image scanning is **preventive**. Falco is **detective**. It monitors what containers are actually doing and alerts on suspicious behavior.

---

## 1. Falco: The Eyes in the Cluster

Falco is a **runtime security tool** that monitors system calls inside containers and alerts on policy violations.

### Installation in Kubernetes

```bash
# Using Helm
helm repo add falcosecurity https://falcosecurity.github.io/charts
helm install falco falcosecurity/falco \
  --namespace falco \
  --create-namespace \
  --set ebpf.enabled=true  # Use eBPF for performance

# Falco runs as a DaemonSet on every node
kubectl get pods -n falco
# falco-s4jkl (node1)
# falco-m8pwq (node2)
# falco-z2nqr (node3)
```

### Example Rules (Detecting Suspicious Activity)

```yaml
# /etc/falco/rules.d/custom-rules.yaml

- rule: Unauthorized Shell in Container
  desc: A shell was spawned in a container (possible compromise)
  condition: >
    spawned_process
    and container
    and proc.name in (bash, sh)
  output: >
    Unauthorized shell
    (user=%user.name command=%proc.cmdline container_id=%container.id)
  priority: WARNING
  tags: [shell, container]

- rule: Write to System Binaries
  desc: Attempt to modify system binaries (malware installation?)
  condition: >
    open
    and container
    and fd.name startswith /bin/
    and write
  output: >
    File write to binary
    (user=%user.name file=%fd.name container_id=%container.id)
  priority: CRITICAL
  tags: [malware, persistence]

- rule: Suspicious Network Connection
  desc: Container connecting to rare external IP
  condition: >
    outbound
    and container
    and not fd.sip in (10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16)
  output: >
    External network connection
    (src=%fd.sip dst=%fd.dip port=%fd.dport container=%container.name)
  priority: WARNING
  tags: [network, exfil]
```

### Alert Output (What Falco Sees)

```
2024-01-15T14:23:45.123Z WARNING Unauthorized shell in Container
  user=root
  command=/bin/bash
  container_id=a1b2c3d4e5f6
  container_name=redis-prod

2024-01-15T14:25:12.456Z CRITICAL Write to System Binaries
  user=nobody
  file=/usr/bin/curl
  container_id=xyz789
  container_name=app-staging
```

---

## 2. Syscall Monitoring (The Foundation)

Falco uses eBPF (extended Berkeley Packet Filter) to monitor all system calls.

### What's a System Call?

```c
// Application code
FILE *f = fopen("/etc/passwd", "r");

// Kernel gets invoked
syscall: open("/etc/passwd", O_RDONLY)
// Falco logs this
```

### Key Syscalls Falco Watches

| Syscall | Significance | Risk |
| :--- | :--- | :--- |
| `execve` | Process creation | High (shellcode execution) |
| `open` | File access | Medium (data exfiltration) |
| `connect` | Network connection | High (C&C communication) |
| `clone` | Process fork | Medium (resource exhaustion) |
| `mmap` | Memory mapping | Medium (code injection) |
| `load_kernel_module` | Kernel module load | Critical (rootkit) |

---

## 3. Falco Rules (The Policies)

Rules are YAML that define conditions and actions.

```yaml
- rule: Read Sensitive File
  desc: Detect reads of sensitive files
  condition: >
    read
    and fd.name in (/etc/shadow, /etc/passwd, /root/.ssh/id_rsa)
  output: >
    Sensitive file read
    (user=%user.name file=%fd.name command=%proc.name)
  priority: CRITICAL
  tags: [sensitive_data]
```

### Rule Anatomy

```
condition: >
  [SYSCALL_TYPE] [OPERATOR] [VALUE]
  and container
  and not exception
```

**Operators:**
- `=` Equal
- `!=` Not equal
- `startswith` String starts with
- `contains` String contains
- `in` Value in list
- `not` Logical NOT
- `and` Logical AND
- `or` Logical OR

---

## 4. Integration with Alert Systems

### Send Alerts to Slack

```yaml
# Falco Alerts → Slack
json:
  output_format: json

listeners:
  - name: gRPC
    address: "0.0.0.0:5060"

outputs:
  - name: slack_alert
    type: webhook
    url: "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"
    filter:
      - priority: ["WARNING", "CRITICAL"]
```

### Send to Datadog

```yaml
outputs:
  - name: datadog
    type: http
    url: "https://http-intake.logs.datadoghq.com/v1/input"
    config:
      api_key: "YOUR_DATADOG_API_KEY"
```

---

## 5. Tuning Falco (Reduce False Positives)

Falco can be noisy. You must tune rules to your environment.

```yaml
# Suppress expected activity
- macro: safe_bash_commands
  condition: >
    proc.cmdline in (
      /bin/bash -c "echo test",
      /bin/bash /scripts/startup.sh
    )

- rule: Unauthorized Shell (Tuned)
  desc: Shell spawned in container (excluding safe commands)
  condition: >
    spawned_process
    and container
    and proc.name = bash
    and not safe_bash_commands
  output: Suspicious shell
  priority: WARNING
```

---

## 6. Production Falco Deployment

```yaml
# DaemonSet with alerts
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: falco
spec:
  template:
    spec:
      hostNetwork: true
      hostPID: true
      containers:
      - name: falco
        image: falcosecurity/falco:latest
        securityContext:
          privileged: true  # eBPF requires elevated privileges
        volumeMounts:
        - name: docker
          mountPath: /var/run/docker.sock
      volumes:
      - name: docker
        hostPath:
          path: /var/run/docker.sock
```

---

## 7. Response to Falco Alerts

**Alert: Unauthorized shell in production container**

```
Action Plan:
1. Immediately isolate the pod (kubectl delete pod)
2. Preserve logs (kubectl logs for evidence)
3. Trigger incident response
4. Investigate: How did shell get there?
   - Compromised image?
   - Privilege escalation?
5. Quarantine and analyze container image
6. Notify security team
```

---

## Key Takeaway

Falco answers: **"What is currently happening inside my containers?"**

Combined with image scanning:
- **Image Scanning:** Prevents known vulnerabilities
- **Falco:** Detects actual attacks

Together they form a **defense-in-depth** security posture.

---

Topic 8.8:
Title: Kubernetes Security
Order: 8

Class 8.8.1:
	Title: Pod Security Standards and seccomp
	Description: PSS, seccomp, and AppArmor.
Content Type: text
Duration: 500 
Order: 1
		Text Content :
 # Kubernetes Security: Locking Down the Cluster

## 1. Pod Security Standards (PSS) - The Framework

Pod Security Standards define **how privileged a Pod is allowed to be**. They replace the deprecated Pod Security Policy (PSP).

### Three Profiles

#### Privileged (Least Restrictive)

Allows all capabilities. Used for system/infrastructure pods.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: privileged-pod
spec:
  containers:
  - name: app
    image: myapp:latest
    securityContext:
      privileged: true        # Root-like access to host
      allowPrivilegeEscalation: true
      capabilities:
        add:
        - SYS_ADMIN
        - NET_ADMIN
```

**Use Case:** CNI plugins, kubelet, storage drivers. (NOT application containers)

#### Baseline (Restricted)

Prevents privilege escalation and dangerous capabilities.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: baseline-pod
spec:
  containers:
  - name: app
    image: myapp:latest
    securityContext:
      runAsNonRoot: true
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
```

**Prevents:**
- Running as root
- Adding capabilities
- Writing to root filesystem

**Allows:**
- Reading files
- Network access
- Some filesystem writes

#### Restricted (Most Secure)

Enforces strong isolation.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: restricted-pod
spec:
  containers:
  - name: app
    image: distroless-app:latest
    securityContext:
      runAsNonRoot: true
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
      seccompProfile:
        type: RuntimeDefault
    volumeMounts:
    - name: tmp
      mountPath: /tmp
  volumes:
  - name: tmp
    emptyDir: {}
```

**Enforces:**
- Non-root user
- No privilege escalation
- Read-only root filesystem
- seccomp
- SELinux
- Resource limits

---

## 2. Pod Security Admission (PSA) - Enforcement

PSA is a controller that enforces PSS at admission time.

### Enable PSA

```yaml
# In apiserver configuration
# /etc/kubernetes/manifests/kube-apiserver.yaml
spec:
  containers:
  - name: kube-apiserver
    command:
    - kube-apiserver
    - --enable-admission-plugins=PodSecurity
    - --admission-control-config-file=/etc/kubernetes/psa.yaml
```

### PSA Configuration

```yaml
# /etc/kubernetes/psa.yaml
apiVersion: apiserver.config.k8s.io/v1
kind: AdmissionConfiguration
plugins:
- name: PodSecurity
  configuration:
    apiVersion: pod-security.admission.config.k8s.io/v1
    kind: PodSecurityAdmissionConfiguration
    defaults:
      enforce: "restricted"      # Enforce restricted
      audit: "restricted"        # Log violations
      warn: "restricted"         # Warn on violations
    exemptions:
      namespaces:
      - kube-system              # System namespaces exempt
      - kube-public
      runtimeClasses:
      - gvisor                   # Special runtimes exempt
```

### Per-Namespace Enforcement

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: myapp
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
```

---

## 3. Seccomp (Secure Computing)

Seccomp restricts which system calls a container can make.

### Default Seccomp Profile

```json
{
  "defaultAction": "SCMP_ACT_ERRNO",
  "defaultErrnoRet": 1,
  "archMap": [
    {
      "architecture": "SCMP_ARCH_X86_64",
      "subArchitectures": ["SCMP_ARCH_X86", "SCMP_ARCH_X32"]
    }
  ],
  "syscalls": [
    {
      "names": ["read", "write", "open", "close", "stat", ...],
      "action": "SCMP_ACT_ALLOW"
    },
    {
      "names": ["load_kernel_module", "create_module"],
      "action": "SCMP_ACT_ERRNO"  # Deny
    }
  ]
}
```

**What Default Seccomp Blocks:**
- Loading kernel modules
- Raw socket creation (network spoofing)
- ptrace (process debugging)
- Administrative operations

### Custom Seccomp Profile

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: secure-pod
spec:
  securityContext:
    seccompProfile:
      type: Localhost
      localhostProfile: my-profile.json
  containers:
  - name: app
    image: myapp:latest
```

### Why Seccomp Matters

**Scenario:** A vulnerability in your app allows arbitrary code execution. The attacker can:

**Without seccomp:**
- Load a kernel module (rootkit)
- Modify network packets (MITM attacks)
- Attach debugger to other processes

**With seccomp:**
- None of the above. System call is denied.
- Attack surface dramatically reduced.

---

## 4. AppArmor Profiles

AppArmor is a Linux Mandatory Access Control (MAC) system.

### Create an AppArmor Profile

```
#include <tunables/global>
profile myapp-profile flags=(attach_disconnected) {
  #include <abstractions/base>
  #include <abstractions/nameservice>

  # Allow read-only access to config
  /etc/myapp/config.yaml r,

  # Allow writing to logs
  /var/log/myapp/*.log w,

  # Deny write to root filesystem
  deny /root/** w,

  # Deny executing shells
  deny /bin/bash x,
  deny /bin/sh x,

  # Allow network
  network inet stream,
  network inet dgram,
}
```

### Load Profile

```bash
# Load into AppArmor
sudo apparmor_parser -r /etc/apparmor.d/myapp-profile

# Verify
sudo aa-status | grep myapp-profile
```

### Use in Kubernetes

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app-with-apparmor
  annotations:
    container.apparmor.security.beta.kubernetes.io/app: localhost/myapp-profile
spec:
  containers:
  - name: app
    image: myapp:latest
```

### AppArmor vs seccomp

| Aspect | seccomp | AppArmor |
| :--- | :--- | :--- |
| **Level** | Syscall | File/Network |
| **Granularity** | Very fine | Coarse |
| **Performance** | Minimal overhead | Moderate |
| **Setup** | Complex | Medium |
| **Coverage** | All containers | Only with Linux |

---

## 5. Migration from Deprecated PSP

```yaml
# Old (Deprecated) Pod Security Policy
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: restricted
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
  - ALL
  volumes:
  - 'configMap'
  - 'emptyDir'
  - 'projected'
  - 'secret'
  - 'downwardAPI'
  - 'persistentVolumeClaim'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'MustRunAs'
  supplementalGroups:
    rule: 'MustRunAs'
  fsGroup:
    rule: 'MustRunAs'
  readOnlyRootFilesystem: false

---

# New (Recommended) Pod Security Standards + Admission
apiVersion: v1
kind: Namespace
metadata:
  name: myapp
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
```

---

Class 8.8.2:
	Title: OPA/Kyverno Policy as Code
	Description: Kubernetes admission control and policy enforcement.
Content Type: text
Duration: 450 
Order: 2
		Text Content :
 # Policy as Code: Automating Security Decisions

Wouldn't it be amazing if security violations were **impossible** instead of just documented?

That's what policy-as-code tools do. They enforce rules at admission time—before the resource ever enters the cluster.

---

## 1. OPA/Gatekeeper (The Industry Standard)

Open Policy Agent (OPA) is a policy engine that can enforce arbitrary rules.

### Installation

```bash
# Install Gatekeeper (OPA for Kubernetes)
kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/master/deploy/gatekeeper.yaml

# Verify
kubectl get pods -n gatekeeper-system
```

### Rego Policy Language

Policies are written in Rego, a logic programming language.

```rego
package kubernetes.admission

import future.keywords.contains
import future.keywords.if

# Deny deployments without resource limits
deny[msg] {
    input.request.kind.kind == "Deployment"
    container := input.request.object.spec.template.spec.containers[_]
    not container.resources.limits
    msg := sprintf("Container %v must have resource limits", [container.name])
}

# Deny images from untrusted registries
deny[msg] {
    input.request.kind.kind == "Pod"
    container := input.request.object.spec.containers[_]
    not startswith(container.image, "gcr.io/") 
    not startswith(container.image, "quay.io/")
    msg := sprintf("Image %v must be from trusted registry", [container.image])
}

# Deny pods running as root
deny[msg] {
    input.request.kind.kind == "Pod"
    container := input.request.object.spec.containers[_]
    not container.securityContext.runAsNonRoot == true
    msg := sprintf("Container %v must run as non-root", [container.name])
}

# Deny LoadBalancer services in dev namespace
deny[msg] {
    input.request.kind.kind == "Service"
    input.request.namespace == "dev"
    input.request.object.spec.type == "LoadBalancer"
    msg := "LoadBalancer services not allowed in dev namespace"
}
```

### Deploying OPA Rules

```yaml
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: require-labels
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
  parameters:
    labels: ["app", "version", "owner"]
---
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8srequiredlabels
spec:
  crd:
    spec:
      names:
        kind: K8sRequiredLabels
      validation:
        openAPIV3Schema:
          properties:
            labels:
              type: array
              items:
                type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package kubernetes.admission
        deny[msg] {
            input.request.kind.kind == "Pod"
            not input.request.object.metadata.labels
            msg := "Pod must have labels"
        }
```

### Testing OPA Policies

```bash
# Test locally before deploying
opa eval -d policy.rego 'data.kubernetes.admission.deny'

# Unit test
opa test policy_test.rego -v
```

---

## 2. Kyverno (Kubernetes-Native Alternative)

Kyverno is simpler than OPA. Policies are YAML, not Rego.

### Installation

```bash
helm repo add kyverno https://kyverno.github.io/kyverno/
helm install kyverno kyverno/kyverno --namespace kyverno --create-namespace
```

### Kyverno Policy Example

```yaml
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: require-requests-limits
spec:
  validationFailureAction: enforce  # Block violations
  rules:
  - name: check-container-resources
    match:
      resources:
        kinds:
        - Pod
    validate:
      message: "CPU and memory limits required"
      pattern:
        spec:
          containers:
          - resources:
              limits:
                memory: "?*"
                cpu: "?*"
---
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: disallow-privileged
spec:
  validationFailureAction: enforce
  rules:
  - name: no-privileged
    match:
      resources:
        kinds:
        - Pod
    validate:
      message: "Privileged pods not allowed"
      pattern:
        spec:
          containers:
          - securityContext:
              privileged: false
```

### Kyverno for Image Verification

```yaml
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: verify-images-signed
spec:
  validationFailureAction: enforce
  rules:
  - name: check-signature
    match:
      resources:
        kinds:
        - Pod
    verifyImages:
    - imageReferences:
      - "gcr.io/myproject/*"
      attestors:
      - name: check-cosign
        entries:
        - keys:
            publicKeys: |
              -----BEGIN PUBLIC KEY-----
              MFkwEwYHKoZIzj0CAQYIKoZIzj...
              -----END PUBLIC KEY-----
```

---

## 3. OPA vs Kyverno

| Aspect | OPA/Gatekeeper | Kyverno |
| :--- | :--- | :--- |
| **Language** | Rego | YAML |
| **Learning Curve** | Steep | Gentle |
| **Flexibility** | Extremely flexible | Good for common rules |
| **Performance** | Slower | Faster |
| **Ecosystem** | Broader (works outside K8s) | K8s-specific |
| **Policy Reuse** | Libraries, modularity | Templates |

---

## 4. Production Policy Examples

### Example 1: Enforce Security Best Practices

```yaml
# Prevent sensitive data in environment variables
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: no-sensitive-env-vars
spec:
  validationFailureAction: audit  # Start with audit
  rules:
  - name: check-env-vars
    match:
      resources:
        kinds:
        - Pod
    validate:
      message: "Secrets in env vars. Use secret volume mounts instead."
      pattern:
        spec:
          containers:
          - env:
            - name: "?*PASSWORD*|*SECRET*|*KEY*"
              value: "?*"
```

### Example 2: Multi-Tenancy Isolation

```yaml
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: enforce-namespace-isolation
spec:
  validationFailureAction: enforce
  rules:
  - name: deny-cross-namespace-traffic
    match:
      resources:
        kinds:
        - NetworkPolicy
    validate:
      message: "NetworkPolicy must isolate to same namespace"
      pattern:
        spec:
          podSelector: {}
          policyTypes:
          - Ingress
          ingress:
          - from:
            - podSelector:
                matchLabels: {}
            - namespaceSelector:
                matchLabels:
                  name: ?*  # Must select specific namespace
```

### Example 3: Cost Control (Prod vs Dev)

```yaml
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: resource-limits-by-env
spec:
  validationFailureAction: enforce
  rules:
  - name: enforce-prod-limits
    match:
      resources:
        kinds:
        - Pod
        selector:
          matchLabels:
            env: production
    validate:
      message: "Production pods must have high resource limits"
      pattern:
        spec:
          containers:
          - resources:
              limits:
                cpu: "2"
                memory: "4Gi"
  - name: allow-dev-flexibility
    match:
      resources:
        kinds:
        - Pod
        selector:
          matchLabels:
            env: development
    validate:
      message: "Dev pods must have some limits"
      pattern:
        spec:
          containers:
          - resources:
              limits:
                cpu: "500m"
                memory: "512Mi"
```

---

## 5. Remediation with Kyverno

Kyverno can not just validate—it can automatically fix issues.

```yaml
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: add-network-policy
spec:
  validationFailureAction: audit
  background: true  # Apply to existing resources
  rules:
  - name: add-default-deny-ingress
    match:
      resources:
        kinds:
        - Namespace
    mutate:
      patchStrategicMerge:
        metadata:
          labels:
            network-policy: enabled
```

---

## 6. Audit and Monitoring

```bash
# Check policy violations
kubectl get policyreport -A

# See violations per namespace
kubectl get policyreport -n myapp -o wide

# Detailed violation logs
kubectl logs -n kyverno -l app=kyverno-admission-controller -f
```

---

Topic 8.9:
Title: Compliance Automation
Order: 9

Class 8.9.1:
	Title: Compliance Frameworks and Cloud Custodian
	Description: SOC 2, HIPAA, PCI-DSS automation and remediation.
Content Type: text
Duration: 500 
Order: 1
		Text Content :
 # Compliance Automation: Security at Scale

Compliance is not a one-time audit. It's a continuous practice. Cloud Custodian and OPA automate compliance checks and remediation.

---

## 1. Compliance Frameworks Overview

### SOC 2 (Service Organization Control 2)

**Target:** SaaS companies storing customer data

**Key Controls:**
- Access logging (who accessed what)
- Encryption in transit and at rest
- Incident response procedures
- Change management process

**Cloud Custodian Policy (Example):**

```yaml
policies:
  - name: ensure-s3-encryption
    resource: s3
    filters:
      - ServerSideEncryptionConfiguration: absent
    actions:
      - type: encrypt-s3
        key-id: arn:aws:kms:us-east-1:123456789012:key/12345678
```

---

### HIPAA (Health Insurance Portability and Accountability Act)

**Target:** Healthcare providers handling patient data (Protected Health Information - PHI)

**Key Requirements:**
- Encryption of PHI at rest and in transit
- Network isolation (no direct internet access)
- Access logs (audit trail)
- Data residency (data must stay in US)

**Cloud Custodian Policy:**

```yaml
policies:
  - name: hipaa-ensure-rds-encryption
    resource: rds
    filters:
      - StorageEncrypted: false
    actions:
      - type: terminate-instance  # Nuclear option for violations
        force: true

  - name: hipaa-ensure-no-public-rds
    resource: rds
    filters:
      - PubliclyAccessible: true
    actions:
      - type: modify-db-instance
        PubliclyAccessible: false
```

---

### PCI-DSS (Payment Card Industry Data Security Standard)

**Target:** Anyone handling credit card data

**Key Rules:**
- **Never** store CVV/CVC (card verification code)
- Isolated network for payment processing
- Encryption in transit (TLS 1.2+)
- Access restricted to card data
- Annual penetration testing

**Cloud Custodian Policy:**

```yaml
policies:
  - name: pci-isolate-payment-environment
    resource: security-group
    filters:
      - type: ingress
        IpProtocol: -1
        CidrIp: 0.0.0.0/0
    actions:
      - type: modify-ingress
        rule_type: egress
        group-id: sg-payment-restricted
        CidrIp: 10.0.1.0/24  # Only internal traffic

  - name: pci-require-tls-1-2
    resource: elb
    filters:
      - type: listener
        PolicyNames: [non-tls-policy]
    actions:
      - type: modify-listener
        PolicyNames: [ELBSecurityPolicy-TLS-1-2-2017-01]
```

---

## 2. Cloud Custodian (Compliance Automation Engine)

Cloud Custodian is a policy-as-code tool for AWS, Azure, and GCP. It enforces and remediates compliance violations automatically.

### Installation

```bash
# Install Cloud Custodian
pip install c7n

# Verify
custodian --version
```

### Policy Structure

```yaml
# compliance-policy.yaml
policies:
  # Policy 1: Find non-compliant resources
  - name: find-unencrypted-volumes
    description: Alert on EBS volumes without encryption
    resource: ebs
    filters:
      - Encrypted: false
    actions:
      - type: notify
        template: default.html
        transport:
          type: sqs
          queue: https://sqs.us-east-1.amazonaws.com/123456789012/alerts

  # Policy 2: Remediate automatically
  - name: auto-encrypt-volumes
    description: Automatically encrypt unencrypted EBS volumes
    resource: ebs
    filters:
      - Encrypted: false
      - VolumeSize: lt 1000  # Only small volumes (safer)
    actions:
      - type: copy-instance-snapshot
        encrypted: true
        copy-to-region: us-east-1
```

### Running Custodian

```bash
# Dry-run (see what would happen, don't change anything)
custodian run -s output compliance-policy.yaml --dryrun

# Actually execute
custodian run -s output compliance-policy.yaml

# Get results
custodian report --format csv output/find-unencrypted-volumes.json > report.csv
```

---

## 3. Common Cloud Custodian Policies

### Enforce IAM Best Practices

```yaml
policies:
  - name: disable-root-account-access
    resource: iam-user
    filters:
      - type: access-key
        key-state: Active
    actions:
      - type: remove-keys
        delete: true

  - name: enforce-mfa-on-console-users
    resource: iam-user
    filters:
      - type: login-profile
      - type: mfa-device
        mfa-device: false
    actions:
      - type: notify
        template: mfa-required.html
        transport:
          type: email
          to: security@company.com
```

### S3 Security

```yaml
policies:
  - name: block-public-s3-buckets
    resource: s3
    filters:
      - type: bucket-access-control
        access-control: PublicRead
    actions:
      - type: set-bucket-acl
        acl: private

  - name: enable-bucket-logging
    resource: s3
    filters:
      - type: logging
        key: null  # Logging not enabled
    actions:
      - type: enable-logging
        target-bucket: central-logs
        target-prefix: s3-logs/
```

### EC2 Cost Optimization

```yaml
policies:
  - name: stop-idle-instances
    resource: ec2
    filters:
      - type: instance-uptime-days
        days: 30
      - type: cpu-utilization
        percent: 5
        days: 7
    actions:
      - type: notify
        template: idle-instance.html
        transport:
          type: sns
          topic: arn:aws:sns:us-east-1:123456789012:alerts
      - type: stop

  - name: delete-unattached-volumes
    resource: ebs
    filters:
      - type: volume-attachment-count
        count: 0
      - type: age-value
        days: 7
        op: greater-than
    actions:
      - type: delete
```

---

## 4. Policy Enforcement Patterns

### Pattern 1: Alert Only

```yaml
actions:
  - type: notify
    template: default.html
    transport:
      type: sns
      topic: arn:aws:sns:us-east-1:123456789012:compliance-alerts
```

**Use Case:** New policies in testing phase

---

### Pattern 2: Auto-Remediate

```yaml
actions:
  - type: modify-db-instance
    PubliclyAccessible: false
    ApplyImmediately: true
```

**Use Case:** Clear violations with low risk (e.g., disabling public access)

---

### Pattern 3: Terminate for Severe Violations

```yaml
actions:
  - type: terminate
    force: true
```

**Use Case:** Instances in restricted security groups, root account in use, etc.

---

## 5. Compliance Reporting

Cloud Custodian generates reports for auditors.

```bash
# Generate CSV report
custodian report --format csv output/*.json > compliance-report.csv

# Generate JSON for integration with SIEM
custodian report --format json output/*.json > compliance-report.json

# Alert to Slack
custodian run -s output policy.yaml && \
  curl -X POST https://hooks.slack.com/services/... \
    -d @output/summary.json
```

---

## 6. Policy as Code Workflow

```
┌──────────────────────┐
│ Write Policy YAML    │
│ (Compliance team)    │
└──────────┬───────────┘
           │
┌──────────▼───────────┐
│ Test on Dev Account  │
│ (--dryrun mode)      │
└──────────┬───────────┘
           │
┌──────────▼───────────┐
│ Code Review          │
│ (GitHub PR)          │
└──────────┬───────────┘
           │
┌──────────▼───────────┐
│ Merge to Main        │
│ Auto-run in Prod     │
└──────────┬───────────┘
           │
┌──────────▼───────────┐
│ Generate Reports     │
│ Audit Trail          │
└──────────────────────┘
```

---

## 7. Multi-Cloud Compliance

Cloud Custodian supports AWS, Azure, and GCP with unified policies.

```yaml
policies:
  # AWS Policy
  - name: ensure-s3-encryption-aws
    resource: aws.s3
    filters:
      - ServerSideEncryptionConfiguration: absent

  # Azure Policy
  - name: ensure-blob-encryption-azure
    resource: azure.blob-container
    filters:
      - type: value
        key: properties.encryption
        value: null
        value_type: absent

  # GCP Policy
  - name: ensure-gcs-encryption-gcp
    resource: gcp.gcs
    filters:
      - type: value
        key: encryption
        value: null
        value_type: absent
```

---

## Production Checklist

- [ ] Policies in Git (version controlled)
- [ ] Dry-run on every deployment
- [ ] Reports sent to security team weekly
- [ ] Alerts configured for each policy
- [ ] Exceptions documented and reviewed
- [ ] No hardcoded AWS credentials in policies
- [ ] Separate policies for dev/staging/prod
- [ ] Regular policy audit (quarterly)

---

Module 9:
Title: Scripting & Programming for DevOps
Description: Master automation through scripting. Deep dive into Bash and Python for infrastructure automation, API integration, and operational tooling.
Order: 9
Learning Outcomes:
Master advanced Bash scripting
Use Python for infrastructure automation
Integrate with cloud APIs
Build operational tools

Topic 9.1:
Title: Bash Scripting
Order: 1

Class 9.1.1:
	Title: Advanced Bash Techniques
	Description: Functions, arrays, and error handling.
Content Type: text
Duration: 450 
Order: 1
		Text Content :

# Bash: The Duct Tape of DevOps

## 1. Beyond One-Liners
Anyone can write `mkdir test`. Senior DevOps engineers write **maintainable, safe, reusable Bash**.
That means:
* Functions instead of copy-paste
* Explicit error handling
* Predictable behavior in failure scenarios

Treat Bash as a programming language, not a shell toy.

---

## 2. Functions & Scope
Use functions to enforce structure and reuse.

* **Scope Rule:** Bash variables are **global by default**.
  * A variable modified inside a function affects the entire script.
  * This is a common source of subtle bugs.

* **Best Practice:** Always declare function variables as `local`.

```bash
my_function() {
    local my_var="value"
    echo "$my_var"
}
```


### Return Values

* Bash functions return **exit codes**, not data.
* Exit codes range from `0–255`.

  * `0` → success
  * non-zero → failure

**To return data:**

```bash
result=$(my_function)
```

---

## 3. Arrays (Indexed & Associative)

### Indexed Arrays

Ordered lists.

```bash
servers=("web01" "web02" "web03")
echo "${servers[0]}"
```

### Associative Arrays (Dictionaries)

Key-value mapping (Bash 4+).
Used heavily in real-world scripts (region → AMI, env → config).

```bash
declare -A regions
regions[us-east-1]="ami-12345"
regions[eu-west-1]="ami-67890"
```

**Iterating over keys (Interview Favorite):**

```bash
for key in "${!regions[@]}"; do
    echo "$key -> ${regions[$key]}"
done
```

---

## 4. Error Handling (Strict Mode)

By default, Bash **ignores failures** and keeps running.
This is unacceptable in CI/CD and infra automation.

### Mandatory Safety Header

Put this at the top of every serious script:

```bash
set -euo pipefail
```

* `set -e`
  Exit immediately if any command fails.

* `set -u`
  Fail on undefined variables (prevents destructive commands).

* `set -o pipefail`
  Fail pipelines if **any** command inside fails.

**Example Bug Without `pipefail`:**

```bash
false | true   # Script continues unless pipefail is set
```

---

## 5. Argument Parsing (`getopts`)

Never manually parse arguments using `$1`, `$2`.

### Why `getopts`

* Standard flag parsing (`-f file`, `-v`)
* Supports combined flags (`-vf`)
* Cleaner, safer, predictable behavior

```bash
while getopts ":f:v" opt; do
  case $opt in
    f) file="$OPTARG" ;;
    v) verbose=true ;;
    *) echo "Invalid option" ;;
  esac
done
```

---

## Bash Concepts Summary

| Concept             | Purpose                        | Production Importance |
| ------------------- | ------------------------------ | --------------------- |
| Functions           | Reuse & structure              | High                  |
| `local` variables   | Prevent side effects           | Critical              |
| Exit codes          | Control flow & error detection | Critical              |
| Indexed arrays      | Ordered data                   | Medium                |
| Associative arrays  | Key-value mapping              | High                  |
| `set -euo pipefail` | Script safety                  | Mandatory             |
| `getopts`           | Argument parsing               | High                  |

---

### Interview Reality Check

If you cannot explain **why** `set -euo pipefail` exists,
you are not ready to write production Bash.



---

Class 9.1.2:
	Title: Shell Scripting Best Practices
	Description: Linting, portability, and testing.
Content Type: text
Duration: 400 
Order: 2
		Text Content :
 # Professional Shell Scripting

## 1. ShellCheck (Linting)
Never rely on your eyes to catch bugs.
* **ShellCheck:** A static analysis tool that finds common bugs and security issues.
    * *Example:* It will warn you if you forget to quote a variable: `rm $file` (Dangerous if the filename has spaces) vs `rm "$file"` (Safe).
* *Interview Tip:* Mentioning "I run ShellCheck in my CI pipeline" is a massive green flag for hiring managers.

---

## 2. Defensive Programming
Assume the user (or the environment) will try to break your script.
* **Input Validation:** Check arguments immediately at the start.
    ```bash
    if [[ -z "${1-}" ]]; then
        echo "Error: Missing argument."
        exit 1
    fi
    ```
* **Cleanup with `trap`:** If your script creates temporary files, they must be deleted even if the script crashes or the user hits `Ctrl+C`.
    ```bash
    temp_file=$(mktemp)
    trap 'rm -f "$temp_file"' EXIT
    # ... do work ...
    # File is automatically deleted when script exits
    ```

---

## 3. Portability (Bash vs. Sh)
* **`/bin/sh`:** The POSIX standard. Runs on everything (Alpine, Debian, Old Unix). Limited features (no arrays, no `[[ ]]`).
* **`/bin/bash`:** The modern standard. Rich features.
* *Best Practice:* If writing for a lightweight Docker container (Alpine), utilize `sh`. For general automation where size doesn't matter, explicit `bash` is safer and easier.

---

## 4. Testing (BATS)
Yes, you can unit test Bash scripts.
* **bats-core:** A testing framework for Bash.
* *Scenario:* You write a test that runs your script with specific flags and asserts that the output matches a regex or that the exit code is 0.

---

Topic 9.2:
Title: Python for DevOps
Order: 2

Class 9.2.1:
	Title: Python Fundamentals for Automation
	Description: JSON parsing, Requests, and CLI tools.
Content Type: text
Duration: 450 
Order: 1
		Text Content :
 # Python: The Glue of the Cloud

## 1. Why Python over Bash?
Bash is great for *running commands*, but Python is superior for *processing data*.
* *Scenario:* "Parse this 1GB JSON log file and find the IP with the most errors."
    * **Bash:** Complex `sed`/`awk` chains. Hard to read, hard to debug.
    * **Python:** Standard `json` library. Readable, testable, and robust.

---

## 2. File I/O & Parsing
* **JSON/YAML:** The native language of Cloud Configs.
    * Use `yaml.safe_load()` (PyYAML) to read Kubernetes manifests safely.
    * Use `json.dump()` to generate valid config files programmatically.
* **CSV:** Python's `csv` module handles quoting and delimiters automatically, preventing common parsing errors.

---

## 3. Working with APIs (Requests)
DevOps is mostly talking to APIs (AWS, Slack, GitHub, Jira).
* **The `requests` Library:** The industry standard.
* *Error Handling:* Always wrap calls in `try/except`.
    ```python
    import requests
    try:
        response = requests.get(url)
        response.raise_for_status() # Raises error for 4xx/5xx codes automatically
    except requests.exceptions.RequestException as e:
        print(f"API Failed: {e}")
        sys.exit(1)
    ```

---

## 4. Building CLIs (`argparse` vs `click`)
Don't hardcode variables. Build tools your team can use.
* **`argparse`:** Built-in, robust, but verbose. Good for simple scripts.
* **`click`:** Third-party, creates beautiful CLIs with decorators. Preferred for building internal platform tools (IDPs).

---

## 5. Regular Expressions (`re`)
* Used for log parsing and validation.
* *Interview Q:* "How do you extract an IP address from a line of text?"
* *Answer:* `re.search(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}', line)`

---

Class 9.2.2:
	Title: Python Cloud SDKs
	Description: Automating AWS with Boto3.
Content Type: text
Duration: 450 
Order: 2
		Text Content :

# Cloud Automation with Python (Boto3)

## 1. Boto3 (AWS SDK)
Boto3 is the official **AWS SDK for Python**.  
It allows you to provision, configure, and operate AWS infrastructure programmatically instead of using the Console or CLI.

**Common Use Cases**
* Automate EC2 lifecycle (start/stop/terminate)
* Manage S3 objects and lifecycle
* Trigger infrastructure actions from CI/CD
* Build internal tooling and self-service platforms

---

## 2. Client vs. Resource (Critical Distinction)

### Client (Low-Level API)
* Direct, thin wrapper over AWS REST APIs
* Returns **raw dictionaries**
* Requires you to handle pagination, retries, and response parsing

**Use When**
* You need maximum control
* A feature is missing in Resource APIs
* Performance and predictability matter

```python
import boto3
client = boto3.client('ec2')
response = client.describe_instances()
```

### Resource (High-Level Abstraction)

* Object-oriented interface
* Returns **Python objects**
* Cleaner, more readable code

**Use When**

* Writing scripts quickly
* Managing common resources (S3, EC2, DynamoDB)
* Readability > control

```python
s3 = boto3.resource('s3')
bucket = s3.Bucket('my-bucket')
bucket.upload_file('file.txt', 'file.txt')
```

**Production Reality:**
Most serious automation uses **Clients** under the hood.

---

## 3. Pagination (The Silent Failure Trap)

AWS APIs almost never return *all* results in one call.

### The Problem

APIs like:

* `describe_instances`
* `list_objects_v2`
* `describe_log_streams`

often return **only the first 1000 results**.

Your script:

* Appears to work
* Produces incomplete data
* Fails silently

### The Correct Pattern

Always use **Paginators**.

```python
paginator = client.get_paginator('list_objects_v2')

for page in paginator.paginate(Bucket='my-bucket'):
    for obj in page.get('Contents', []):
        print(obj['Key'])
```

**Interview Rule:**
If pagination is ignored, the answer is wrong.

---

## 4. Authentication & Credential Chain

Boto3 follows a strict credential resolution order:

1. Environment variables
2. AWS credentials file (`~/.aws/credentials`)
3. IAM Role (EC2 / EKS / ECS)
4. Explicit credentials in code (avoid)

**Best Practice**

* Never hardcode credentials
* Use IAM Roles in production
* Use short-lived credentials (STS)

---

## 5. Kubernetes Python Client

Stop shelling out to `kubectl` and parsing text.

### Why Use the Python Client

* Structured API access
* Watches events in real time
* No brittle `grep | awk | sed` pipelines

**Use Cases**

* Detect `CrashLoopBackOff`
* Auto-remediate failing pods
* Trigger Slack / PagerDuty alerts
* Build custom controllers or operators

```python
from kubernetes import client, config
config.load_incluster_config()
v1 = client.CoreV1Api()
```

---

## Boto3 & K8s Automation Summary

| Concept           | Purpose                  | Production Importance |
| ----------------- | ------------------------ | --------------------- |
| Boto3 Client      | Low-level AWS API access | Critical              |
| Boto3 Resource    | High-level abstraction   | Medium                |
| Pagination        | Prevent silent data loss | Mandatory             |
| IAM Roles         | Secure authentication    | Mandatory             |
| STS               | Short-lived credentials  | High                  |
| K8s Python Client | Event-driven automation  | High                  |

---

### Interview Reality Check

If your automation script:

* Hardcodes credentials
* Ignores pagination
* Parses `kubectl` output

…it is not production-grade.



---

Class 9.2.3:
	Title: Building Operational Tools
	Description: Real-world scripting scenarios.
Content Type: text
Duration: 400 
Order: 3
		Text Content :
 # Real-World Automation Scenarios

## 1. The "Janitor" Script (Cost Optimization)
* *Problem:* Dev environment costs are skyrocketing because engineers forget to delete EC2 instances / EBS volumes.
* *Solution:* A Python Lambda function that runs every night at 8 PM. It scans all EC2 instances, checks for a tag `KeepAlive=True`. If the tag is missing, it terminates the instance.

---

## 2. The Log Parser & Alerter
* *Problem:* Nginx logs are unstructured. We need to alert Slack if 500 errors spike above 5%.
* *Solution:* A script that `tails` the log file, uses Regex to extract the status code, calculates a rolling average in memory, and sends a POST request to a Slack Webhook if the threshold is breached.

---

## 3. The Deployment Verifier (Smoke Test)
* *Problem:* CI says "Success," but the app is returning 404s or 502s.
* *Solution:* A "Smoke Test" script included in the CD pipeline. After deployment, it hits the health endpoint `/healthz`. If it doesn't get a 200 OK within 60 seconds (retry logic), it triggers an automatic rollback via the Cloud Provider API.

---

## 4. Bulk Operations
* *Scenario:* "Rotate the SSH key on 500 servers."
* *Tool:* Use Python with the `paramiko` library (or Ansible) to loop through the inventory and execute the key update command safely.

---
Topic 9.3:
Title: Scripting Challenge
Order: 3

Class 9.3.1:
	Title: Automation Scripting - Challenge
	Description: Practical automation problems to test your coding skills.
Content Type: text
Duration: 600 
Order: 1
		Text Content :

# Scripting & Automation – Bash and Python Challenge
**Contest Format | 5 Questions**

This challenge evaluates **production-grade scripting**, not toy examples. Focus is on safety, correctness, and real operational usage.

---

## Question 1: Bash Script Writing (Loops, Conditionals, Functions)

### Problem  
You need a Bash script that checks disk usage on multiple servers and prints an alert if usage exceeds 80%.

**Tasks:**
1. Use a loop to iterate over mount points.
2. Use conditionals to compare usage.
3. Encapsulate logic inside a function.

---

### Answer

```bash
check_disk() {
  local threshold=80
  for mount in $(df -h | awk 'NR>1 {print $6}'); do
    usage=$(df -h "$mount" | awk 'NR==2 {gsub("%",""); print $5}')
    if [ "$usage" -gt "$threshold" ]; then
      echo "ALERT: $mount at ${usage}%"
    fi
  done
}

check_disk
````

**Key Points**

* Functions improve reusability
* Conditionals enforce thresholds
* Loops enable multi-resource checks

---

## Question 2: Error Handling with `set -euo pipefail`

### Problem

A deployment script continues execution even after a critical command fails, causing partial deployments.

**Tasks:**

1. Explain each flag in `set -euo pipefail`.
2. Demonstrate why it is necessary.

---

### Answer

`set -euo pipefail`

**Explanation**

* `-e`: Exit immediately on command failure
* `-u`: Exit on use of undefined variables
* `pipefail`: Fail pipelines if any command fails

**Why It Matters**

`false | true   # Without pipefail → success`

With `pipefail`, the script exits immediately, preventing unsafe execution.

**Production Rule**
Always enable strict mode at the top of automation scripts.

---

## Question 3: Python API Integration (Requests Library)

### Problem

You need to call a REST API, handle failures, and parse JSON responses safely.

**Tasks:**

1. Perform a GET request
2. Handle HTTP errors
3. Parse the JSON response

---

### Answer

```python
import requests

url = "https://api.example.com/users"

response = requests.get(url, timeout=5)
response.raise_for_status()

data = response.json()
for user in data["users"]:
    print(user["id"], user["email"])
```

**Best Practices**

* Always use `timeout`
* Use `raise_for_status()` to fail fast
* Never assume the API is reliable

---

## Question 4: Boto3 AWS Automation (EC2/S3 Pagination)

### Problem

A script lists EC2 instances but misses some when the account has many resources.

**Tasks:**

1. Explain why this happens
2. Fix it using pagination

---

### Answer

**Root Cause**
AWS APIs return paginated results (usually 100–1000 items).

**Correct Approach**

```python
import boto3

ec2 = boto3.client("ec2")
paginator = ec2.get_paginator("describe_instances")

for page in paginator.paginate():
    for reservation in page["Reservations"]:
        for instance in reservation["Instances"]:
            print(instance["InstanceId"])
```

**Interview Insight**
Missing pagination is a common silent production bug.

---

## Question 5: Log Parsing, Process Management & Safety Controls

### Problem

A system is slow due to zombie processes and noisy logs. You must safely analyze logs and clean up processes.

**Tasks:**

1. Parse logs using regex
2. Identify zombie processes
3. Implement input validation and dry-run safety

---

### Answer

**Log Parsing with Regex**

```bash
grep -E "ERROR|CRITICAL" /var/log/app.log
```

**Finding Zombie Processes**

```bash
ps aux | awk '$8=="Z" {print $2, $11}'
```

**Safe Cleanup (Dry Run)**

```bash
DRY_RUN=true

kill_zombie() {
  local pid=$1
  if [[ "$pid" =~ ^[0-9]+$ ]]; then
    if [ "$DRY_RUN" = true ]; then
      echo "DRY RUN: Would kill $pid"
    else
      kill -9 "$pid"
    fi
  else
    echo "Invalid PID"
  fi
}
```

**Why This Matters**

* Regex ensures precise log filtering
* Zombie cleanup prevents resource leaks
* Dry-run prevents accidental outages

---

## Contest Evaluation Criteria

* Safe scripting practices
* Proper error handling
* API and cloud automation correctness
* Defensive programming mindset
* Production-grade logic over shortcuts

This challenge mirrors **real on-call scripting and automation incidents**.


---

Module 10:
Title: Databases & Data Management
Description: Master database operations, backup strategies, replication, and performance tuning for both SQL and NoSQL databases.
Order: 10
Learning Outcomes:
Understand database fundamentals for DevOps
Implement backup and recovery strategies
Configure replication and high availability
Basic performance tuning

Topic 10.1:
Title: SQL Databases
Order: 1

Class 10.1.1:
	Title: Database Fundamentals for DevOps
	Description: ACID properties, Indexing, and Pooling.
Content Type: text
Duration: 400 
Order: 1
		Text Content :
 # SQL Fundamentals: The DevOps Perspective

## 1. ACID Properties (The Interview Definition)
You don't need to be a DBA, but you must know what guarantees a database provides.
* **Atomicity:** All or nothing. If the power fails halfway through a transaction, the DB rolls back. No partial data.
* **Consistency:** The data must meet all rules (Foreign Keys, Constraints) before and after the transaction.
* **Isolation:** Transactions don't interfere with each other. (I can't read your data until you commit).
* **Durability:** Once you say "Commit," it is written to the disk. Even if the server explodes, the data is safe.

### Isolation Levels (Common Interview Topic)
| Level | Dirty Read | Non-Repeatable Read | Phantom Read |
| :--- | :---: | :---: | :---: |
| Read Uncommitted | Yes | Yes | Yes |
| Read Committed | No | Yes | Yes |
| Repeatable Read | No | No | Yes |
| Serializable | No | No | No |

* **Read Committed:** Default for PostgreSQL. You only see committed data.
* **Repeatable Read:** Default for MySQL InnoDB. Same query returns same results within a transaction.
* **Serializable:** Slowest but safest. Transactions execute as if they were sequential.

---

## 2. Indexes: The Performance Switch
* **The Concept:** Think of a book's index. Without it, you scan every page (Full Table Scan). With it, you jump to page 42 (Index Seek).
* **B-Tree:** The default index structure. Good for ranges (`WHERE age > 20`).
* **Cost:** Indexes speed up **Reads** but slow down **Writes** (because you have to update the index every time you insert a row).
* *DevOps Role:* Use `EXPLAIN` to analyze query plans. If you see "Seq Scan" on a huge table, you are missing an index.

### Index Types Deep Dive
| Index Type | Best For | Example |
| :--- | :--- | :--- |
| B-Tree | Range queries, equality | `WHERE age BETWEEN 20 AND 30` |
| Hash | Exact equality only | `WHERE id = 123` |
| GIN | Full-text search, arrays | `WHERE tags @> '{sports}'` |
| GiST | Geospatial, range types | `WHERE location <-> point` |
| BRIN | Very large, naturally ordered tables | Time-series data |

### Query Plan Analysis
```sql
-- PostgreSQL
EXPLAIN ANALYZE SELECT * FROM orders WHERE customer_id = 100;

-- MySQL
EXPLAIN SELECT * FROM orders WHERE customer_id = 100;
```
* **Seq Scan:** Full table scan - usually bad on large tables
* **Index Scan:** Using index to find rows - good
* **Index Only Scan:** Data returned from index itself - best
* **Bitmap Index Scan:** Multiple index conditions combined

---

## 3. Connection Pooling
Opening a TCP connection to a database is expensive (Handshake + Auth).
* **The Problem:** If every user request opens a new DB connection, the DB crashes under load.
* **The Solution:** Use a Connection Pool (like PgBouncer). It keeps 10 connections open and reuses them for 1000 users.

### Connection Pool Configuration
```ini
# PgBouncer example
[databases]
mydb = host=127.0.0.1 port=5432 dbname=mydb

[pgbouncer]
pool_mode = transaction
max_client_conn = 1000
default_pool_size = 20
```

### Pool Modes
* **Session:** Connection held for entire session (least efficient)
* **Transaction:** Connection returned after each transaction (recommended)
* **Statement:** Connection returned after each statement (most aggressive)

---

## 4. Database Locks and Deadlocks
* **Row-Level Locks:** `SELECT ... FOR UPDATE` locks specific rows
* **Table-Level Locks:** DDL operations lock entire tables
* **Deadlock Detection:** Database automatically detects and kills one transaction

### Avoiding Lock Contention
* Keep transactions short
* Access resources in consistent order
* Use optimistic locking with version columns
* Monitor lock waits: `SELECT * FROM pg_stat_activity WHERE wait_event_type = 'Lock';`

---

Class 10.1.2:
	Title: High Availability & Replication
	Description: Master-Slave, Failover, and Backups.
Content Type: text
Duration: 450 
Order: 2
		Text Content :
 # Keeping the Database Alive (HA)

## 1. Master-Slave Replication
The standard architecture for 90% of apps.
* **Master (Primary):** Handles **Writes** and Reads.
* **Slave (Replica):** Handles **Reads** only. Copies data from Master.
* **Replication Lag:** The time delay between data hitting the Master and appearing on the Slave.
    * *Interview Q:* "A user updates their profile but still sees the old name. Why?"
    * *Answer:* They wrote to Master, but read from a lagging Slave. (Fix: "Read your own writes" consistency).

### Replication Types
| Type | Data Loss Risk | Performance Impact | Use Case |
| :--- | :--- | :--- | :--- |
| Synchronous | Zero | Higher latency | Financial systems |
| Asynchronous | Possible | Lower latency | Most applications |
| Semi-Synchronous | Minimal | Medium | Balanced approach |

### Monitoring Replication Health
```sql
-- PostgreSQL: Check replication lag
SELECT client_addr, state, 
       pg_wal_lsn_diff(pg_current_wal_lsn(), sent_lsn) as send_lag,
       pg_wal_lsn_diff(sent_lsn, flush_lsn) as flush_lag
FROM pg_stat_replication;

-- MySQL: Check slave status
SHOW SLAVE STATUS\G
```

---

## 2. Master-Master (Multi-Master)
* **Concept:** You can write to Node A or Node B.
* **The Trap:** **Write Conflicts.** (User A updates Row 1 on Node A. User B updates Row 1 on Node B. Who wins?).
* *Advice:* Avoid this unless you are Netflix/Uber level. It is complex and error-prone.

### Conflict Resolution Strategies
* **Last Write Wins (LWW):** Timestamp-based, simple but can lose data
* **Application-Level:** Business logic decides the winner
* **CRDTs:** Conflict-free Replicated Data Types for specific use cases

---

## 3. Backups: The Last Line of Defense
* **Logical (mysqldump/pg_dump):** Exports SQL statements (`INSERT INTO...`). Portable but slow to restore.
* **Physical (Snapshots):** Copies the raw disk blocks. Fast to restore, but huge size.
* **Point-in-Time Recovery (PITR):** "Restore the DB to exactly 14:03 PM yesterday, right before the intern dropped the table." Requires Base Backup + WAL (Write Ahead Logs).

### Backup Comparison Table
| Method | Speed (Backup) | Speed (Restore) | Size | Portability |
| :--- | :--- | :--- | :--- | :--- |
| pg_dump/mysqldump | Slow | Very Slow | Small | High |
| Physical Snapshot | Fast | Fast | Large | Low |
| pg_basebackup | Fast | Fast | Large | Medium |
| WAL Archiving | Continuous | Medium | Medium | Medium |

### Backup Best Practices
```bash
# PostgreSQL PITR setup
# 1. Enable WAL archiving in postgresql.conf
archive_mode = on
archive_command = 'cp %p /backup/wal/%f'

# 2. Take base backup
pg_basebackup -D /backup/base -Ft -z -P

# 3. Restore to point in time
restore_command = 'cp /backup/wal/%f %p'
recovery_target_time = '2024-01-15 14:03:00'
```

---

## 4. Failover Strategies
* **Manual Failover:** DBA promotes replica manually. RTO: 30+ minutes.
* **Automated Failover:** Tools like Patroni, Orchestrator detect failure and promote.
* **Witness/Quorum:** Prevents split-brain in distributed setups.

### Automated Failover with Patroni (PostgreSQL)
```yaml
# patroni.yml
scope: postgres-cluster
name: node1

restapi:
  listen: 0.0.0.0:8008

etcd:
  hosts: etcd1:2379,etcd2:2379,etcd3:2379

bootstrap:
  dcs:
    ttl: 30
    loop_wait: 10
    maximum_lag_on_failover: 1048576
```

---

## 5. Disaster Recovery Testing
* **Regular DR Drills:** Test your backups monthly. Untested backups are not backups.
* **Runbook Documentation:** Step-by-step recovery procedures
* **RTO/RPO Validation:** Measure actual recovery time vs. targets

Topic 10.2:
Title: NoSQL Databases
Order: 2

Class 10.2.1:
Title: NoSQL Overview & Use Cases
Description: CAP Theorem, Mongo, and Cassandra.
Content Type: text
Duration: 350 
Order: 2
		Text Content :

 # Redis vs. Memcached (what interviewers actually care about)

The question is rarely “which is faster?” It’s:
* what data structures and features you need,
* what durability story you need,
* and what operational complexity you can handle.

---

## 1. Memcached
* **Simple distributed cache:** key → value (strings/blobs).
* **Multithreaded:** great raw throughput.
* **No persistence:** cache is disposable.

Typical use:
* full-page fragments, rendered HTML, short-lived computed results.

---

## 2. Redis
* **Rich data structures:** Strings, Hashes, Lists, Sets, Sorted Sets, Streams, Bitmaps.
* **Persistence options:**
  * **RDB snapshots:** periodic snapshots (faster, possible data loss window).
  * **AOF:** append-only log (better durability, more IO).
* **Replication/HA:** replicas + Sentinel; or Redis Cluster for sharding.

Typical use:
* sessions, rate limiting, leaderboards, distributed locks (carefully), job queues, pub/sub.

### Key operational trade-offs
* Redis can become a **central dependency** → design for degradation.
* Single-threaded per shard/process means you scale by **sharding** or **multiple nodes**.

---

## 3. Eviction policies (common interview follow-up)
When Redis memory is full, eviction policy decides what gets removed.
* **noeviction:** writes fail (safer correctness, can break app).
* **allkeys-lru / volatile-lru:** remove least recently used keys.
* **ttl-based variants:** prefer evicting keys with TTL.

---

## 4. Monitoring (what to watch)
* **Hit rate:** low hit rate often means wrong keys, too short TTL, or insufficient memory.
* **Evictions:** sustained evictions usually mean underprovisioned cache.
* **Latency (p95/p99):** spikes indicate CPU saturation, big values, or slow persistence.
* **Keyspace:** number/size of keys; watch for unexpected growth.
* **Replication lag:** affects read correctness if reading from replicas.

## Quick checklist
* Use Memcached for pure ephemeral blobs at huge throughput
* Use Redis when you need data structures, TTL semantics, or HA/persistence
* Always define eviction policy + alert on evictions and latency
| Redis Cluster | CP | Strong |
| CockroachDB | CP | Serializable |

### PACELC Extension
When there's no Partition (P), you still trade off between Latency (L) and Consistency (C):
* **PA/EL:** Cassandra - Availability during partition, Latency normally
* **PC/EC:** MongoDB - Consistency always, even at cost of latency

---

## 2. Document Stores (MongoDB)
* **Structure:** JSON Documents. Flexible Schema (No `ALTER TABLE` needed).
* **Use Case:** Content Management, User Profiles, Catalogs.
* *DevOps Note:* Sharding is hard. Plan your "Shard Key" carefully or you will have "Hot Shards" (one server doing all the work).

### MongoDB Architecture
```
┌─────────────────────────────────────────┐
│              mongos (Router)            │
└─────────────────────────────────────────┘
         │              │              │
    ┌────▼────┐    ┌────▼────┐    ┌────▼────┐
    │ Shard 1 │    │ Shard 2 │    │ Shard 3 │
    │ Primary │    │ Primary │    │ Primary │
    │Secondary│    │Secondary│    │Secondary│
    │Secondary│    │Secondary│    │Secondary│
    └─────────┘    └─────────┘    └─────────┘
```

### Shard Key Selection (Critical Interview Topic)
| Shard Key Type | Pros | Cons |
| :--- | :--- | :--- |
| Hashed | Even distribution | No range queries |
| Range-based | Efficient range queries | Potential hot spots |
| Compound | Best of both | More complex |

```javascript
// Good shard key: high cardinality, even distribution
sh.shardCollection("orders.transactions", { "customer_id": "hashed" })

// Bad shard key: low cardinality causes hot shards
// { "country": 1 }  // Only ~200 values!
```

---

## 3. Column-Family (Cassandra)
* **Structure:** Wide Columns.
* **Superpower:** Infinite Write Scalability. You can write to *any* node.
* **Trade-off:** Eventual Consistency. You might read stale data for a few milliseconds.
* **Use Case:** IoT Sensor logs, Chat History (Discord uses it).

### Cassandra Data Modeling
```cql
-- Design tables around your queries, not entities
CREATE TABLE messages_by_channel (
    channel_id UUID,
    message_time TIMESTAMP,
    message_id UUID,
    content TEXT,
    PRIMARY KEY ((channel_id), message_time, message_id)
) WITH CLUSTERING ORDER BY (message_time DESC);
```

### Consistency Levels
| Level | Meaning | Use Case |
| :--- | :--- | :--- |
| ONE | Single replica | Fastest, risky |
| QUORUM | Majority of replicas | Balanced (recommended) |
| ALL | All replicas | Slowest, safest |
| LOCAL_QUORUM | Majority in local DC | Multi-DC deployments |

---

## 4. Key-Value Stores (DynamoDB)
* **Fully Managed:** No servers to manage, auto-scaling built-in
* **Pricing Model:** Pay per request or provisioned capacity
* **Global Tables:** Multi-region, multi-master replication

### DynamoDB Capacity Planning
```
Read Capacity Units (RCU):
  - 1 RCU = 1 strongly consistent read/sec (up to 4KB)
  - 1 RCU = 2 eventually consistent reads/sec

Write Capacity Units (WCU):
  - 1 WCU = 1 write/sec (up to 1KB)
```

### DynamoDB Best Practices
* Use composite keys (partition + sort) for flexible queries
* Implement single-table design for related entities
* Use Global Secondary Indexes (GSI) for alternate access patterns
* Enable DynamoDB Streams for change data capture

Class 10.2.2:
	Title: Redis for DevOps
	Description: Caching strategies and Persistence.
Content Type: text
Duration: 350 
Order: 2
		Text Content :
 # Redis: The Speed Layer

## 1. What is Redis?
In-Memory Key-Value store. Sub-millisecond latency.
* **Use Cases:** Caching (session tokens), Leaderboards (Sorted Sets), Job Queues (Pub/Sub).

### Redis Data Structures
| Structure | Use Case | Example Commands |
| :--- | :--- | :--- |
| Strings | Counters, caching | `SET`, `GET`, `INCR` |
| Hashes | User profiles, objects | `HSET`, `HGET`, `HGETALL` |
| Lists | Queues, recent items | `LPUSH`, `RPOP`, `LRANGE` |
| Sets | Tags, unique items | `SADD`, `SMEMBERS`, `SINTER` |
| Sorted Sets | Leaderboards, rankings | `ZADD`, `ZRANGE`, `ZRANK` |
| Streams | Event logs, messaging | `XADD`, `XREAD`, `XGROUP` |
| HyperLogLog | Cardinality estimation | `PFADD`, `PFCOUNT` |

---

## 2. Persistence (It's not just a cache)
* **RDB (Snapshot):** Saves the whole RAM to disk every X minutes. Fast restart, but you lose data since the last snapshot.
* **AOF (Append Only File):** Logs every write command. Slower restart, but zero data loss.
* *Best Practice:* Use both for critical data.

### Persistence Configuration
```conf
# redis.conf

# RDB snapshots
save 900 1      # Save if 1 key changed in 900 sec
save 300 10     # Save if 10 keys changed in 300 sec
save 60 10000   # Save if 10000 keys changed in 60 sec

# AOF persistence
appendonly yes
appendfsync everysec  # Options: always, everysec, no

# AOF rewrite to compact the file
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
```

### Persistence Comparison
| Aspect | RDB | AOF |
| :--- | :--- | :--- |
| Data Safety | Some data loss | Near zero loss |
| Performance | Higher | Slightly lower |
| File Size | Compact | Larger |
| Recovery Time | Fast | Slower |
| Fork Overhead | Yes (for snapshots) | No |

---

## 3. Eviction Policies
What happens when Redis runs out of RAM?
* **noeviction:** Returns error. (Good for DB mode).
* **allkeys-lru:** Deletes the **Least Recently Used** keys. (Standard for Caching).
* **volatile-ttl:** Deletes keys that are expiring soon.

### All Eviction Policies
| Policy | Behavior | Use Case |
| :--- | :--- | :--- |
| noeviction | Error on writes | Primary database |
| allkeys-lru | LRU across all keys | General caching |
| allkeys-lfu | LFU across all keys | Hot data caching |
| volatile-lru | LRU on keys with TTL | Mixed workloads |
| volatile-lfu | LFU on keys with TTL | Mixed workloads |
| volatile-ttl | Shortest TTL first | Expiring data |
| allkeys-random | Random eviction | When LRU overhead matters |

---

## 4. Redis Cluster & Sentinel

### Redis Sentinel (HA without sharding)
* Monitors master/replica health
* Automatic failover when master fails
* Provides service discovery for clients

```conf
# sentinel.conf
sentinel monitor mymaster 127.0.0.1 6379 2
sentinel down-after-milliseconds mymaster 5000
sentinel failover-timeout mymaster 60000
```

### Redis Cluster (HA with sharding)
* Data sharded across multiple nodes using hash slots (16384 slots)
* Each master has 1+ replicas for failover
* No single point of failure

```bash
# Create 6-node cluster (3 masters, 3 replicas)
redis-cli --cluster create \
  node1:6379 node2:6379 node3:6379 \
  node4:6379 node5:6379 node6:6379 \
  --cluster-replicas 1
```

---

## 5. Redis Monitoring & Troubleshooting

### Key Metrics to Monitor
* **Memory:** `used_memory`, `used_memory_rss`, `mem_fragmentation_ratio`
* **Performance:** `instantaneous_ops_per_sec`, `latency`
* **Connections:** `connected_clients`, `blocked_clients`
* **Replication:** `master_link_status`, `master_last_io_seconds_ago`

```bash
# Get all stats
redis-cli INFO

# Monitor commands in real-time (dangerous in prod)
redis-cli MONITOR

# Analyze memory usage
redis-cli MEMORY DOCTOR

# Find big keys
redis-cli --bigkeys
```

Topic 10.3:
Title: Database Operations
Order: 3

Class 10.3.1:
Title: Database Automation
Description: Migrations and Monitoring.
Content Type: text
Duration: 400 
Order: 2
Text Content :

 # Message Queues: Decoupling, Backpressure, and Reliability

Queues are introduced when the system needs to:
* absorb spikes,
* decouple producers from consumers,
* run slow tasks asynchronously,
* or increase reliability via retries and durable storage.

---

## 1. Synchronous vs. Asynchronous
* **Sync (REST):** user waits; your request timeouts and thread pools become bottlenecks.
* **Async (queue + workers):** user gets quick acknowledgement; work happens later.

Typical async flow:
1) API validates request and writes a job message.
2) Worker consumes job and performs work.
3) Result is stored (DB/object storage) and user is notified (poll/webhook/email).

---

## 2. Load leveling and backpressure (shock absorber)
If 10,000 events happen in 1 second:
* Without a queue: DB or downstream service gets hammered.
* With a queue: backlog grows, workers drain at a controlled rate.

### Backpressure patterns
* **Consumer autoscaling:** increase workers based on lag.
* **Rate limiting at producer:** refuse/slow down producers.
* **Load shedding:** drop non-critical work when the system is overloaded.

---

## 3. Delivery semantics (must-know)

| Semantics | What it means | Typical reality | What you do |
| :--- | :--- | :--- | :--- |
| At-most-once | may lose messages | rare for critical systems | use only for best-effort |
| At-least-once | messages can repeat | very common | make consumers idempotent |
| Exactly-once | no loss, no duplicates | hard across boundaries | approximate via patterns |

### Idempotency (the interview keyword)
If the consumer gets the same message twice, it should not double-charge or double-create.
Common approaches:
* store a **dedupe key** (message id) and ignore repeats,
* use **upserts** instead of inserts,
* make operations **commutative** where possible.

---

## 4. Retries, DLQ, and poison messages
* **Retries:** use exponential backoff + jitter.
* **Poison message:** a message that always fails (bad payload). Don’t retry forever.
* **DLQ (Dead Letter Queue):** move failed messages after N attempts for inspection.

Interview answer shape:
* “We retry transient failures; we DLQ permanent failures; we alert on DLQ growth.”

---

## 5. RabbitMQ vs. Kafka (choose based on problem)

### RabbitMQ
* **Queue-first:** task distribution, routing, acknowledgements.
* Good for **work queues**, RPC-style async, complex routing.

### Kafka
* **Log-first:** durable ordered log with retention.
* **Partitions + consumer groups:** parallelism model.
* Good for **event streaming**, analytics pipelines, replay, multiple independent consumers.

### Key Kafka concept: partitions
Ordering is only guaranteed **within a partition**. Your partition key choice matters.

## Quick checklist
* Use queues to absorb spikes + decouple slow work
* Plan delivery semantics: assume at-least-once and design idempotency
* Add retries with backoff + DLQ for poison messages
* Choose RabbitMQ for work queues; Kafka for event streams + replay
);
```
-- V2__Add_profile_fields.sql
ALTER TABLE users ADD COLUMN first_name VARCHAR(100);
ALTER TABLE users ADD COLUMN last_name VARCHAR(100);
```

### CI/CD Pipeline Integration
```yaml
# GitLab CI example
migrate:
  stage: deploy
  script:
    - flyway -url=jdbc:postgresql://$DB_HOST/mydb migrate
  only:
    - main
```

---

## 2. Zero-Downtime Migrations
How do you rename a column without stopping the app?
* **The Pattern:**
    1.  Add new column (allow NULLs).
    2.  Code writes to *both* columns.
    3.  Backfill old data to new column.
    4.  Code switches to read new column.
    5.  Drop old column.
* *Interview Tip:* This separates "Mid-level" from "Senior." Seniors know that `DROP COLUMN` locks the table and causes downtime.

### Dangerous Operations Table
| Operation | PostgreSQL | MySQL | Risk Level |
| :--- | :--- | :--- | :--- |
| ADD COLUMN (nullable) | Fast | Fast | Low |
| ADD COLUMN (with default) | Fast (PG11+) | Lock | Medium |
| DROP COLUMN | Lock | Lock | High |
| ADD INDEX | CONCURRENTLY option | Lock | Medium |
| RENAME COLUMN | Fast | Fast | Low |
| CHANGE COLUMN TYPE | Lock | Lock | High |

### Safe Index Creation
```sql
-- PostgreSQL: Non-blocking index creation
CREATE INDEX CONCURRENTLY idx_users_email ON users(email);

-- MySQL: Use pt-online-schema-change
pt-online-schema-change --alter "ADD INDEX idx_email (email)" D=mydb,t=users
```

---

## 3. Monitoring What Matters
* **Slow Query Log:** The most useful log. Shows queries taking >1 second.
* **IOPS (Input/Output Operations):** If your Disk IOPS are maxed out, your DB will freeze.
* **Connections:** Monitoring active vs. idle connections to tune the Pool.

### Key Database Metrics Dashboard
| Metric | Warning Threshold | Critical Threshold | Action |
| :--- | :--- | :--- | :--- |
| CPU Usage | >70% | >90% | Scale up or optimize queries |
| Memory Usage | >80% | >95% | Increase RAM or tune buffers |
| Connections | >80% of max | >95% of max | Tune pool, add replicas |
| Replication Lag | >10 seconds | >60 seconds | Check network, slave health |
| Disk IOPS | >80% provisioned | >95% | Upgrade storage tier |
| Slow Queries | >10/min | >50/min | Analyze and add indexes |

### Prometheus + Grafana Setup
```yaml
# postgres_exporter for Prometheus
DATA_SOURCE_NAME: "postgresql://user:pass@localhost:5432/db?sslmode=disable"

# Key queries to monitor
pg_stat_user_tables_seq_scan      # Sequential scans (needs indexing?)
pg_stat_user_tables_idx_scan      # Index usage
pg_stat_activity_count            # Active connections
pg_stat_replication_lag           # Replication health
```

---

## 4. Database Security

### Access Control Best Practices
* **Principle of Least Privilege:** Apps get only required permissions
* **Separate Users:** Different credentials for app, admin, backup, monitoring
* **Network Isolation:** DB in private subnet, no public IP

```sql
-- Create read-only user for reporting
CREATE USER reporting_user WITH PASSWORD 'secure_password';
GRANT CONNECT ON DATABASE mydb TO reporting_user;
GRANT SELECT ON ALL TABLES IN SCHEMA public TO reporting_user;

-- Create app user with limited writes
CREATE USER app_user WITH PASSWORD 'app_password';
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO app_user;
REVOKE DELETE ON sensitive_table FROM app_user;
```

### Encryption
* **At Rest:** Enable TDE (Transparent Data Encryption) or use encrypted storage
* **In Transit:** Enforce SSL/TLS connections
* **Application Level:** Encrypt sensitive columns (PII, payment data)

---

Topic 10.4:
Title: Database Performance Tuning
Order: 4

Class 10.4.1:
Title: Query Optimization
Description: Identifying and fixing slow queries.
Content Type: text
Duration: 400 
Order: 2
Text Content :

# Query Optimization

**Identifying, analyzing, and fixing slow database queries**

---

## 1. What Is Query Optimization?

Query optimization is the process of improving database query performance by:

* Reducing execution time
* Minimizing disk I/O
* Using indexes efficiently
* Enabling better execution plans

Poorly optimized queries cause:

* Slow application responses
* High CPU and memory usage
* Lock contention
* System-wide performance degradation

---

## 2. How Databases Execute Queries

When a query is submitted, the database:

1. Parses the SQL
2. Generates multiple execution plans
3. Estimates cost using statistics
4. Selects the lowest-cost plan
5. Executes the query

Query optimization focuses on influencing the planner to choose efficient plans.

---

## 3. Query Analysis Tools

| Tool               | Database          | Purpose                  |
| ------------------ | ----------------- | ------------------------ |
| EXPLAIN            | PostgreSQL, MySQL | Estimated execution plan |
| EXPLAIN ANALYZE    | PostgreSQL        | Actual execution metrics |
| pg_stat_statements | PostgreSQL        | Aggregated query stats   |
| Slow Query Log     | MySQL             | Captures slow queries    |
| Performance Schema | MySQL             | Low-level metrics        |
| pt-query-digest    | MySQL             | Slow query analysis      |

---

## 4. Reading Execution Plans

### PostgreSQL Example

```sql
EXPLAIN (ANALYZE, BUFFERS)
SELECT * FROM orders WHERE customer_id = 100;
```

**Sample Output**

```
Index Scan using idx_orders_customer on orders
(cost=0.43..8.45 rows=1 width=100)
(actual time=0.025..0.026 rows=1 loops=1)
Buffers: shared hit=3
Execution Time: 0.042 ms
```

### Key Metrics

| Metric      | Meaning             |
| ----------- | ------------------- |
| Seq Scan    | Full table scan     |
| Index Scan  | Index-based lookup  |
| Cost        | Planner’s estimate  |
| Actual Time | Real execution time |
| Rows        | Estimated vs actual |
| Buffers     | Memory vs disk I/O  |

Large differences between estimated and actual rows usually indicate outdated statistics.

---

## 5. Common Query Anti-Patterns

| Anti-Pattern           | Problem           | Solution                   |
| ---------------------- | ----------------- | -------------------------- |
| SELECT *               | Unnecessary data  | Select only needed columns |
| Missing WHERE          | Full scans        | Always filter              |
| WHERE function(column) | Index unusable    | Rewrite condition          |
| LIKE '%term%'          | Full scan         | Use full-text search       |
| No LIMIT               | Large result sets | Paginate                   |
| N+1 queries            | Many round trips  | Use joins or batching      |

---

## 6. N+1 Query Problem

### Inefficient Approach

```sql
SELECT * FROM orders WHERE customer_id = 1;
-- For each order:
SELECT * FROM order_items WHERE order_id = ?;
```

### Optimized Approach

```sql
SELECT o.*, oi.*
FROM orders o
JOIN order_items oi ON o.id = oi.order_id
WHERE o.customer_id = 1;
```

This reduces multiple round trips to a single query.

---

## 7. Index Optimization

### When to Create Indexes

* Columns in WHERE clauses
* Join columns
* Columns in ORDER BY
* High-cardinality columns

### When Not to Create Indexes

* Small tables
* Low-cardinality columns
* Write-heavy tables
* Rarely queried columns

---

## 8. Composite Index Best Practices

### Example

```sql
CREATE INDEX idx_orders_search
ON orders(status, customer_id, created_at);
```

### Efficient Query

```sql
SELECT *
FROM orders
WHERE status = 'pending'
  AND customer_id = 100
  AND created_at > '2024-01-01';
```

### Inefficient Query

```sql
SELECT *
FROM orders
WHERE created_at > '2024-01-01';
```

Index rule: equality columns first, range columns last.

---

## 9. Join Optimization

### Join Types

* Nested Loop: efficient for small datasets
* Hash Join: efficient for large datasets
* Merge Join: requires sorted inputs

### Optimization Tips

* Index join columns
* Filter early
* Avoid unnecessary joins

---

## 10. Pagination Strategies

### Offset Pagination

```sql
SELECT *
FROM orders
ORDER BY created_at
LIMIT 20 OFFSET 10000;
```

Large offsets degrade performance.

### Keyset Pagination

```sql
SELECT *
FROM orders
WHERE created_at > '2024-01-01'
ORDER BY created_at
LIMIT 20;
```

Keyset pagination scales better.

---

## 11. Statistics and Maintenance

Keep planner statistics up to date:

```sql
ANALYZE;
VACUUM ANALYZE;
```

Outdated statistics lead to inefficient plans.

---

## 12. Optimization Checklist

* Use EXPLAIN ANALYZE
* Avoid SELECT *
* Index filtering and join columns
* Eliminate N+1 queries
* Paginate results
* Prefer keyset pagination
* Keep statistics current
* Monitor slow queries

---

## 13. Interview Talking Points

* Indexes improve reads but slow writes
* Query optimization is I/O focused
* Execution plans must be verified, not assumed
* ORMs commonly cause N+1 issues
* Composite index order matters

---

## Final Note

Query optimization is an iterative process based on measurement and validation.
Small changes can produce significant performance gains.

---

Class 10.4.2:
	Title: Database Configuration Tuning
	Description: Memory, connections, and buffer settings.
Content Type: text
Duration: 400 
Order: 2
		Text Content :
 # Configuration Tuning: Getting More from Your Hardware

## 1. Memory Configuration

### PostgreSQL Key Parameters
```conf
# postgresql.conf - for 16GB RAM server

# Shared memory for caching
shared_buffers = 4GB              # 25% of RAM

# Memory per query operation
work_mem = 256MB                  # Per sort/hash operation
maintenance_work_mem = 1GB        # For VACUUM, CREATE INDEX

# OS cache hint
effective_cache_size = 12GB       # 75% of RAM

# WAL settings
wal_buffers = 64MB
checkpoint_completion_target = 0.9
```

### MySQL/InnoDB Key Parameters
```conf
# my.cnf - for 16GB RAM server

# Buffer pool (most important setting!)
innodb_buffer_pool_size = 12G     # 70-80% of RAM
innodb_buffer_pool_instances = 8  # Reduce contention

# Log settings
innodb_log_file_size = 2G
innodb_log_buffer_size = 256M

# I/O settings
innodb_io_capacity = 2000         # For SSDs
innodb_io_capacity_max = 4000
```

---

## 2. Connection Tuning

### Connection Limits
```sql
-- PostgreSQL
ALTER SYSTEM SET max_connections = 200;

-- But don't set too high! Each connection uses ~10MB RAM
-- Better approach: Use connection pooler
```

### PgBouncer Configuration
```ini
[databases]
mydb = host=localhost port=5432 dbname=mydb

[pgbouncer]
listen_port = 6432
max_client_conn = 1000        # Many app connections
default_pool_size = 20        # Few DB connections
pool_mode = transaction       # Return conn after txn
server_idle_timeout = 600
```

---

## 3. Storage and I/O Tuning

### Filesystem Recommendations
| Aspect | Recommendation |
| :--- | :--- |
| Filesystem | XFS or ext4 |
| Mount Options | `noatime,nodiratime` |
| Scheduler | `noop` or `deadline` for SSDs |
| RAID | RAID 10 for databases |

### PostgreSQL I/O Settings
```conf
# For SSDs
random_page_cost = 1.1            # Default 4.0 (for HDDs)
effective_io_concurrency = 200    # Parallel I/O operations
```

---

## 4. Vacuum and Maintenance

### PostgreSQL Autovacuum Tuning
```conf
autovacuum = on
autovacuum_max_workers = 4
autovacuum_naptime = 30s

# For high-traffic tables
autovacuum_vacuum_scale_factor = 0.05    # Default 0.2
autovacuum_analyze_scale_factor = 0.025  # Default 0.1

# Prevent wraparound issues
autovacuum_freeze_max_age = 500000000
```

### Regular Maintenance Tasks
```sql
-- Rebuild bloated indexes
REINDEX INDEX CONCURRENTLY idx_name;

-- Update statistics
ANALYZE table_name;

-- Check table bloat
SELECT schemaname, tablename, 
       pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
FROM pg_tables 
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
```

Class 10.4.3:
	Title: Database Troubleshooting Scenarios
	Description: Common production issues and solutions.
Content Type: text
Duration: 400 
Order: 3
		Text Content :
 # Database Troubleshooting: Real-World Scenarios

## 1. "Database is Slow" Investigation

### Systematic Approach
```bash
# Step 1: Check system resources
top                          # CPU, memory
iostat -x 1                  # Disk I/O
vmstat 1                     # System stats

# Step 2: Check database status
psql -c "SELECT * FROM pg_stat_activity WHERE state != 'idle';"

# Step 3: Check for blocking queries
psql -c "SELECT * FROM pg_stat_activity WHERE wait_event_type = 'Lock';"

# Step 4: Check slow query log
tail -f /var/log/postgresql/postgresql-*-main.log
```

---

## 2. Common Issues and Solutions

### Issue: Connection Exhaustion
```sql
-- Symptoms
FATAL: too many connections for role "appuser"
FATAL: remaining connection slots reserved for superuser

-- Diagnosis
SELECT count(*), state FROM pg_stat_activity GROUP BY state;
SELECT usename, count(*) FROM pg_stat_activity GROUP BY usename;

-- Quick fix: Kill idle connections
SELECT pg_terminate_backend(pid) 
FROM pg_stat_activity 
WHERE state = 'idle' AND query_start < now() - interval '1 hour';

-- Long-term fix: Use connection pooler (PgBouncer)
```

### Issue: Long-Running Queries Blocking Others
```sql
-- Find blocking queries
SELECT blocked_locks.pid AS blocked_pid,
       blocked_activity.usename AS blocked_user,
       blocking_locks.pid AS blocking_pid,
       blocking_activity.usename AS blocking_user,
       blocked_activity.query AS blocked_statement
FROM pg_catalog.pg_locks blocked_locks
JOIN pg_catalog.pg_stat_activity blocked_activity ON blocked_activity.pid = blocked_locks.pid
JOIN pg_catalog.pg_locks blocking_locks ON blocking_locks.locktype = blocked_locks.locktype
JOIN pg_catalog.pg_stat_activity blocking_activity ON blocking_activity.pid = blocking_locks.pid
WHERE NOT blocked_locks.granted;

-- Kill blocking query if necessary
SELECT pg_terminate_backend(blocking_pid);
```

### Issue: Table Bloat
```sql
-- Check for bloat
SELECT schemaname, relname, 
       n_dead_tup, n_live_tup,
       round(n_dead_tup * 100.0 / nullif(n_live_tup + n_dead_tup, 0), 2) as dead_pct
FROM pg_stat_user_tables
WHERE n_dead_tup > 10000
ORDER BY n_dead_tup DESC;

-- Fix with vacuum
VACUUM (VERBOSE, ANALYZE) table_name;

-- For severe bloat, use pg_repack (online)
pg_repack -d mydb -t bloated_table
```

---

## 3. Replication Troubleshooting

### Replication Lag Investigation
```sql
-- On primary: Check replication status
SELECT client_addr, state, sent_lsn, write_lsn, flush_lsn, replay_lsn,
       pg_wal_lsn_diff(sent_lsn, replay_lsn) AS lag_bytes
FROM pg_stat_replication;

-- On replica: Check recovery status
SELECT pg_is_in_recovery(),
       pg_last_wal_receive_lsn(),
       pg_last_wal_replay_lsn(),
       pg_last_xact_replay_timestamp();
```

### Common Causes of Replication Lag
| Cause | Solution |
| :--- | :--- |
| Network bottleneck | Increase bandwidth, check latency |
| Slow queries on replica | Tune replica, add more replicas |
| WAL generation too fast | Tune checkpoint settings |
| Replica under-resourced | Match hardware with primary |

---

## 4. Emergency Recovery Procedures

### Database Won't Start
```bash
# Check logs first!
tail -100 /var/log/postgresql/postgresql-*.log

# Common issues:
# - Disk full: Free space, check pg_wal directory
# - Corrupted files: Restore from backup
# - Port conflict: Check for other processes

# Emergency single-user mode
postgres --single -D /var/lib/postgresql/14/main mydb
```

### Data Corruption Recovery
```bash
# 1. Stop database
sudo systemctl stop postgresql

# 2. Check filesystem
fsck /dev/sdX

# 3. For minor corruption, try recovery mode
echo "fsync = off" >> /etc/postgresql/14/main/recovery.conf
echo "full_page_writes = off" >> /etc/postgresql/14/main/recovery.conf

# 4. If all else fails: Restore from backup
pg_restore -d mydb backup.dump
```
---
Topic 10.5:
Title: Database - Challenge
Order: 5

Class 10.5.1:
	Title: Database Operations - Challenge
	Description: Scenario-based database troubleshooting.
Content Type: text
Duration: 300 
Order: 1
		Text Content :
# Database Operations & Optimization Challenge
**Contest Format | 5 Questions**

This challenge evaluates your ability to **manage, optimize, and troubleshoot databases** in production environments.

---

## Question 1: Query Optimization & Execution Plans

### Problem  
A SQL query on a large table is taking minutes to execute.

**Tasks:**
1. Explain how to analyze the query execution plan.
2. Suggest indexes or query changes to improve performance.

---

### Answer

**Steps**
```sql
EXPLAIN ANALYZE SELECT * FROM orders WHERE customer_id = 12345;
````

**Optimization Techniques**

* Add an index on `customer_id` (`CREATE INDEX idx_customer ON orders(customer_id);`)
* Avoid `SELECT *`; select only required columns
* Use partitioning for large tables
* Ensure statistics are up-to-date

---

## Question 2: Backup and Recovery Procedures

### Problem

You must ensure minimal downtime and data loss in case of database failure.

**Tasks:**

1. Describe full and incremental backups.
2. Explain point-in-time recovery.

---

### Answer

**Full Backup**

* Complete snapshot of the database

**Incremental Backup**

* Captures only changes since the last backup

**Point-in-Time Recovery (PITR)**

* Use transaction logs or binlogs to restore to a specific timestamp
* Ensures minimal data loss

**Best Practice**

* Automate daily backups and test restores regularly

---

## Question 3: Replication Configuration & Troubleshooting

### Problem

A MySQL replica is lagging behind the primary.

**Tasks:**

1. Identify causes of replication lag
2. Suggest monitoring and fixes

---

### Answer

**Causes**

* Heavy write load on primary
* Slow network between primary and replica
* Long-running queries on replica

**Monitoring**

`SHOW SLAVE STATUS\G`

* Check `Seconds_Behind_Master`

**Fixes**

* Tune replica resources (CPU/RAM)
* Optimize replication queries
* Consider asynchronous vs synchronous replication trade-offs

---

## Question 4: Connection Pooling & Performance Tuning

### Problem

An application frequently opens new DB connections, causing high latency.

**Tasks:**

1. Explain connection pooling
2. Recommend pool size and monitoring strategy

---

### Answer

**Connection Pooling**

* Maintain a pool of pre-opened connections
* Reduces overhead of creating/destroying connections

**Tuning Tips**

* Pool size = number of app threads or processes that need DB
* Monitor pool metrics (idle vs active connections)
* Adjust based on workload spikes

---

## Question 5: NoSQL Data Modeling & CAP Theorem

### Problem

Design a highly available, globally distributed key-value store.

**Tasks:**

1. Explain CAP theorem
2. Suggest appropriate NoSQL model

---

### Answer

**CAP Theorem**

* **C**onsistency, **A**vailability, **P**artition Tolerance
* Cannot achieve all three simultaneously; pick two

**NoSQL Design**

* Key-Value or Document store for horizontal scaling
* Use eventual consistency (AP) for distributed apps
* Example: DynamoDB Global Tables or Cassandra

**Interview Tip**

* Be ready to justify trade-offs for consistency vs latency



Module 11:
Title: System Design for DevOps
Description: Learn to design scalable, resilient systems. Master architecture patterns, load balancing, caching strategies, and distributed system concepts.
Order: 11
Learning Outcomes:
Design scalable architectures
Understand distributed system patterns
Implement caching strategies
Design for high availability

Topic 11.1:
Title: Scalability Patterns
Order: 1

Class 11.1.1:
	Title: Horizontal vs Vertical Scaling
	Description: Scaling strategies and Database Sharding.
Content Type: text
Duration: 400 
Order: 1
		Text Content :

 # The Art of Scaling (Interview-Grade)

Scaling is not “add servers.” It is a decision process:
1) identify the bottleneck, 2) choose the cheapest lever, 3) avoid introducing new failure modes.

---

## 1. Vertical vs. Horizontal Scaling

| Dimension | Vertical (Scale Up) | Horizontal (Scale Out) |
| :--- | :--- | :--- |
| What it means | Bigger machine (CPU/RAM/IO) | More machines (instances/pods) |
| Typical use | Quick wins, legacy apps, single-node databases | Stateless APIs, worker fleets, read paths |
| Main limit | Hardware ceiling + single-node blast radius | Distributed complexity |
| New risks | Bigger single outage | Consistency and coordination issues |

### Practical heuristics
* **Scale up first when:** the app is still single-instance and you need immediate relief.
* **Scale out when:** you need fault tolerance, elastic capacity, or you hit hard node limits.

---

## 2. Bottleneck-first thinking (what senior candidates do)
Pick the scaling lever only after you know the limiting resource:
* **CPU-bound:** high CPU, long run queues → optimize code, add CPU, scale out.
* **Memory-bound:** OOMs, swap thrash → fix leaks, add RAM, reduce working set.
* **IOPS/latency-bound:** high iowait, slow DB queries → caching, query tuning, SSDs, partitioning.
* **Network-bound:** saturated NIC, high p99 → CDN, compression, batching, regionalization.

---

## 3. Statelessness (Cattle, Not Pets)
Horizontal scaling typically requires stateless services.

### What “stateless” means in practice
* No user session stored only in RAM.
* No important data stored only on local disk.
* No correctness-critical in-memory locks for cross-request logic.

### Common patterns
* **Sticky sessions:** LB pins a user to one instance.
  * *When acceptable:* small systems, short sessions.
  * *Risk:* uneven load, poor failover, painful deploys.
* **External state:** sessions in Redis/DB; files in object storage.
  * *Best practice:* keep the app stateless and externalize state.

---

## 4. Database scaling: replication vs sharding

### 4.1 Read scaling (replication)
* **Read replicas** offload read-heavy traffic.
* **Trade-off:** replica lag → stale reads.

### 4.2 Write scaling (sharding)
* **Sharding** splits data across nodes.
* Common strategies:
  * **Range-based:** UserID 1–1M → Shard A.
  * **Hash-based:** hash(UserID) % N → Shard.

### The “hot shard” problem (and fixes)
One shard melts because one tenant/user dominates.
* Fix with better shard key, tenant isolation, re-sharding plan, caching.

---

## 5. Capacity planning (simple model)
If asked “how many servers do we need?” do rough math:
* $\text{QPS} = \frac{\text{DAU} \times \text{requests per user per day}}{86400}$
* Choose safe utilization (e.g., 60–70%) and add headroom (deployments + failures).

## Quick checklist
* Identify bottleneck first
* Prefer stateless apps for horizontal scaling
* Read scaling via replicas; write scaling via sharding
* Have a re-sharding and hotspot mitigation plan

Class 11.1.2:
	Title: Load Balancing
	Description: L4 vs L7 and Algorithms.
Content Type: text
Duration: 450 
Order: 2
		Text Content :

 # Load Balancing: The Traffic Cops (and Failure Isolators)

Load balancers do more than distribute traffic. They also enable safe rollouts, enforce health routing, and protect backends.

---

## 1. Layer 4 vs. Layer 7

| Feature | Layer 4 (TCP/UDP) | Layer 7 (HTTP/HTTPS) |
| :--- | :--- | :--- |
| Routing based on | IP/port | Host/path/headers/cookies |
| Visibility | no HTTP awareness | full HTTP awareness |
| Performance | typically higher | slightly more overhead |
| Typical use | non-HTTP traffic, low-latency | microservices, smart routing |

### TLS termination (common follow-up)
* **Terminate TLS at LB:** simpler cert management, enables L7 routing.
* **Pass-through TLS:** end-to-end encryption, but less L7 visibility.

---

## 2. Algorithms
* **Round robin:** simple; assumes equal backends.
* **Weighted round robin:** supports mixed instance sizes.
* **Least connections:** good for long-lived connections.
* **Least response time:** better under uneven performance.
* **Consistent hashing:** same key tends to go to same backend (useful for caches).

---

## 3. Health checks (make them meaningful)
* **Passive:** infer from observed failures.
* **Active:** hit `/healthz` periodically.

Important distinction:
* **Liveness:** “restart me?”
* **Readiness:** “send traffic to me?”

Best practice: separate endpoints:
* `/live` (no dependencies)
* `/ready` (dependency-aware)

---

## 4. Global load balancing (high level)
When traffic is worldwide:
* Route users to nearest region via geo-DNS/anycast/global LB.
* Plan regional failover and test it.

## Quick checklist
* Choose L4 vs L7 based on routing needs
* Pick algorithm matching connection pattern
* Implement readiness/liveness correctly
* Plan global routing + regional failover

Topic 11.2:
Title: Caching Strategies
Order: 2

Class 11.2.1:
	Title: Caching Patterns
	Description: Cache-Aside, Write-Through, and Invalidation.
Content Type: text
Duration: 450 
Order: 1
		Text Content :

 # Caching: The Performance Cheat Code (and Consistency Trap)

Caching improves latency and reduces backend load, but it introduces staleness and stampede risks.

---

## 1. Cache-Aside (Lazy Loading)
Flow:
1) read cache
2) miss → read DB
3) write cache
4) return

* **Pros:** DB stays source of truth; cache outage degrades but doesn’t break correctness.
* **Cons:** first read is slow; stampedes; stale reads if invalidation is weak.

### Negative caching
Cache “not found” briefly to avoid repeated DB hits for missing keys.

---

## 2. Write-Through
Write to cache and DB together.
* **Pros:** fresher reads.
* **Cons:** higher write latency; cache availability may affect writes.

---

## 3. Write-Back / Write-Behind
Write to cache first; flush to DB asynchronously.
* **Pros:** very fast writes.
* **Cons:** durability risk if cache crashes; more complex correctness.

---

## 4. Refresh-Ahead
Refresh hot keys before they expire to reduce misses.

---

## 5. Invalidation strategies

| Strategy | How it works | Best for | Risk |
| :--- | :--- | :--- | :--- |
| TTL | expire after time | tolerates staleness | stale reads within TTL |
| Explicit delete | delete on write | stronger correctness | missed invalidations cause staleness |
| Versioned keys | key includes version | complex objects | key bloat |
| Event-driven | publish invalidation events | microservices | event loss/ordering |

### Stampede (thundering herd)
Mitigations:
* TTL jitter
* single-flight lock
* serve stale + refresh in background

---

## 6. Where to cache
* browser/client (HTTP caching)
* CDN/edge
* reverse proxy (Nginx/Varnish)
* application cache (Redis/Memcached)

## Quick checklist
* Choose pattern (aside/through/behind)
* Define staleness tolerance
* Add stampede protection
* Decide cache layer (CDN/proxy/app)

Class 11.2.2:
	Title: Redis & Memcached
	Description: In-memory data stores.
Content Type: text
Duration: 350 
Order: 2
		Text Content :

 # Redis vs. Memcached

The decision is usually about features and operational requirements, not just speed.

---

## 1. Memcached
* simple key/value blob cache
* multithreaded throughput
* no persistence (disposable)

Good for: ephemeral HTML fragments, computed results.

---

## 2. Redis
* rich data structures (Hashes, Sets, Sorted Sets, Streams)
* optional persistence (RDB/AOF)
* replication and HA patterns (replicas, Sentinel, Cluster)

Good for: sessions, rate limiting, leaderboards, queues (carefully).

---

## 3. Eviction policies (common follow-up)
When memory is full:
* `noeviction` (writes fail)
* LRU/LFU variants (allkeys/volatile)

---

## 4. Monitoring
* hit rate
* evictions
* latency (p95/p99)
* keyspace growth
* replication lag (if reading from replicas)

## Quick checklist
* Memcached: pure ephemeral cache at high throughput
* Redis: richer features + optional durability
* Always set eviction policy and alert on evictions/latency

Topic 11.3:
Title: Message Queues & Async Processing
Order: 3

Class 11.3.1:
	Title: Message Queue Patterns
	Description: Decoupling and Backpressure.
Content Type: text
Duration: 400 
Order: 1
		Text Content :

 # Message Queues: Decoupling, Backpressure, Reliability

Queues help absorb spikes, decouple services, and run slow work asynchronously.

---

## 1. Synchronous vs. Asynchronous
* Sync: user waits; request timeouts and thread pools become limits.
* Async: accept request, enqueue job, process later with workers.

---

## 2. Load leveling and backpressure
* Queue buffers bursts; workers drain at a controlled rate.
* Backpressure options:
  * autoscale consumers based on lag
  * rate limit producers
  * load shed non-critical tasks

---

## 3. Delivery semantics (must-know)

| Semantics | Reality | What you design for |
| :--- | :--- | :--- |
| At-most-once | can lose messages | best-effort only |
| At-least-once | can duplicate | idempotent consumers |
| Exactly-once | hard across systems | patterns + idempotency |

Idempotency patterns:
* dedupe key/message id store
* upserts
* idempotency keys for “create” operations

---

## 4. Retries, DLQ, poison messages
* exponential backoff + jitter
* DLQ after N attempts
* alert on DLQ growth

---

## 5. RabbitMQ vs Kafka
* RabbitMQ: work queues + routing
* Kafka: durable event log + replay

Kafka partition note: ordering is only guaranteed within a partition.

## Quick checklist
* Assume at-least-once; make consumers idempotent
* Add retries with backoff + DLQ
* Choose RabbitMQ for work distribution, Kafka for event streaming + replay

Topic 11.4:
Title: Microservices Architecture
Order: 4

Class 11.4.1:
	Title: Microservices Patterns
	Description: Discovery, Circuit Breakers, and Sagas.
Content Type: text
Duration: 450 
Order: 1
		Text Content :

 # Microservices: Distributed Complexity

Microservices enable independent deployments and team autonomy, but introduce network failures, versioning challenges, and consistency trade-offs.

---

## 1. Service discovery
IPs are ephemeral in Kubernetes.
* Use DNS/service discovery: call `http://user-service`, not an IP.
* Options: Kubernetes Services + CoreDNS, or registries like Consul.

---

## 2. Resilience patterns
* **Timeouts:** always set client timeouts.
* **Retries:** exponential backoff + jitter; avoid retry storms.
* **Circuit breaker:** fail fast when dependency is unhealthy.

Circuit breaker states:
* closed → open → half-open

---

## 3. Bulkheads, rate limits, load shedding
* isolate resources per dependency (bulkhead)
* rate limit to protect backends
* shed load when overloaded

---

## 4. Saga pattern (distributed transactions)
Transactions don’t span multiple services easily.
* choreography: event-driven
* orchestration: coordinator-driven

Always define compensations and make them idempotent.

---

## 5. Observability requirements
* correlation IDs
* distributed tracing
* golden signals: latency, traffic, errors, saturation

## Quick checklist
* discovery + DNS naming
* timeouts everywhere; cautious retries
* circuit breakers + bulkheads
* sagas for cross-service workflows
* trace IDs + golden-signal monitoring

Class 11.4.2:
	Title: Service Mesh
	Description: Istio and Sidecars.
Content Type: text
Duration: 400 
Order: 2
		Text Content :

 # Service Mesh: The Infrastructure Layer (when it’s worth it)

A service mesh standardizes service-to-service networking features across microservices.

---

## 1. Sidecar pattern
Run a proxy (often Envoy) next to each workload.
* app → sidecar (localhost)
* sidecar → network

This centralizes retries, timeouts, and telemetry.

---

## 2. What Istio commonly provides
* mTLS and service identity
* authorization policies
* traffic splitting (canary)
* metrics/traces/logs integration

---

## 3. Costs
* added CPU/memory overhead
* control-plane operations and upgrades
* harder debugging (policy vs app)

---

## 4. When to use (and when not to)
Use it when you have many services and strong security/traffic-policy needs.
Avoid it when you are small and lack platform bandwidth.

## Quick checklist
* mesh centralizes service-to-service policy and telemetry
* Istio enables mTLS + authz + canary
* complexity is the main trade-off

Module 12:
Title: Incident Management & SRE Practices
Description: Master incident response, on-call best practices, post-mortem analysis, and Site Reliability Engineering principles.
Order: 12
Learning Outcomes:
Respond effectively to production incidents
Conduct blameless post-mortems
Implement SRE best practices
Build reliable systems

Topic 12.1:
Title: Incident Response
Order: 1

Class 12.1.1:
	Title: Incident Management Framework
	Description: Roles, Severity levels, and War Rooms.
Content Type: text
Duration: 450 
Order: 1
		Text Content :
 # Incident Management Framework (How Senior Engineers Run Outages)

Incident response is a system, not heroics. Interviewers look for structure: fast mitigation, clear roles, strong communication, and learning loops.

---

## 1. The incident lifecycle (end-to-end)
1. **Detect:** monitoring alerts, synthetic checks, or user reports.
2. **Triage:** confirm impact, scope, and whether it’s real.
3. **Stabilize (mitigate):** stop the bleeding quickly (rollback, feature flag off, scale up, failover).
4. **Recover:** restore service to healthy SLO state.
5. **Resolve:** identify and fix the root cause.
6. **Learn:** run post-mortem and prevent recurrence.

Key principle:
* **Mitigation comes before root cause.** First get the service back; then dig deeper.

---

## 2. Severity levels (SEV) and what they imply

Severity is not a “feeling.” It sets:
* who gets paged,
* update frequency,
* escalation path,
* and whether you trigger an incident bridge/war room.

Example SEV model (adjust to company):
* **SEV-0 (Catastrophic):** full outage, major revenue/brand impact, widespread.
* **SEV-1 (Critical):** core flow broken or high error rate impacting many users.
* **SEV-2 (High):** partial degradation, limited blast radius, workaround exists.
* **SEV-3 (Low):** minor issues, no immediate action required.

Interview tip: mention **objective triggers**:
* “SEV-1 if checkout success rate drops below 98% for 5 minutes”
* “SEV-0 if >50% of all requests fail across regions”

---

## 3. War room roles (avoid chaos)
The fastest way to lose time is “everyone debugging” with no coordination.

Minimum roles:
* **Incident Commander (IC):** leads, assigns tasks, keeps time, decides mitigation.
* **Ops/Tech Lead:** runs the investigation and executes changes.
* **Comms Lead:** stakeholder updates, status page, exec comms.

Optional roles (for bigger incidents):
* **Scribe/Timeline:** records actions + timestamps.
* **Subject Matter Experts (SMEs):** DB, networking, application owners.

---

## 4. The first 10 minutes (what “good” looks like)
1) Acknowledge page and declare incident if impact is real.
2) Assign IC, comms, and scribe.
3) Define the symptom using metrics: “5xx is 12% in us-east only.”
4) Pick a safe mitigation candidate (rollback / disable feature / failover).
5) Announce next update time (e.g., every 15 minutes for SEV-1).

---

## 5. Common mitigation levers (DevOps toolbox)
* **Rollback:** fastest, safest if a recent deploy correlates.
* **Feature flag off:** reduce blast radius without full rollback.
* **Scale out:** buy time if saturation is the issue.
* **Failover:** move traffic to healthy region/cluster.
* **Load shedding:** return 429/503 for non-critical endpoints.
* **Read-only mode:** preserve core functionality.

---

## 6. Incident metrics (signals of maturity)
* **MTTA:** mean time to acknowledge.
* **MTTR:** mean time to restore.
* **Change failure rate:** % of changes causing incidents.
* **Time to mitigate vs time to root cause:** mitigation should be fast.

## Quick checklist
* Mitigate before root cause
* Assign IC + comms + scribe early
* Use objective SEV triggers and update cadence
* Track timeline and actions for the post-mortem

Class 12.1.2:
	Title: On-Call Best Practices
	Description: Surviving the pager and reducing fatigue.
Content Type: text
Duration: 400 
Order: 2
		Text Content :
 # On-Call Best Practices (How to Make Paging Sustainable)

On-call is a product of your system design. Great teams reduce pages via good alerting, automation, and reliability work—not by “being tough.”

---

## 1. Rotation design
* **Primary:** owns response and coordination.
* **Secondary:** backup + parallel investigation.
* **Escalation policy:** if not acknowledged in X minutes, page secondary, then manager.
* **Follow-the-sun:** best for global teams; reduces burnout and improves response quality.

Good rotation properties:
* predictable schedule,
* reasonable load,
* time to recover after a bad night.

---

## 2. Alert design (signal > noise)

### What should page a human?
Page only for user-impacting symptoms or imminent SLO violations.
Examples:
* error rate above threshold for 5 minutes
* saturation (CPU/queue lag) rising with clear user impact

### What should not page?
* “CPU is 80%” without impact.
* flappy alerts that self-resolve.
* informational events.

### The golden signals (SRE default)
* **Latency**
* **Traffic**
* **Errors**
* **Saturation**

---

## 3. Runbooks (the real on-call superpower)
Every page must include:
* link to runbook,
* dashboard link,
* recent deploy link,
* owner/team,
* clear “what to do first.”

Runbook format (keep it short):
1) **Symptom** (what the alert means)
2) **Impact** (user/business impact)
3) **Verify** (how to confirm)
4) **Triage** (where to look)
5) **Mitigate** (safe actions)
6) **Escalate** (who/when)
7) **Rollback/disable** (if applicable)

---

## 4. Paging hygiene (reduce fatigue)
* **Weekly alert review:** top noisy alerts, top incidents.
* **Delete/retune alerts** that don’t lead to action.
* **Add suppression during maintenance** (but keep guardrails).
* **Dedupe and grouping:** one incident, one page (not 50).

---

## 5. Automation and self-healing
The best page is the one you never receive.
* auto-rollbacks for bad deploys
* auto-scaling for saturation
* safe remediations (restart, drain node, rotate leader)

Interview tip: say “automate the first 80% of runbooks.”

## Quick checklist
* Page only for actionable, user-impacting signals
* Every alert links to a runbook and dashboards
* Review alert noise weekly and delete useless pages
* Invest in automation and self-healing

Topic 12.2:
Title: Post-Mortem Analysis
Order: 2

Class 12.2.1:
	Title: Blameless Post-Mortems
	Description: Root Cause Analysis and The 5 Whys.
Content Type: text
Duration: 450 
Order: 1
		Text Content :
 # Post-Mortems: Learning Loops That Prevent Repeat Incidents

A post-mortem is not a blame exercise; it is an engineering document that turns pain into system improvements.

---

## 1. Blameless culture (what it means)
Blameless is not “no accountability.” It means:
* focus on system and process failures,
* assume humans will make mistakes,
* design guardrails to prevent the same class of error.

Good phrasing:
* “The deploy process allowed an unsafe config to reach prod.”

Avoid phrasing:
* “Alice broke prod.”

---

## 2. Post-mortem structure (simple, complete)
1) **Summary:** what happened, what users saw.
2) **Impact:** duration, affected % users, revenue/SLO impact.
3) **Detection:** how we learned (alert, customer, on-call).
4) **Timeline:** timestamped actions and observations.
5) **Root cause:** the technical root cause.
6) **Contributing factors:** why it became an incident (missing alert, poor runbook, risky deploy).
7) **What went well / what didn’t:** objective, short.
8) **Action items:** corrective + preventive.

---

## 3. Root cause analysis techniques

### 5 Whys (useful but not always sufficient)
Ask “why” until you reach a **controllable system change**.

Example chain:
1) OOM killed the service
2) memory spike due to huge payload
3) endpoint allowed very large uploads
4) missing validation + missing WAF rule
5) no SLO-based guardrails/tests for payload size

### Contributing factors (where senior answers shine)
Often the incident is not one bug; it’s a stack of gaps:
* no canary
* no circuit breaker
* noisy alerts
* unclear ownership
* missing rollback automation

---

## 4. Action items that actually prevent repeats
Classify actions:
* **Corrective:** fix the bug.
* **Preventive:** reduce the chance of recurrence.
* **Detective:** improve monitoring so you find it earlier.

Action items must have:
* **owner** and **due date**
* clear success criteria
* prioritization aligned with incident severity

---

## 5. Common high-leverage prevention actions
* add input validation and limits
* add canary + automated rollback
* add SLO/burn-rate alerts
* improve runbooks and dashboards
* reduce blast radius (timeouts, bulkheads, feature flags)

## Quick checklist
* Write a timeline, not a narrative
* Separate root cause vs contributing factors
* Create action items with owners + deadlines
* Track actions to completion (otherwise post-mortems don’t work)

Topic 12.3:
Title: SRE Principles
Order: 3

Class 12.3.1:
	Title: Error Budgets & SLOs
	Description: Measuring reliability mathematically.
Content Type: text
Duration: 450 
Order: 1
		Text Content :
 # SRE Fundamentals: SLIs, SLOs, SLAs, and Error Budgets

SRE is about making reliability a measurable product feature.

---

## 1. Terminology (say it crisply in interviews)
* **SLI (Service Level Indicator):** the metric you measure (availability, latency, correctness).
* **SLO (Service Level Objective):** the target for the SLI over a time window.
* **SLA (Service Level Agreement):** external contract with penalties.

Example:
* SLI: “99th percentile latency for GET /checkout.”
* SLO: “99% of requests under 300ms over 30 days.”
* SLA: “99.9% monthly uptime; credits if violated.”

---

## 2. Choosing good SLIs
Great SLIs are:
* user-centric (what users feel),
* measurable and stable,
* resistant to gaming.

Common SLIs:
* **Availability:** successful responses / total.
* **Latency:** p95/p99 of key endpoints.
* **Correctness:** wrong results rate.
* **Durability:** lost writes per million.

---

## 3. Error budgets (the core mechanism)

Error budget is “how much unreliability you can spend.”

Formula:
* `Error Budget = 1 - SLO`

Example:
* SLO 99.9% monthly availability → 0.1% error budget.

Use it to balance velocity vs reliability:
* **Budget healthy:** ship features, accept risk.
* **Budget burning fast:** slow down risky changes.
* **Budget exhausted:** focus on reliability work (or controlled freeze for core systems).

---

## 4. Burn rate alerts (how to alert without noise)
Instead of paging on “availability dropped,” alert on how fast you are consuming budget.

Burn rate concept:
* fast burn means you will violate SLO soon.

Typical practice:
* **multi-window alerts** (fast window + slow window) to avoid flapping.

---

## 5. Practical policy examples
* Canary required for high-risk services.
* Rollbacks automated when error rate crosses threshold.
* Release pauses when burn rate exceeds a limit.

## Quick checklist
* Define SLIs that match user experience
* Set SLOs with clear windows and endpoints
* Use error budgets to make trade-offs explicit
* Alert on burn rate, not raw metrics

Class 12.3.2:
	Title: Toil Reduction
	Description: Automating repetitive work.
Content Type: text
Duration: 350 
Order: 2
		Text Content :
 # Toil Reduction: Turning Ops Pain into Engineering

Toil is the silent scalability killer. If toil grows with usage, your team becomes the bottleneck.

---

## 1. What is toil (precise definition)
Toil is work that is:
1) **manual**,
2) **repetitive**,
3) **tactical** (little enduring value),
4) **linearly scaling** with the system.

Examples:
* manual restarts after predictable failures
* hand-running one-off DB maintenance
* repeating the same incident steps with no automation

Not toil:
* one-time migrations,
* strategic reliability engineering,
* building a self-service platform.

---

## 2. Measuring toil (so you can reduce it)
Track:
* pages per week and page hours
* top recurring incident types
* manual tickets per service
* time spent on repetitive tasks

If you cannot measure it, you cannot improve it.

---

## 3. The 50% rule (why it exists)
If a reliability/platform team spends >50% time on toil:
* automation stops,
* reliability debt grows,
* and incidents increase.

When toil is high, you either:
* stop onboarding new services,
* reduce scope,
* or invest in platform improvements.

---

## 4. Automation ROI (simple model)
Decide what to automate using expected savings.

Rule of thumb:
* If a task happens frequently and is safe to automate, automate it.

Example:
* 5 minutes × 10/day = 50 minutes/day.
* A 2-day script pays back quickly.

---

## 5. High-leverage toil reduction patterns
* **Self-service:** golden paths for deploys, access requests, environments.
* **Auto-remediation:** safe, bounded actions (restart, scale, drain).
* **Better alerting:** fewer pages, more signal.
* **Standardization:** templates for services, dashboards, and runbooks.
* **Reduce handoffs:** clear ownership and on-call boundaries.

Interview tip: connect toil reduction to reliability:
* fewer pages → more time to engineer → fewer incidents.

## Quick checklist
* Define and measure toil
* Keep toil under control with automation + self-service
* Prefer safe auto-remediation over manual runbooks
* Reduce alert noise to reclaim engineering time

—
Module 13:
Title: Technical Precap - Core Concepts at a Glance
Description: A quick-reference guide to all technical topics covered in this course. Perfect for skimming before deep dives or as a study refresher.
Order: 13
Learning Outcomes:
Quickly reference core DevOps concepts and technologies
Identify knowledge gaps across the entire curriculum
Understand relationships between different technologies
Refresh key concepts before interviews
Topic 13.1:
Title: Core Technologies & Concepts Reference
Order: 1

Class 13.1.1:
	Title: Technical Topics Overview
	Description: Comprehensive reference guide for all technical concepts.
Content Type: text
Duration: 600 
Order: 1
		Text Content :

# Technical Precap: Core Concepts at a Glance

This module provides a quick-reference guide to all major technical topics covered in this course. Use this to identify gaps, refresh concepts, or prepare for interview deep dives.

---

## 1. Linux & System Fundamentals

### Process Management
* **Processes vs Threads:** Processes are isolated; threads share memory
* **Process lifecycle:** Fork → Exec → Exit → Reap
* **Signals:** SIGTERM (graceful), SIGKILL (force), SIGCHLD (child exit)
* **Key command:** `ps aux`, `kill`, `jobs`, `bg`, `fg`
* **Interview focus:** Graceful shutdown, zombie processes, signal handling

### Networking Basics
* **TCP/IP stack:** Application → Transport (TCP/UDP) → Internet (IP) → Link
* **Ports & sockets:** Listening sockets accept, connected sockets communicate
* **DNS:** Domain name resolution, record types (A, CNAME, MX, NS)
* **Key metrics:** Latency, throughput, jitter, packet loss
* **Interview focus:** Connection pooling, port conflicts, DNS caching issues

### Storage & Filesystems
* **Filesystem hierarchy:** `/etc` (config), `/var` (logs), `/tmp` (temporary), `/home` (user data)
* **Disk I/O:** IOPS (operations/sec), throughput (bytes/sec), latency (response time)
* **Mounting:** Attaching filesystems to directory trees, mount options (ro, rw, noexec)
* **Key metric:** Inodes (file count limit) vs blocks (size limit)
* **Interview focus:** Disk filling issues, inode exhaustion, fsck recovery

### Permissions & Access Control
* **Unix permissions:** rwx for user, group, others (chmod, chown)
* **Special bits:** SUID (run as owner), SGID (inherit group), sticky (delete only as owner)
* **sudoers file:** Privilege escalation rules, NOPASSWD directives
* **Interview focus:** Least privilege, permission-denied debugging

---

## 2. Scripting & Automation

### Bash & Shell Scripting
* **Variables:** `$var`, arrays, environment variables
* **Control flow:** if/then/else, loops (for, while), case statements
* **String manipulation:** `sed`, `awk`, `grep`, parameter expansion
* **Error handling:** Exit codes ($?), `set -e`, `set -o pipefail`
* **Functions & modularity:** Reusable script functions, argument passing
* **Interview focus:** Script debugging, error handling, production-safe automation

### Python for DevOps
* **Modules:** subprocess (run commands), requests (HTTP), boto3 (AWS)
* **Configuration parsing:** JSON, YAML, INI files
* **Logging & monitoring:** Structured logs, metrics emission
* **Concurrency:** Threading, multiprocessing for parallel tasks
* **Interview focus:** Infrastructure automation, API integration, data processing

### Infrastructure as Code (IaC)
* **Terraform:** HCL syntax, state management, modules, plan/apply workflow
* **CloudFormation:** JSON/YAML templates, stack lifecycle, drift detection
* **Ansible:** YAML playbooks, idempotency, handler patterns
* **Interview focus:** State drift, modularity, testing infrastructure changes

---

## 3. Containerization & Orchestration

### Docker
* **Images:** Layers, Dockerfile syntax, build context, image registry
* **Containers:** Namespaces (pid, net, ipc), cgroups (resource limits), overlay2 filesystem
* **Networking:** bridge (default), host, custom networks, service discovery
* **Volumes:** Bind mounts vs named volumes, volume drivers
* **Interview focus:** Layer caching, image size optimization, container security

### Kubernetes (K8s)
* **Objects:** Pods (smallest unit), Deployments (scaling), Services (network access), ConfigMaps (config)
* **Scheduling:** Node selectors, taints/tolerations, affinity rules
* **Storage:** PVs (persistent volumes), PVCs (claims), storage classes
* **Networking:** CNI plugins, Services (ClusterIP, NodePort, LoadBalancer), Ingress
* **Operators:** Custom Resource Definitions (CRDs), controller pattern
* **Interview focus:** Multi-zone deployments, graceful shutdown, resource limits

---

## 4. Cloud Platforms

### AWS Core Services
* **Compute:** EC2 (VMs), ECS (container management), Lambda (serverless), Fargate (managed containers)
* **Storage:** S3 (object storage), EBS (block storage), EFS (file storage)
* **Networking:** VPC (virtual network), Security Groups (firewalls), ALB/NLB (load balancers), Route 53 (DNS)
* **Databases:** RDS (managed SQL), DynamoDB (NoSQL), Redshift (data warehouse)
* **Messaging:** SQS (queue), SNS (pub/sub), EventBridge (event bus)
* **Interview focus:** High availability across AZs, IAM least privilege, cost optimization

### Google Cloud Platform (GCP)
* **Compute:** Compute Engine (VMs), GKE (Kubernetes), Cloud Run (serverless), App Engine (PaaS)
* **Storage:** Cloud Storage (object storage), Persistent Disks (block), Cloud Filestore (NFS)
* **Databases:** Cloud SQL (managed SQL), Firestore (NoSQL), BigQuery (data warehouse)
* **Interview focus:** GKE cluster management, IAM bindings, networking abstractions

### Azure
* **Compute:** VMs, AKS (Kubernetes), Container Instances, Azure Functions
* **Storage:** Azure Storage (blobs, disks, files), managed disks
* **Networking:** VNets, NSGs, Azure Load Balancer, Application Gateway
* **Interview focus:** RBAC, subscriptions/resource groups, disaster recovery

---

## 5. CI/CD & Deployment

### CI/CD Concepts
* **Continuous Integration:** Automated tests on every commit, fast feedback
* **Continuous Deployment:** Automated release to production on test pass
* **Continuous Delivery:** Ready to release, manual trigger to production
* **Key metrics:** Build time, test coverage, deployment frequency, MTTR
* **Interview focus:** Failure detection, rollback strategy, dependency management

### GitOps & Deployment Strategies
* **GitOps:** Git as single source of truth, automated sync to production
* **Blue-Green:** Two identical environments, instant cutover, instant rollback
* **Canary:** Route small % of traffic to new version, monitor, gradually increase
* **Rolling:** Gradually replace old instances with new, zero downtime
* **Feature flags:** Toggle features without deployment
* **Interview focus:** Rollback mechanisms, monitoring during deployment

### Popular CI/CD Tools
* **Jenkins:** Pipeline as Code, extensive plugin ecosystem, self-hosted
* **GitLab CI/CD:** YAML-based, integrated with version control
* **GitHub Actions:** Workflow automation, integrated with GitHub
* **ArgoCD:** GitOps for Kubernetes, declarative, automated sync
* **Interview focus:** Pipeline optimization, secret management, artifact handling

---

## 6. Monitoring & Observability

### Metrics
* **System metrics:** CPU, memory, disk, network I/O
* **Application metrics:** Request rate, latency (p50, p95, p99), error rate
* **Business metrics:** Conversion rate, revenue, user engagement
* **Collection:** Prometheus (scrape model), StatsD (push), CloudWatch
* **Interview focus:** Metric cardinality, retention policies, alerting thresholds

### Logging
* **Log levels:** DEBUG, INFO, WARN, ERROR, FATAL
* **Structured logging:** JSON format with key-value pairs for better parsing
* **Aggregation:** ELK Stack (Elasticsearch, Logstash, Kibana), Splunk, Datadog
* **Parsing & filtering:** Grok patterns, log processing
* **Interview focus:** Log retention costs, PII masking, searchability

### Distributed Tracing
* **Span concept:** Trace (request) → Spans (service steps) → timeline
* **Sampling:** Full trace vs 1% sample to control overhead
* **Tools:** Jaeger, Zipkin, Datadog, AWS X-Ray
* **Interview focus:** Service latency identification, dependency mapping

### Alerting & On-Call
* **SLOs (Service Level Objectives):** Target availability percentage
* **Error budget:** Allowed downtime before SLO breach
* **Alert rules:** Threshold (static), anomaly (ML-based), composite
* **On-call rotation:** Escalation policies, page acknowledgment, incident commander
* **Interview focus:** Alert fatigue reduction, runbook usefulness

---

## 7. Reliability & Resilience

### High Availability
* **Redundancy:** Multiple instances across zones, active-active, active-passive
* **Load balancing:** Distribute traffic, health checks, connection draining
* **Circuit breaker:** Stop requests to failing service, recover gracefully
* **Retry logic:** Exponential backoff, jitter, max retries
* **Interview focus:** Single points of failure, graceful degradation

### Disaster Recovery
* **RTO (Recovery Time Objective):** Max acceptable downtime
* **RPO (Recovery Point Objective):** Max acceptable data loss
* **Backup strategy:** Full vs incremental, retention periods, testing restores
* **Disaster recovery plan:** Automated failover, traffic rerouting, data sync
* **Interview focus:** Testing DR, multi-region setup, runbook documentation

### Capacity Planning
* **Forecasting:** Historical growth, seasonal patterns, marketing campaigns
* **Headroom:** Reserve capacity for traffic spikes (usually 30-50%)
* **Scaling triggers:** CPU %, memory %, request rate, custom metrics
* **Cost vs performance:** Right-sizing instances, reserved capacity
* **Interview focus:** Scaling time, cost optimization, predictive scaling

---

## 8. Security

### Network Security
* **Firewalls:** Inbound/outbound rules, stateful inspection
* **Network segmentation:** DMZ, private subnets, security groups
* **DDoS protection:** Rate limiting, WAF (Web Application Firewall), traffic filtering
* **VPN & encryption:** TLS/SSL, certificate management, key rotation
* **Interview focus:** Zero trust architecture, least privilege access

### Application Security
* **OWASP Top 10:** Injection, broken auth, sensitive data exposure, XXE, CSRF, etc.
* **Secret management:** Vault, AWS Secrets Manager, encrypted config
* **Code scanning:** SAST (static analysis), DAST (dynamic analysis), dependency scanning
* **Interview focus:** Secret rotation, secure defaults, compliance (SOC2, PCI-DSS)

### IAM (Identity & Access Management)
* **Authentication:** Verify who you are (user/pass, 2FA, SSO)
* **Authorization:** Verify what you can do (role-based, attribute-based)
* **Service accounts:** Machine-to-machine authentication (keys, tokens)
* **Audit logging:** Track all access and changes
* **Interview focus:** Principle of least privilege, IAM policy review

---

## 9. Database Fundamentals

### Relational Databases (SQL)
* **ACID:** Atomicity, Consistency, Isolation, Durability
* **Normalization:** 1NF, 2NF, 3NF to reduce redundancy
* **Indexing:** B-tree indexes on frequently queried columns
* **Query optimization:** Explain plans, index selection, query rewriting
* **Backup & recovery:** WAL (Write-Ahead Logging), point-in-time recovery
* **Interview focus:** Replication (master-slave), sharding strategies

### NoSQL Databases
* **Key-value stores:** Redis, Memcached for caching, session storage
* **Document stores:** MongoDB, CouchDB with JSON-like documents
* **Time-series databases:** InfluxDB, Prometheus for metrics storage
* **Graph databases:** Neo4j for relationship data
* **Interview focus:** CAP theorem, eventual consistency, schema flexibility

### Data Warehouses & Lakes
* **Column storage:** Optimized for analytical queries on large datasets
* **Data transformation:** ETL (extract, transform, load), ELT patterns
* **Schema:** Star schema, snowflake schema for analytics
* **Interview focus:** Query performance, cost optimization, data governance

---

## 10. Incident Response & Troubleshooting

### Incident Lifecycle
* **Detection:** Alerts fire, customer report, metrics anomaly
* **Triage:** Severity assessment, initial impact estimation
* **Mitigation:** Quick fixes, temporary workarounds, service restoration
* **Resolution:** Root cause fix, permanent solution deployment
* **Postmortem:** Blameless analysis, action items, learning documentation
* **Interview focus:** Communication, decision-making under pressure

### Debugging Methodology
* **Gather data:** Logs, metrics, traces, customer reports
* **Form hypothesis:** What could cause this symptom?
* **Test hypothesis:** Change variables, watch for effect
* **Iterate:** If wrong, update hypothesis and repeat
* **Verify fix:** Confirm issue gone, monitor for regression
* **Interview focus:** Systematic approach, avoiding confirmation bias

### Common Failure Modes
* **Resource exhaustion:** CPU, memory, disk, connection pools
* **Configuration drift:** Servers diverging from desired state
* **Cascading failures:** One service failure triggering others
* **Network issues:** DNS failures, packet loss, latency spikes
* **Interview focus:** Detecting early, monitoring signals, preventive measures

---

## Quick Study Tips

1. **Review by category:** Spend 5 mins on each section to refresh memory
2. **Deep dive systematically:** After identifying gaps, read the full modules
3. **Practice hands-on:** Don't just read—build and break things
4. **Connect concepts:** Understand how networking affects Kubernetes, how monitoring helps reliability, etc.
5. **Interview simulation:** Use this as a mental model during system design discussions

---
Module 14:
Title: Problem-Solving & System Design Interviews
Description: Prepare for the toughest interview rounds. Master system design thinking, troubleshooting methodologies, and complex scenario handling.
Order: 14
Learning Outcomes:
Approach system design problems systematically
Debug production issues methodically
Communicate technical decisions clearly
Handle ambiguous requirements

Topic 14.1:
Title: System Design Interview Approach
Order: 1

Class 14.1.1:
	Title: System Design Framework
	Description: The 45-minute structure and Capacity Estimation.
Content Type: text
Duration: 500 
Order: 1
		Text Content :
# The 45-Minute Drill: System Design

## 1. The Structure (Non-Negotiable)
You cannot wing a system design interview. You need a repeatable mental framework.

**0–5 mins: Requirements Gathering (Most Important)**
Never start designing before this.
* Functional: What the system must do  
  Example: Users can upload photos.
* Non-functional: How well it must do it  
  Example: 99.99% availability, <200ms latency, global users.

If you skip this, you risk designing a Ferrari when they asked for a bicycle.

---

**5–10 mins: Capacity Estimation (Back-of-the-Envelope)**
Show that you can reason about scale.
* Example:  
  1M DAU × 10 requests/day × 10 KB/request  
  = 100 GB/day of ingress traffic
* Key outcome:  
  Do we need sharding, caching, or async processing?

The numbers don’t need to be perfect. The thinking does.

---

**10–15 mins: High-Level Design (The Big Boxes)**
Draw the core components and traffic flow.
User → Load Balancer → App Servers → Cache → Database → Object Storage

At this stage:
* No internal class diagrams
* No tuning knobs
* Just responsibilities and data flow

---

**15–40 mins: Deep Dive (Where Interviews Are Won)**
The interviewer will attack one component.

Examples:
* Database goes down → Read replicas? Multi-AZ? Failover?
* Cache hot keys → Key sharding? TTL jitter?
* Traffic spike → Auto Scaling? Queue-based load leveling?

Answer with trade-offs, not absolutes.

---

**40–45 mins: Wrap-Up**
Summarize clearly:
* Bottlenecks
* Failure points
* Why your choices make sense for *these* requirements

This shows ownership-level thinking.

---

## 2. Capacity Estimation Cheatsheet
Memorize these to save time and sound confident.

* 1M requests/day ≈ 12 requests/sec  
* 100M requests/day ≈ 1.2k requests/sec  
* 1B requests/day ≈ 12k requests/sec  

Storage rules of thumb:
* 1 character ≈ 1 byte
* Integer ≈ 4 bytes
* UUID ≈ 16 bytes

Precision is less important than order-of-magnitude correctness.

---

## 3. The C4 Model (How to Draw Clean Diagrams)
Avoid spaghetti diagrams by zooming levels.

* Context: User → System  
* Container: App → DB → Cache → Queue  
* Component: API layer → Service layer → Data access layer  

Interview tip:
Start at Context, then zoom in only when asked.

---

### Final Interview Rule
A strong system design interview is not about drawing more boxes.  
It is about asking better questions, reasoning under constraints, and defending trade-offs.


---

Class 14.1.2:
	Title: Common System Design Problems
	Description: URL Shortener, Distributed Cache, and CI/CD.
Content Type: text
Duration: 600 
Order: 2
		Text Content :
# Standard Design Patterns

## 1. Design a URL Shortener (TinyURL)

This is one of the most common system design questions, and many candidates fail it by focusing on the wrong thing.

**The Trap**  
Candidates obsess over the hashing algorithm. That is not the real challenge.

**The Real Test**  
URL shorteners have:
* Extremely high **read-to-write ratio** (often 100:1 or more)
* Simple access patterns (`GET short_id`)
* Potentially massive scale (billions of redirects)

Your design must optimize for fast reads and efficient storage.

**ID Generation**
You need a short, unique, URL-safe identifier.
* Hashing (MD5/SHA256):
  * Pros: Deterministic
  * Cons: Long output, collisions need handling, inefficient for URLs
* Base62 Encoding (Preferred):
  * Convert an auto-incrementing ID to `[a-zA-Z0-9]`
  * Produces short, predictable-length URLs
  * Easy to scale horizontally

**Database Choice**
This is a classic Key-Value problem.
* Key: `short_id`
* Value: `original_url`
* No joins, no complex queries

A NoSQL store (DynamoDB, Cassandra, Redis) fits better than SQL.

**Concurrency Problem**
Two users shorten the same URL at the same time.
* Option 1: Allow duplicates (simplest, acceptable in most systems)
* Option 2: Deduplicate using a hash lookup (adds read latency and complexity)

In interviews, always call out the trade-off.

---

## 2. Design a Rate Limiter

This problem tests your understanding of **state, consistency, and scale**.

**Requirement**
Allow 10 requests per second per IP (or per user/token).

This immediately implies:
* You must track request counts
* Across multiple servers
* In real time

**Why In-Memory Counters Fail**
If you store counters inside the application:
* Each server has a different count
* The user can bypass limits by hitting another server

This breaks correctness.

**Algorithms**
* Token Bucket:
  * Allows bursts
  * Widely used for APIs
  * Best balance of simplicity and flexibility
* Leaky Bucket:
  * Smooths traffic
  * Not good for bursty workloads
* Sliding Window:
  * Most accurate
  * Most expensive (memory + computation)

In interviews, Token Bucket is usually the safest answer.

**Storage Layer**
Redis is the default choice because it provides:
* Atomic increments (`INCR`)
* TTLs for automatic window expiry
* Sub-millisecond latency

Without Redis (or equivalent), a distributed rate limiter is impossible.

---

## 3. Design a CI/CD Pipeline

This is the DevOps-focused system design question.

**Goal**
Convert source code into a running production system:
* Reliably
* Reproducibly
* With fast rollback

**Core Components**
* Source Control:
  * Git is the source of truth
  * Every deployment maps to a commit SHA
* Build Server:
  * Jenkins (Master schedules, Agents execute)
  * Or GitHub Actions runners
* Artifact Store:
  * Store immutable outputs (Docker images, JARs)
  * S3, Artifactory, or ECR
* Deployment Strategy:
  * Blue/Green or Rolling
  * Traffic switching via Load Balancer or DNS

**The Critical Question: Rollback**
“How do you rollback safely?”

Correct answer:
* Never rebuild old code
* Redeploy the **previous immutable artifact**
* Flip traffic back (Blue/Green) or redeploy last known-good version

If you rebuild on rollback, you no longer know what you are running.

---

### Interview Signal to Aim For
Strong candidates don’t just list components.
They explain:
* Why each component exists
* What breaks if you remove it
* The trade-offs under scale and failure

That is what turns a “correct” answer into a “hire” signal.


---

Topic 14.2:
Title: Troubleshooting Scenarios
Order: 2

Class 14.2.1:
	Title: Troubleshooting Methodology
	Description: The Systematic Approach to Debugging.
Content Type: text
Duration: 450 
Order: 1
		Text Content :
 # The Art of Debugging

## 1. The Method: Divide and Conquer
When the site is down, don't panic. Shrink the problem space.
1.  **Scope:** Is it Global? Or just one Region? Just one User?
2.  **Recent Changes:** "What changed in the last hour?" (Code deploy? Config change? Traffic spike?).
3.  **Check the Edges:** Can I ping the Load Balancer? Yes. Can I ping the App? No. -> **Problem is between LB and App.**

---

## 2. The Tools
* **`curl -v`:** Is it a DNS issue? (Name not resolving). Is it a 500 error?
* **`top` / `htop`:** Is the CPU at 100%?
* **`df -h`:** Is the Disk full? (The silent killer).
* **`netstat -tulpn`:** Is the port actually open?

---

## 3. The Hypothesis Loop
* **Hypothesis:** "The DB is slow."
* **Test:** Check Slow Query Log.
* **Result:** Log is empty. Hypothesis rejected.
* **New Hypothesis:** "The Network is dropping packets."
* **Test:** Run `mtr`.

---

Class 14.2.2:
	Title: Real-World Scenarios
	Description: Solving common production outages.
Content Type: text
Duration: 500 
Order: 2
		Text Content :


# War Stories: Real Scenarios

These scenarios test whether you can **debug under pressure**, not whether you know commands.
Interviewers look for structured thinking, signal isolation, and root-cause reasoning.
 
---

## 1. Scenario: "The Website is Down" (502 Bad Gateway)

**What 502 Really Means**  
A `502 Bad Gateway` from an ALB/Nginx does **not** mean the Load Balancer is broken.
It means:
> The Load Balancer is healthy, but it cannot get a valid response from the backend.

This immediately narrows the blast radius to **Backend Health**.

**Debug Flow (Outside → Inside)**

1. **Check Load Balancer Health Checks**
   * Are targets marked `Unhealthy`?
   * Is the health check path (`/health`, `/status`) correct?
   * Did someone deploy a change that broke the health endpoint?

   If all targets are unhealthy, traffic will fail even if the app is partially running.

2. **Access a Backend Instance**
   * SSH or SSM into one instance.
   * Check whether the process is alive:
     ```bash
     ps aux | grep java
     ```
   If the process is missing, this is no longer a networking problem.

3. **Check Application Logs**
   * Look for startup failures, not request errors.
   * Common patterns:
     * `Connection refused`
     * `Authentication failed`
     * `Timeout while connecting to DB`

4. **Identify the Root Cause**
   * In this case:
     * Database password expired
     * App failed during startup
     * Health check endpoint never came up
     * Load Balancer correctly removed the instance

**Key Insight**
The Load Balancer did its job perfectly.
The failure was **configuration drift** in secrets management.

---

## 2. Scenario: "Kubernetes Pods in CrashLoopBackOff"

**What CrashLoopBackOff Means**  
Kubernetes is not broken.
It is telling you:
> The container starts, crashes, restarts, and repeats.

This means the failure is **inside the container**, not in scheduling or networking.

**Debug Flow**

1. **Confirm the Status**
   `kubectl get pods`

`CrashLoopBackOff` indicates repeated failures after startup.

2. **Inspect Container Logs**

   ```bash
   kubectl logs <pod-name>
   ```

   Logs usually reveal:

   * Application exceptions
   * Config errors
   * Resource exhaustion

3. **Check Pod Events**

   ```bash
   kubectl describe pod <pod-name>
   ```

   Look for:

   * `OOMKilled`
   * `Exit Code 137`
   * Restart count rapidly increasing

4. **Root Cause Analysis**

   * The container exceeded its memory limit
   * Kernel OOM killer terminated the process
   * Kubernetes restarted it automatically

5. **Fix Options**

   * Increase `resources.limits.memory`
   * Profile memory usage
   * Fix memory leaks
   * Add proper JVM heap sizing (for Java apps)

**Key Insight**
If you only increase memory without understanding usage, you are masking the problem.
Good engineers fix **both** symptoms and cause.

---

## 3. Scenario: "High Cloud Bill"

**Why This Is a Production Emergency**
Cost spikes are silent failures.
They do not page you — finance does.

**Investigation Flow (Top-Down)**

1. **Start with Cost Explorer**

   * Identify which service spiked
   * Example suspects:

     * EC2
     * S3
     * NAT Gateway
     * Data Transfer

2. **Zoom Into the Cost Driver**

   * NAT Gateway charges are often:

     * Data processed
     * Data transfer out
   * Large spikes usually mean **misrouted traffic**

3. **Enable VPC Flow Logs**

   * Identify source and destination traffic
   * Look for large volumes from:

     * Private subnets
     * Application nodes
     * To public AWS endpoints

4. **Root Cause**

   * Application in a private subnet downloads data from S3
   * Traffic exits via NAT Gateway
   * NAT Gateway charges per GB
   * This is unnecessary for AWS services

5. **The Correct Fix**

   * Create an **S3 VPC Gateway Endpoint**
   * Route S3 traffic privately within AWS
   * Eliminates NAT data charges entirely

**Key Insight**
This was not a scaling issue.
It was a **network architecture mistake**.

---

## The Interview Pattern They Look For

Strong candidates:

* Start broad, then narrow
* Validate assumptions with signals
* Identify whether the failure is:

  * Infra
  * Config
  * App
  * Architecture
* Fix root cause, not just symptoms

Weak candidates:

* Jump to commands randomly
* Restart everything
* Increase limits blindly

---

### Final Rule of War Stories

Every outage teaches you **where your mental model was wrong**.
That lesson is more valuable than the fix itself.


---

Topic 14.3:
Title: Practice System Design Questions
Order: 3

Class 14.3.1:
	Title: System Design - Mock Interview
	Description: Practice prompts for self-study.
Content Type: text
Duration: 600 
Order: 1
		Text Content :
# Mock Interview Prompts
 
Use these prompts to practice:
- Drawing diagrams
- Explaining trade-offs
- Handling follow-up questions
- Thinking out loud under pressure

---

## Problem 1: Design a Log Aggregation System

### Requirements
- 10,000 services producing logs
- Logs must be searchable within 1 minute
- System must not lose logs during outages or traffic spikes

This immediately implies:
- High write throughput
- Eventual consistency is acceptable
- Durability is more important than strict ordering

### High-Level Architecture
- **Producers:** Application services
- **Buffer:** Kafka
- **Processing:** Logstash
- **Search:** Elasticsearch
- **Long-term Archive:** S3

### Data Flow
1. Applications push logs asynchronously
2. Logs go into Kafka topics (partitioned by service or log type)
3. Logstash consumes from Kafka
4. Logstash parses/enriches logs and indexes them into Elasticsearch
5. Older logs are rolled to S3 for cheap storage

### Why Kafka Is Mandatory
Kafka decouples log producers from consumers.
If Elasticsearch slows down or crashes:
- Producers continue writing to Kafka
- No log loss
- Consumers catch up later

### Handling a Sudden Log Spike (During an Outage)
- Kafka absorbs the spike via partitions
- Logstash scales horizontally
- Elasticsearch indexing may lag, but data is safe
- Search freshness degrades temporarily, not correctness

### Follow-Up Questions to Practice
- What happens if Kafka disk fills up?
- How do you decide partition count?
- How do you handle schema changes in logs?

---

## Problem 2: Design Instagram Image Storage

### Requirements
- Users upload images
- Other users view images
- Low latency globally
- High read-to-write ratio

This is a **content delivery** problem, not a database problem.

### High-Level Architecture
- **Upload API:** Application servers
- **Storage:** Object Storage (S3)
- **Metadata Store:** SQL DB (sharded)
- **Delivery:** CDN (CloudFront)

### Upload Flow
1. Client uploads image
2. App server validates and stores image in S3
3. Metadata (user_id, image_url, timestamp) stored in DB
4. S3 object is immutable after upload

### Read / View Flow
1. Client requests image
2. CDN checks edge cache
3. If cache hit → serve immediately
4. If cache miss → fetch from S3, cache at edge

### Latency Optimization for Global Users
- Images stored in one region
- CDN edge nodes serve users close to them
- Only first request per region hits S3

### Why Metadata Is in SQL
- Need strong consistency for likes, comments, ownership
- Query patterns: user → images
- Sharding by user_id avoids hotspots

### Follow-Up Questions to Practice
- How do you handle image resizing?
- How do you invalidate CDN cache?
- What happens if S3 is slow?

---

## Problem 3: Design a Secrets Management System

### Requirements
- Secure storage for API keys, DB passwords
- Automatic rotation
- Auditability
- No secrets in code or Git

### High-Level Architecture
- **Secret Store:** HashiCorp Vault
- **Auth:** IAM Roles / Kubernetes Service Accounts
- **Encryption:** At rest and in transit
- **Audit Logs:** Mandatory

### Secret Access Flow
1. Application starts
2. App authenticates to Vault using:
   - IAM Role (cloud VM)
   - Service Account (Kubernetes)
3. Vault verifies identity
4. Vault issues a short-lived token
5. App fetches secrets dynamically

### Secret Rotation
- Vault rotates secrets automatically
- Database credentials updated
- Old credentials revoked
- Apps fetch fresh secrets without redeploy

### Why Identity Matters
The hardest problem is not encryption.
It is answering:
> “How does Vault know this app is allowed to read this secret?”

Answer:
- Strong workload identity (not IP-based trust)

### Follow-Up Questions to Practice
- What if Vault is down?
- How do you cache secrets safely?
- How do you prevent secret sprawl?

---

## Problem 4: Design a Multi-Region Active-Active Architecture

### Requirements
- Zero downtime if a region fails
- Users served from nearest region
- Writes allowed in multiple regions

### High-Level Architecture
- **Traffic Routing:** Route53 Latency-based routing
- **Compute:** Identical stacks in multiple regions
- **Database:** DynamoDB Global Tables
- **Replication:** Automatic, multi-master

### Read Flow
- User routed to closest healthy region
- Reads served locally
- Low latency

### Write Flow
- Writes can occur in any region
- Data replicated to all regions asynchronously

### The Hard Problem: Write Conflicts
If the same record is updated in two regions:
- Conflict resolution is required

### Common Strategies
- **Last Writer Wins:** Simple, but may lose data
- **Versioning:** Detect conflicts and resolve at app layer
- **User Pinning:** Route a user consistently to one region to avoid conflicts

In interviews, explicitly acknowledge this trade-off.

### Follow-Up Questions to Practice
- How do you handle partial region outages?
- How do you test failover?
- How do you handle schema migrations?

---

## How to Practice Effectively

For each problem:
1. State assumptions clearly
2. Draw the high-level diagram first
3. Explain data flow
4. Call out failure scenarios
5. Discuss trade-offs

If you can do this calmly, you are already operating at **senior+ level**.


---

Module 15:
Title: Behavioral Interviews & Career Development
Description: Master the behavioral interview using STAR framework. Learn to showcase your DevOps experience, handle difficult questions, and negotiate offers.
Order: 15
Learning Outcomes:
Use STAR framework effectively
Prepare compelling DevOps stories
Handle behavioral questions confidently
Navigate salary negotiations

Topic 15.1:
Title: Behavioral Interview Preparation
Order: 1

Class 15.1.1:
	Title: The STAR Framework for DevOps
	Description: Structuring your stories for maximum impact.
Content Type: text
Duration: 450 
Order: 1
		Text Content :
 # The STAR Framework: Storytelling for Engineers

## 1. Why Great Engineers Fail Interviews
You can be a Kubernetes wizard, but if you cannot explain *why* you built something or *how* you handled a conflict, you will be rejected. Companies hire humans, not robots.

---

## 2. The STAR Method
Do not ramble. Structure every answer using this 4-step format.

* **S - Situation (10%):** Set the context. "We had a monolithic app that took 45 minutes to deploy, causing frequent timeouts."
* **T - Task (10%):** Define your role. "My goal was to modernize the pipeline and reduce deployment time to under 10 minutes."
* **A - Action (60%):** The meat of the story. **Use "I", not "We".**
    * "**I** containerized the application using Docker."
    * "**I** wrote a Jenkins pipeline with parallel stages."
    * "**I** implemented caching to speed up the build."
* **R - Result (20%):** The happy ending. Quantify it.
    * "Deployment time dropped from 45 mins to 8 mins (**82% reduction**). The team now deploys 5 times a day instead of once a week."

---

## 3. The "Hero" Trap
Avoid saying "We did this." The interviewer isn't hiring your team; they are hiring **you**. Be specific about your contribution.
* *Weak:* "We moved to AWS."
* *Strong:* "I designed the VPC architecture and wrote the Terraform scripts to migrate our database layer to RDS."

Class 15.1.2:
	Title: Key Behavioral Questions
	Description: Preparing your "Stories Bank".
Content Type: text
Duration: 600 
Order: 2
		Text Content :
 # The "Stories Bank" Strategy

Don't memorize answers. Memorize **5 Core Stories** that can be adapted to answer any question.

---

## 1. The "Production Failure" Story
* **Prompt:** "Tell me about a time you broke production."
* **What they want:** Honesty, Ownership, and Learning.
* **Key Points:**
    * "I accidentally deleted a security group." (Own it immediately).
    * "I restored it within 2 minutes." (Bias for Action).
    * "I wrote a Terraform Sentinel policy to prevent anyone from doing it again." (Prevent recurrence).

---

## 2. The "Conflict" Story
* **Prompt:** "Tell me about a time you disagreed with a Developer."
* **What they want:** Empathy and Data-Driven persuasion.
* **Key Points:**
    * "The Dev wanted to use MongoDB. I wanted Postgres."
    * "I didn't argue opinions. I benchmarked both."
    * "I showed that Postgres handled our relational data 2x faster."
    * "The Dev agreed, and we moved forward."

---

## 3. The "Automation" Story
* **Prompt:** "Tell me about a process you improved."
* **What they want:** ROI (Return on Investment).
* **Key Points:**
    * "On-boarding a new dev took 3 days."
    * "I wrote an Ansible script to provision dev environments."
    * "On-boarding now takes 15 minutes."

---

## 4. The "Pushback" Story
* **Prompt:** "Tell me about a time you said No."
* **What they want:** Prioritization and protecting the system.
* **Key Points:**
    * "Management wanted to deploy on Friday afternoon."
    * "I explained the risk to our SLA if things went wrong."
    * "We agreed to deploy Monday morning instead."

Topic 15.2:
Title: Company-Specific Behavioral Frameworks
Order: 2

Class 15.2.1:
	Title: Amazon Leadership Principles
	Description: Cracking the hardest behavioral interview in tech.
Content Type: text
Duration: 450 
Order: 1
		Text Content :
 # Amazon Leadership Principles (LPs)

Amazon is unique. Their interview is 50% Technical and 50% LPs. You **must** study these.

---

## 1. Customer Obsession
"Start with the customer and work backwards."
* *DevOps Angle:* "I noticed customers were seeing 500 errors during deployments. Even though it was easier for *us* to deploy with downtime, I implemented Blue/Green deployment to ensure zero downtime for *them*."

---

## 2. Ownership
"Leaders never say 'that's not my job'."
* *DevOps Angle:* "The database team was asleep during an incident. I didn't wait. I jumped in, read the logs, and restarted the service myself to unblock the site."

---

## 3. Bias for Action
"Speed matters."
* *DevOps Angle:* "We had a memory leak. Instead of spending 3 days finding the perfect root cause, I immediately set up an auto-restart script to keep the site alive while we investigated."

---

## 4. Dive Deep
"Operate at all levels."
* *DevOps Angle:* "The app was slow. I didn't just add more servers. I used `strace` to trace the system calls and found a disk I/O bottleneck."

Class 15.2.2:
	Title: Google's Googleyness
	Description: Psychological safety and "Googleyness".
Content Type: text
Duration: 350 
Order: 2
		Text Content :
 # Google & The SRE Mindset

## 1. "Googleyness"
It's not just "culture fit." It measures:
* **Thriving in Ambiguity:** Can you move forward when you don't have all the answers?
* **Valuing Feedback:** Can you take criticism without getting defensive?
* **Challenging the Status Quo:** Do you ask "Why do we do it this way?"

---

## 2. Blamelessness
Google SRE culture is built on **Blameless Post-Mortems**.
* *Interview Tip:* Never blame a person. Blame the process.
* *Bad:* "Dave pushed a bad config."
* *Good:* "Our CI system lacked a validator for that config."

Topic 15.3:
Title: Career Development
Order: 3

Class 15.3.1:
	Title: Resume & Portfolio Building
	Description: How to get the interview.
Content Type: text
Duration: 400 
Order: 1
		Text Content :
 # The DevOps Resume Checklist

## 1. Quantify Impact (XYZ Formula)
Google Recruiter Laszlo Bock's formula: "Accomplished [X] as measured by [Y], by doing [Z]."
* *Bad:* "Managed AWS infrastructure."
* *Good:* "Reduced cloud costs by **20% (X)** ($50k/year) **(Y)** by implementing Spot Instances and auto-scaling policies **(Z)**."

---

## 2. Skills Section
Group your skills logically. Don't just list words.
* **Cloud:** AWS (EC2, S3, RDS, VPC), GCP.
* **Containerization:** Docker, Kubernetes (EKS, Helm).
* **IaC:** Terraform, Ansible.
* **CI/CD:** Jenkins, GitHub Actions.

---

## 3. The Portfolio
DevOps is hard to "show," but you can:
* **GitHub:** Have a repo with a clean `README.md` showing a 3-tier architecture (Terraform code + Ansible playbook).
* **Blog:** Write *one* article: "How I debugged a slow query" or "My journey to CKA certification."

Class 15.3.2:
	Title: Offer Negotiation
	Description: Getting what you are worth.
Content Type: text
Duration: 400 
Order: 2
		Text Content :
 # Negotiating the Offer

## 1. The Golden Rule
**"He who speaks the first number loses."**
* *Recruiter:* "What are your salary expectations?"
* *You:* "I'm focused on finding the right role first. What is the budget range for this position?"

---

## 2. Total Compensation (TC)
Don't just look at the Base Salary.
* **Base:** Monthly cash.
* **Bonus:** Yearly cash (variable).
* **Equity (RSUs/Options):** Stock. In Tech, this is often 30-50% of your pay.
* **Sign-on:** One-time cash. *This is the easiest thing to negotiate up.*

---

## 3. Leverage
You have zero leverage without a second offer.
* *Strategy:* Line up interviews so you get offers at the same time.
* *Script:* "I really like Company A, but Company B offered me $X. If you can match that, I will sign today."


Module 16:
Title: Final Preparation & Mock Interviews
Description: Comprehensive final preparation with full mock interview loops, common mistakes to avoid, and last-minute tips.
Order: 16
Learning Outcomes:
Complete full mock interviews
Identify and fix weak areas
Build interview confidence
Fine-tune communication skills

Topic 16.1:
Title: Mock Interview Practice
Order: 1

Class 16.1.1:
	Title: Full Mock Interview Loop
	Description: Complete interview simulation and self-evaluation.
Content Type: text
Duration: 600 
Order: 1
		Text Content :
 # The Complete Mock Interview Loop

## 1. Complete Interview Simulation
The best way to prepare is to simulate the actual interview experience. Here's a realistic 4-round interview loop for a Senior DevOps Engineer role.

---

### Round 1: Technical Screen (45 minutes)
**Format:** Live problem-solving with an interviewer

**Sample Questions:**
* "You have a web application running on 10 EC2 instances behind an ALB. Users are reporting intermittent 502 errors. Walk me through your debugging process."
* "Write a Bash script that monitors disk usage and sends an alert if any partition exceeds 80%."
* "Explain the difference between a Docker container and a VM. When would you choose one over the other?"

**What They're Testing:**
* Systematic troubleshooting methodology
* Command-line proficiency
* Understanding of fundamentals
* Communication clarity

**Self-Evaluation Checklist:**
- [ ] Did I ask clarifying questions before starting?
- [ ] Did I explain my thought process out loud?
- [ ] Did I consider multiple possibilities?
- [ ] Did I arrive at a solution within the time limit?
- [ ] Did I test my solution (for coding questions)?

---

### Round 2: System Design (60 minutes)
**Prompt:** "Design a CI/CD pipeline for a microservices application with 20 services. The pipeline should support automated testing, security scanning, and blue-green deployments."

**Expected Approach:**
1. **Requirements Gathering (5 mins)**
    * "How many deployments per day?"
    * "What's the criticality? Can we have brief downtime?"
    * "What's the team size?"

2. **High-Level Design (10 mins)**
    * Draw: GitHub -> Jenkins -> Docker Registry -> Kubernetes
    * Show parallel testing stages
    * Include security scanning gates

3. **Deep Dive (35 mins)**
    * **Artifact Management:** "How do we version Docker images?"
    * **Rollback Strategy:** "What happens if deployment fails?"
    * **Secret Management:** "How do we handle DB passwords?"
    * **Monitoring:** "How do we know if the new version is healthy?"

4. **Trade-offs & Bottlenecks (10 mins)**
    * "Jenkins Master could become a bottleneck. We might need a build queue or Kubernetes-based agents."
    * "Blue-green requires 2x infrastructure cost during deployment."

**Self-Evaluation Checklist:**
- [ ] Did I spend time understanding requirements first?
- [ ] Did I draw a clear diagram?
- [ ] Did I discuss scalability and failure scenarios?
- [ ] Did I quantify things (numbers, capacity)?
- [ ] Did I acknowledge trade-offs?

---

### Round 3: Troubleshooting Scenario (45 minutes)
**Scenario:** "A Kubernetes pod is stuck in `CrashLoopBackOff`. The application was working yesterday, and no code changes were deployed. Walk me through your investigation."

**Expected Methodology:**
1. **Gather Information**
    * `kubectl describe pod <name>` - Check events
    * `kubectl logs <name>` - Check application logs
    * `kubectl get pod <name> -o yaml` - Check configuration

2. **Form Hypothesis**
    * "The events show 'OOMKilled'. Hypothesis: Memory limit is too low or there's a memory leak."
    * "Alternative: A dependency (database, external API) is down."

3. **Test Hypothesis**
    * Check `resources.limits.memory` in the pod spec
    * Check if other pods are affected (is it app-wide or just one pod?)
    * Try to exec into the pod (if it stays up long enough): `kubectl exec -it <pod> -- /bin/sh`

4. **Propose Solution**
    * Short-term: Increase memory limit
    * Long-term: Profile the application to find the memory leak

**Common Follow-ups:**
* "How would you prevent this in the future?" (Answer: Better testing, memory profiling in staging)
* "What if increasing memory doesn't fix it?" (Answer: Check for external dependencies, network issues, DNS problems)

**Self-Evaluation Checklist:**
- [ ] Did I follow a systematic debugging process?
- [ ] Did I consider multiple root causes?
- [ ] Did I use the right tools/commands?
- [ ] Did I propose both immediate fixes and long-term solutions?
- [ ] Did I remain calm and methodical?

---

### Round 4: Behavioral (45 minutes)
**Sample Questions:**
1. "Tell me about a time when you had to make a decision between speed and reliability."
2. "Describe a situation where you disagreed with your manager about a technical decision."
3. "Tell me about the most challenging production incident you've handled."
4. "Give me an example of a time you automated a manual process. What was the impact?"

**Evaluation Criteria:**
* **STAR Structure:** Did you follow Situation, Task, Action, Result?
* **Ownership:** Did you say "I" instead of "We"?
* **Quantification:** Did you provide numbers (time saved, cost reduced, uptime improved)?
* **Learning:** Did you show growth from failures?
* **Leadership:** Did you influence others or drive change?

**Self-Evaluation Checklist:**
- [ ] Did I use the STAR framework?
- [ ] Did I highlight MY specific contributions?
- [ ] Did I quantify the results?
- [ ] Did I show learning from failures?
- [ ] Did I stay concise (2-3 minutes per answer)?

---

## 2. Common Weak Areas

### Weak Area 1: Jumping to Solutions Too Quickly
**Problem:** "The DB is slow" → Immediately suggests "Add more RAM"
**Better:** Ask: "What queries are slow? Is it read or write? Is the issue consistent or intermittent?"

### Weak Area 2: Not Thinking About Scale
**Problem:** Designs a solution that works for 100 users but breaks at 100,000
**Better:** Always ask: "What's the expected load? How do we scale this?"

### Weak Area 3: Poor Communication
**Problem:** Solves the problem in their head silently, then announces the answer
**Better:** Think out loud. "I'm considering two approaches: A and B. Let me weigh the pros and cons..."

### Weak Area 4: Forgetting the "Why"
**Problem:** "I used Kubernetes because it's popular"
**Better:** "I chose Kubernetes because we needed auto-scaling and had multiple services that benefited from container orchestration"

---

## 3. Improvement Strategies

### Strategy 1: Record Yourself
* Do a mock interview while recording video
* Watch it back (painful but effective)
* Note: filler words, pace, clarity

### Strategy 2: The "5-Minute Rule"
* For any complex problem, spend the first 5 minutes just asking questions and clarifying requirements
* Don't write a single line of code or draw a single box until you fully understand the problem

### Strategy 3: Practice with Constraints
* "Design X, but you can only use open-source tools"
* "Debug this issue, but you don't have SSH access"
* Constraints force creative thinking

### Strategy 4: Teach Someone Else
* Explain a concept (e.g., "How Kubernetes networking works") to a friend or junior colleague
* If you can't explain it simply, you don't understand it well enough

Class 16.1.2:
	Title: Time Management During Interviews
	Description: Pacing yourself and handling time pressure.
Content Type: text
Duration: 400 
Order: 2
		Text Content :
 # Time Management: The Hidden Skill

## 1. The 60-Minute System Design Breakdown
Many candidates fail not because they lack knowledge, but because they run out of time.

**Time Allocation:**
* **0-5 mins:** Requirements clarification (DON'T SKIP THIS)
* **5-10 mins:** Capacity estimation & constraints
* **10-20 mins:** High-level architecture diagram
* **20-50 mins:** Deep dive into 2-3 components
* **50-60 mins:** Bottlenecks, trade-offs, wrap-up

**The Trap:** Spending 30 minutes on the high-level design and having no time for the deep dive

---

## 2. Recognizing When You're Stuck
**Red Flags:**
* You've been silent for more than 60 seconds
* You're rewriting the same diagram for the third time
* You're going in circles ("Maybe we use Redis... no, actually DynamoDB... wait, Redis...")

**What to Do:**
* **Ask for a hint:** "I'm debating between X and Y. Could you provide a hint about the direction?"
* **Verbalize your block:** "I'm stuck on how to handle write conflicts in a multi-master setup. Let me think about this systematically..."
* **Move on:** "I'm not 100% certain about this part. Can we continue and circle back if we have time?"

---

## 3. The Pause Technique
**Before answering any question:**
1. Take a 5-second pause
2. Structure your answer in your head
3. Then speak

**Benefits:**
* Avoids "umm" and "like" filler words
* Gives a more confident impression
* Prevents rambling

**Example:**
* Interviewer: "Why did you choose Terraform over CloudFormation?"
* [5-second pause]
* You: "Three reasons: multi-cloud support, larger community, and state management. Let me elaborate..."

Topic 16.2:
Title: Common Mistakes & How to Avoid Them
Order: 2

Class 16.2.1:
	Title: Technical Interview Mistakes
	Description: Pitfalls to avoid in technical rounds.
Content Type: text
Duration: 400 
Order: 1
		Text Content :
 # Technical Interview Mistakes

## 1. Over-Engineering Solutions
**Mistake:** "We need a service mesh with Istio, a multi-region active-active setup, and Kafka for all events"
**Reality:** The company has 3 developers and 100 users

**Fix:**
* Always start simple
* Ask: "What's the actual scale and team size?"
* You can mention complex solutions as "future considerations" without diving deep

---

## 2. Not Asking Clarifying Questions
**Mistake:** Jumping straight into "Here's the architecture!" without understanding requirements

**Fix:**
* Spend 5 minutes asking:
    * "What's the expected load?"
    * "What's more important: consistency or availability?"
    * "What's the budget?"
    * "Is there an existing tech stack we need to integrate with?"

---

## 3. Ignoring Constraints
**Mistake:** Designing a solution that requires 50 servers when the budget is $500/month

**Fix:**
* Explicitly state assumptions: "This design assumes we have X budget and Y scale"
* Ask about constraints upfront

---

## 4. Poor Communication
**Mistake:** Working silently for 10 minutes, then presenting a finished solution

**Fix:**
* Think out loud: "I'm considering two approaches..."
* Narrate your drawing: "I'm drawing the load balancer here because..."
* Check in: "Does this make sense so far?"

---

## 5. Rushing to Code
**Mistake:** Starting to write code immediately when asked to "write a script"

**Fix:**
* First, verbally outline the approach
* Get buy-in from the interviewer
* Then code

**Example:**
* Interviewer: "Write a script to find large files"
* You: "I'm thinking I'll use `find` with size filters, pipe to `sort`, and format the output. Sound good?"
* Interviewer: "Yes, proceed"
* [Now you code]

---

## 6. Not Considering Security
**Mistake:** Hardcoding passwords, not mentioning encryption, storing secrets in Git

**Fix:**
* Always mention: "Passwords would be stored in Secrets Manager / Vault"
* Talk about least privilege, encryption at rest/transit
* Mention security scanning in CI/CD

---

## 7. Ignoring Cost Implications
**Mistake:** "We'll just use the biggest EC2 instances for everything"

**Fix:**
* Mention cost-saving strategies:
    * Spot instances for non-critical workloads
    * Auto-scaling to avoid over-provisioning
    * S3 lifecycle policies
* Show awareness of the business side

Class 16.2.2:
	Title: Behavioral Interview Mistakes
	Description: Common pitfalls in behavioral rounds.
Content Type: text
Duration: 350 
Order: 2
		Text Content :
 # Behavioral Interview Mistakes

## 1. Speaking Negatively About Past Employers
**Mistake:** "My last boss was an idiot who didn't understand DevOps"

**Fix:**
* Be diplomatic: "We had different views on risk tolerance. I preferred gradual rollouts; management wanted faster releases"
* Focus on what you learned, not who was wrong

---

## 2. Taking Too Long to Answer
**Mistake:** A 10-minute rambling story about a minor bug fix

**Fix:**
* Use STAR and keep it to 2-3 minutes
* If the interviewer wants more detail, they'll ask

**Time Allocation for STAR:**
* Situation: 20 seconds
* Task: 20 seconds
* Action: 90 seconds
* Result: 30 seconds

---

## 3. Not Using Specific Examples
**Mistake:** "I'm good at debugging production issues" (Generic claim)

**Fix:**
* Give a specific example: "Last month, I debugged a memory leak in our checkout service by using `strace` and heap profiling, which reduced crashes by 90%"

---

## 4. Focusing on Team Instead of Individual Contributions
**Mistake:** "We migrated to Kubernetes" (Who is "we"? What did YOU do?)

**Fix:**
* Use "I" statements: "I designed the Kubernetes architecture, wrote the Helm charts, and trained the team on best practices"

---

## 5. Not Showing Growth from Failures
**Mistake:** When asked about a failure, deflecting: "Well, it wasn't really my fault because..."

**Fix:**
* Own it: "I deleted the production database. It was a mistake"
* Show learning: "I implemented a read-only prod role and a safety script that requires confirmation"

---

## 6. Being Unprepared for Common Questions
**Mistake:** Long pause when asked "Tell me about yourself"

**Fix:**
* Prepare and memorize a 60-second elevator pitch:
    * "I'm a DevOps Engineer with 5 years of experience in cloud infrastructure and automation"
    * "Most recently, I reduced deployment time by 70% at Company X"
    * "I'm looking for a role where I can work on large-scale distributed systems"

Topic 16.3:
Title: Final Tips & Strategy
Order: 3

Class 16.3.1:
	Title: Interview Day Strategy
	Description: Last-minute preparation and execution tips.
Content Type: text
Duration: 400 
Order: 1
		Text Content :
 # Interview Day Strategy

## Day Before the Interview

### 1. Review Key Concepts (2-3 hours max)
**Don't:**
* Try to learn new technologies
* Cram everything
* Stay up late studying

**Do:**
* Review your own projects and be ready to explain them
* Skim your "Stories Bank" (5 prepared STAR stories)
* Review basics: OSI model, TCP 3-way handshake, how DNS works

### 2. Prepare Questions for Interviewers (10-15 questions)
**Technical Questions:**
* "What's your deployment frequency?"
* "How do you handle on-call rotations?"
* "What's your biggest infrastructure challenge right now?"

**Culture Questions:**
* "What does a typical on-call week look like?"
* "How do you balance feature velocity with reliability?"
* "What's the team's approach to blameless post-mortems?"

**Growth Questions:**
* "What learning budget or conference allowance does the team have?"
* "Are there opportunities to work on new technologies?"

### 3. Set Up Environment (Virtual Interviews)
**Technical Setup:**
* Test camera, microphone, internet
* Have backup plan (phone hotspot, secondary device)
* Close all other applications
* Charge laptop fully + keep charger nearby

**Physical Setup:**
* Clean, well-lit background
* Glass of water nearby
* Notebook and pen for notes
* Copy of your resume

### 4. Rest Well
* No alcohol the night before
* Aim for 7-8 hours of sleep
* Don't schedule back-to-back interviews if possible

---

## During the Interview

### 1. First Impressions (First 60 seconds)
* Smile (even for virtual interviews - it changes your tone)
* Strong greeting: "Hi, I'm [Name]. Thanks for taking the time to meet with me"
* Have your elevator pitch ready for "Tell me about yourself"

### 2. Think Out Loud
**Why:** Interviewers want to understand your thought process, not just the final answer

**Example:**
* "I'm considering two approaches: using Redis for caching or implementing an in-memory LRU cache"
* "The trade-off is that Redis gives us shared cache across instances but adds network latency"
* "Given that the problem states we have 10 million users, I think Redis is the better choice"

### 3. Ask for Hints When Stuck
**Don't:**
* Sit in silence for 5 minutes
* Pretend you know something you don't

**Do:**
* "I'm not familiar with that specific AWS service. Could you provide a hint about its purpose?"
* "I'm stuck on this part. Can we move forward and circle back if there's time?"

### 4. Manage Your Time
* For system design: Check the clock at 15, 30, and 45-minute marks
* If running low on time: "I see we have 10 minutes left. Should I go deeper here or move to the next section?"

### 5. Stay Calm Under Pressure
**When you make a mistake:**
* Acknowledge it: "Oh, I see the issue - this approach won't work because..."
* Correct course: "Let me try a different approach"

**When you don't know something:**
* Be honest: "I haven't used that tool, but my understanding is..."
* Show learning ability: "I'm not familiar with it, but I'd research X, Y, Z to learn it quickly"

---

## After the Interview

### 1. Send Thank-You Notes (Within 24 hours)
**Structure:**
* Thank them for their time
* Reference something specific from the conversation
* Restate your interest
* Keep it brief (3-4 sentences)

**Example:**
```
Hi [Name],

Thank you for taking the time to discuss the Senior DevOps Engineer role yesterday. I especially enjoyed our conversation about your team's approach to progressive rollouts and the challenges you're facing with multi-region deployments.

I'm very excited about the opportunity to contribute to solving these problems and to learn from your experienced team.

Looking forward to the next steps.

Best regards,
[Your Name]
```

### 2. Reflect on Performance (30 minutes)
**Write down:**
* Questions you struggled with
* Topics you need to review
* What went well
* What you'd do differently

**Why:** This helps you improve for the next interview

### 3. Follow Up Appropriately
**Timeline:**
* If they said "We'll get back to you in a week" → Wait 7-8 days, then email
* If they didn't give a timeline → Email after 5 business days

**Follow-up Email Template:**
```
Hi [Recruiter Name],

I wanted to follow up on my interview for the DevOps Engineer role on [Date]. I remain very interested in the position and would love to know if there are any updates.

Please let me know if you need any additional information from my end.

Thank you,
[Your Name]
```

---

## Final Mindset Tips

### 1. You're Interviewing Them Too
* This is a two-way conversation
* Assess if you want to work there
* Red flags: Vague answers about on-call, poor work-life balance, no learning budget

### 2. One Interview Doesn't Define You
* Even strong candidates get rejected
* Use rejections as learning opportunities
* Keep applying and interviewing

### 3. Authenticity Over Perfection
* Don't pretend to know everything
* Show enthusiasm for learning
* Be your genuine self

### 4. The Worst Answer is No Answer
* If completely stuck, give your best guess with caveats
* Partial credit is better than zero credit

Class 16.3.2:
	Title: Week Before the Interview Checklist
	Description: Structured preparation timeline.
Content Type: text
Duration: 350 
Order: 2
		Text Content :
 # The 7-Day Preparation Plan

## Day 7 (One Week Before)

**Morning (2 hours):**
* Review all modules in this course (skim, don't deep-dive)
* Identify 3-4 weak areas

**Afternoon (2 hours):**
* Deep dive into your weakest area
* Practice 2-3 problems in that area

**Evening (1 hour):**
* Prepare your "Stories Bank" - write down 5 STAR stories:
    1. Production failure
    2. Conflict resolution
    3. Automation project
    4. Technical deep dive
    5. Leadership/ownership

---

## Day 6

**Morning (2 hours):**
* Practice system design on a whiteboard or drawing tool
* Time yourself: 45 minutes per problem
* Problems: Design Twitter, Design Netflix, Design Uber

**Afternoon (2 hours):**
* Practice troubleshooting scenarios
* Use actual environments (spin up a K8s cluster, intentionally break it, debug it)

**Evening (1 hour):**
* Review behavioral stories
* Practice delivering them out loud (yes, actually speak them)

---

## Day 5

**Morning (2 hours):**
* Mock technical screen with a friend or using Pramp/Interviewing.io
* Get feedback on communication style

**Afternoon (2 hours):**
* Review AWS/GCP/Azure services (whatever is relevant)
* Focus on services mentioned in the job description

**Evening (1 hour):**
* Prepare questions to ask interviewers
* Research the company: recent news, tech blog posts, engineering culture

---

## Day 4

**Morning (2 hours):**
* Practice live coding problems on LeetCode (Easy/Medium)
* Focus on: string manipulation, hash maps, sorting

**Afternoon (2 hours):**
* Review infrastructure concepts:
    * How HTTPS works end-to-end
    * Database replication strategies
    * Caching patterns

**Evening (1 hour):**
* Watch YouTube videos of mock DevOps interviews
* Observe what good candidates do differently

---

## Day 3

**Morning (2 hours):**
* Full mock interview loop (all 4 rounds)
* Time yourself strictly

**Afternoon (2 hours):**
* Review the mock interview performance
* Identify gaps and study those topics

**Evening (Light review):**
* Skim through your notes
* Don't introduce new material

---

## Day 2

**Morning (1 hour):**
* Light review of key concepts only
* Focus on clarity, not memorization

**Afternoon (1 hour):**
* Prepare your interview outfit (if in-person)
* Test technology setup (if virtual)

**Evening (Relax):**
* Do something non-technical
* Early dinner, light exercise
* No cramming

---

## Day 1 (Day Before)

**Morning (30 mins):**
* Quick skim of your STAR stories
* Review questions to ask interviewers

**Afternoon:**
* Do something enjoyable and relaxing
* Light exercise (walk, gym)

**Evening:**
* Early, healthy dinner
* Prep clothes, bag, notebook
* Set 2-3 alarms
* Bed by 10 PM

---

## Interview Day

**Morning:**
* Light breakfast (avoid heavy foods that make you sleepy)
* Arrive 10-15 minutes early (or log in 5 minutes early for virtual)
* Use the restroom before
* Quick breathing exercise: 4 counts in, 7 counts hold, 8 counts out (repeat 3 times)

**During:**
* Execute your preparation
* Trust your preparation
* Stay present

**After:**
* Debrief with yourself
* Send thank-you emails
* Relax - you did your best

---

## Remember

**The 3 Pillars of Interview Success:**
1. **Knowledge:** You've studied the material
2. **Practice:** You've done mock interviews
3. **Communication:** You can explain your thinking clearly

All three are necessary. You've got this!


